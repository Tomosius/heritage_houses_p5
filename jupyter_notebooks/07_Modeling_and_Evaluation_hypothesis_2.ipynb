{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ],
   "id": "3aaf88d26c42c1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "75a72d0651596244"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:27:57.676940Z",
     "start_time": "2024-05-05T21:27:57.448441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from feature_engine.transformation import BoxCoxTransformer\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "84e53adb39c40ff0",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "624018c3aea597b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:27:59.251226Z",
     "start_time": "2024-05-05T21:27:59.235845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "d0307cd1e623595e",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "b4e467105bdd1e91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:27:59.884094Z",
     "start_time": "2024-05-05T21:27:59.874739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "45512d37254c92ce",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "3789f66254e08f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:28:00.768595Z",
     "start_time": "2024-05-05T21:28:00.492403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_parquet('outputs/datasets/cleaned/train.parquet.gzip')\n",
    "df_train.head()\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet('outputs/datasets/cleaned/test.parquet.gzip')\n",
    "df_train.head()"
   ],
   "id": "9dd310e3dcb6619f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ],
   "id": "ce222f2402f6e5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:28:00.972974Z",
     "start_time": "2024-05-05T21:28:00.960383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_features = ['Unnamed: 0', 'EnclosedPorch', 'WoodDeckSF']\n",
    "df_train.drop(columns=drop_features, inplace=True)\n",
    "df_test.drop(columns=drop_features, inplace=True)"
   ],
   "id": "f80b313366f049f6",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:28:01.243170Z",
     "start_time": "2024-05-05T21:28:01.230253Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.columns.tolist()",
   "id": "f176c1a1b93dbbe6",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5f5cb59c8c7e169"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Machine Learning",
   "id": "f3077ebeb6a8d911"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:39:58.469174Z",
     "start_time": "2024-05-05T21:39:58.456463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define feature pipeline\n",
    "def feature_pipeline(model):\n",
    "    from feature_engine.encoding import OrdinalEncoder\n",
    "    from feature_engine.transformation import LogTransformer, YeoJohnsonTransformer, PowerTransformer\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "                                                     variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish',\n",
    "                                                                'KitchenQual'])),\n",
    "        ('LogTransformer', LogTransformer(variables=['GarageYrBlt', 'GrLivArea', 'YearBuilt', 'YearRemodAdd'])),\n",
    "        ('YeoJohnsonTransformer', YeoJohnsonTransformer(\n",
    "            variables=['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtFinType1', 'GarageFinish',\n",
    "                       'LotArea', 'MasVnrArea', 'OpenPorchSF', 'OverallQual', 'TotalBsmtSF'])),\n",
    "        ('PowerTransformer', PowerTransformer(variables=['BsmtFinSF1', 'BsmtUnfSF', 'KitchenQual'])),\n",
    "        (\"Winsorizer\", Winsorizer(capping_method='iqr', tail='both', fold=1.5,\n",
    "                                  variables=['GrLivArea'])),\n",
    "\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "\n",
    "    return pipeline_base\n",
    "\n"
   ],
   "id": "7a58bc2bddf07de0",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ],
   "id": "84dc27d100d55d1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:40:00.456624Z",
     "start_time": "2024-05-05T21:40:00.435689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model =  feature_pipeline(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params,**d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ],
   "id": "bebb2cba05dcd497",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting Sets to Features and targets",
   "id": "b65489074cbe5f03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:40:01.854660Z",
     "start_time": "2024-05-05T21:40:01.844491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify the target variable column name\n",
    "target_column = 'SalePrice'\n",
    "\n",
    "# Extract the target variable\n",
    "y_train = df_train[target_column]\n",
    "y_test = df_test[target_column]\n",
    "\n",
    "# Remove the target variable from the DataFrame to create the feature DataFrame\n",
    "X_train = df_train.drop(columns=[target_column])\n",
    "X_test = df_test.drop(columns=[target_column])\n"
   ],
   "id": "191d1fd26d62f8bd",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ],
   "id": "f9f113541180514c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:40:03.336972Z",
     "start_time": "2024-05-05T21:40:03.327688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ],
   "id": "10c625d779cae244",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running Grid Search CV",
   "id": "e61feca22f472f5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Results Inspection",
   "id": "e3aaf906f555480d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:40:29.372353Z",
     "start_time": "2024-05-05T21:40:04.937823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "d7f0e5efd27e38df",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:40:33.007938Z",
     "start_time": "2024-05-05T21:40:32.945955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "8f27c51e2b115079",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that GradientBoostingRegressor shows most promising results, mean = 0.831497\n",
    "Now we will add extra HyperParameters"
   ],
   "id": "951d2801cb8ac903"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:42:13.865652Z",
     "start_time": "2024-05-05T21:42:13.850554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "models_tune_search = {\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_tune_search = {\n",
    "    \"ExtraTreesRegressor\": {\n",
    "        'model__n_estimators': [100, 300],\n",
    "        'model__max_depth': [3, 10, None],\n",
    "        'model__min_samples_split': [8],\n",
    "    }\n",
    "}\n"
   ],
   "id": "a7dff9d6dc2a69ce",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:42:42.757862Z",
     "start_time": "2024-05-05T21:42:14.676532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_tune_search, params=params_tune_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "20ca6514b7633a06",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:42:44.610765Z",
     "start_time": "2024-05-05T21:42:44.563467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "f04e2add8d9ab224",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Yay, we managed to increase mean from 0.832868 to 0.846216, not a lot but still something.",
   "id": "d9a4d56409a183e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Selecting best model",
   "id": "363f8deb7b017bbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:03.184168Z",
     "start_time": "2024-05-05T21:43:03.175457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model = grid_search_summary.iloc[0, 0]\n",
    "best_model"
   ],
   "id": "9f486b263f820d1a",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Parameters for best model",
   "id": "4b46f7f9e5737b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:05.984563Z",
     "start_time": "2024-05-05T21:43:05.975284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ],
   "id": "55b3dc42b80b6d29",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:06.924070Z",
     "start_time": "2024-05-05T21:43:06.857847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline"
   ],
   "id": "50182ad9226bf9a0",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accessing Feature Importance",
   "id": "f2473c06ac24a8f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:12.201958Z",
     "start_time": "2024-05-05T21:43:11.556753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# after data cleaning and feat engine, the feature may space changes\n",
    "data_cleaning_feat_eng_steps = 5  # how many data cleaning and feature engineering the pipeline has\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()],\n",
    "    'Importance': best_regressor_pipeline['model'].feature_importances_})\n",
    "                         .sort_values(by='Importance', ascending=False)\n",
    "                         )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ],
   "id": "df62143bc114f658",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating Model on Train and Test Sets",
   "id": "212be54b4767cc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:13.963385Z",
     "start_time": "2024-05-05T21:43:13.944736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    # Train plot\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0], color='blue')\n",
    "    axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')  # Red line y=x\n",
    "    axes[0].set_xlabel(\"Actual Values\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set Performance\")\n",
    "\n",
    "    # Test plot\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1], color='green')\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Red line y=x\n",
    "    axes[1].set_xlabel(\"Actual Values\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set Performance\")\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "a4307e48f8f882a6",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:43:17.860704Z",
     "start_time": "2024-05-05T21:43:15.658527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
   ],
   "id": "c8183ba9e91bdceb",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This model is not accurate enough, as mean score of test is just 0.609",
   "id": "30f57d56866133f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hypothesis 3: There Might be a combination of features to predict Sale Price.",
   "id": "d8b4af3e5d2831b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
