{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ],
   "id": "3aaf88d26c42c1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "75a72d0651596244"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:37.993707Z",
     "start_time": "2024-05-04T19:55:37.798142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from feature_engine.transformation import BoxCoxTransformer\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "84e53adb39c40ff0",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "624018c3aea597b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:38.012377Z",
     "start_time": "2024-05-04T19:55:37.996902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "d0307cd1e623595e",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "b4e467105bdd1e91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:38.034244Z",
     "start_time": "2024-05-04T19:55:38.018033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "45512d37254c92ce",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "3789f66254e08f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:38.336834Z",
     "start_time": "2024-05-04T19:55:38.040916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_parquet('outputs/datasets/cleaned/train.parquet.gzip')\n",
    "df_train.head()\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet('outputs/datasets/cleaned/test.parquet.gzip')\n",
    "df_train.head()"
   ],
   "id": "9dd310e3dcb6619f",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we will select these features:"
   ],
   "id": "ce222f2402f6e5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:38.364625Z",
     "start_time": "2024-05-04T19:55:38.344842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# importing home-made functions\n",
    "from extra_funcionality import load_data\n",
    "\n",
    "# getting list of selected features\n",
    "hypothesis_1_features = load_data('hypothesis_1_features')\n",
    "hypothesis_1_features"
   ],
   "id": "3f68f0fedc87a0d0",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:38.383725Z",
     "start_time": "2024-05-04T19:55:38.369319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In dataframe keeping just selected features\n",
    "df_train = df_train[hypothesis_1_features]\n",
    "df_test = df_test[hypothesis_1_features]"
   ],
   "id": "f80b313366f049f6",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5f5cb59c8c7e169"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Machine Learning",
   "id": "f3077ebeb6a8d911"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:39.338549Z",
     "start_time": "2024-05-04T19:55:38.390001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Data Cleaning\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feature Engineering\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine import transformation as vt\n",
    "\n",
    "### Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Custom Transformer for dividing 'YearBuilt' by 1e69\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class DivideByConstant(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, constant=1e69):\n",
    "        self.constant = constant\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X['YearBuilt'] = X['YearBuilt'] / self.constant\n",
    "        return X\n",
    "\n",
    "\n",
    "def PipelineClf(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        # Feature Engineering\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary',\n",
    "                                                     variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish',\n",
    "                                                                'KitchenQual'])),\n",
    "        ('BoxCoxTransformer', BoxCoxTransformer(variables=['GrLivArea', 'YearBuilt'])),\n",
    "        # Divide 'YearBuilt' by 1e69\n",
    "        ('DivideByConstant', DivideByConstant(constant=1e69)),\n",
    "        ('YeoJohnsonTransformer', vt.YeoJohnsonTransformer(variables=['1stFlrSF', 'TotalBsmtSF'])),\n",
    "\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ],
   "id": "7a58bc2bddf07de0",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ],
   "id": "84dc27d100d55d1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:39.394948Z",
     "start_time": "2024-05-04T19:55:39.370548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model = PipelineClf(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ],
   "id": "bebb2cba05dcd497",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting Sets to Features and targets",
   "id": "b65489074cbe5f03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:39.414085Z",
     "start_time": "2024-05-04T19:55:39.400168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify the target variable column name\n",
    "target_column = 'SalePrice'\n",
    "\n",
    "# Extract the target variable\n",
    "y_train = df_train[target_column]\n",
    "y_test = df_test[target_column]\n",
    "\n",
    "# Remove the target variable from the DataFrame to create the feature DataFrame\n",
    "X_train = df_train.drop(columns=[target_column])\n",
    "X_test = df_test.drop(columns=[target_column])\n"
   ],
   "id": "191d1fd26d62f8bd",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ],
   "id": "f9f113541180514c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:55:39.441165Z",
     "start_time": "2024-05-04T19:55:39.429582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ],
   "id": "10c625d779cae244",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running Grid Search CV",
   "id": "e61feca22f472f5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Results Inspection",
   "id": "e3aaf906f555480d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:00.711212Z",
     "start_time": "2024-05-04T19:55:39.448806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "d7f0e5efd27e38df",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:00.754395Z",
     "start_time": "2024-05-04T19:56:00.713997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "8f27c51e2b115079",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that GradientBoostingRegressor shows most promising results, mean = 0.841347\n",
    "Now we will add extra HyperParameters"
   ],
   "id": "951d2801cb8ac903"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:00.769335Z",
     "start_time": "2024-05-04T19:56:00.757764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_tuning_search = {\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "\n",
    "}\n",
    "param_grid = {\n",
    "    \"GradientBoostingRegressor\": {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.05, 0.1],\n",
    "        'model__max_depth': [3, 5],\n",
    "        'model__min_samples_split': [2],\n",
    "        'model__min_samples_leaf': [1]\n",
    "    }\n",
    "}"
   ],
   "id": "a7dff9d6dc2a69ce",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:15.638144Z",
     "start_time": "2024-05-04T19:56:00.772734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_tuning_search, params=param_grid)\n",
    "search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "20ca6514b7633a06",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:15.685192Z",
     "start_time": "2024-05-04T19:56:15.640414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "f04e2add8d9ab224",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Yay, we managed to increase mean from 0.841347 to 0.852885. not a lot but still something.",
   "id": "d9a4d56409a183e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Selecting best model",
   "id": "363f8deb7b017bbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:15.701505Z",
     "start_time": "2024-05-04T19:56:15.687453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ],
   "id": "9f486b263f820d1a",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Parameters for best model",
   "id": "4b46f7f9e5737b6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:15.725306Z",
     "start_time": "2024-05-04T19:56:15.705126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ],
   "id": "55b3dc42b80b6d29",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:15.788285Z",
     "start_time": "2024-05-04T19:56:15.729883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline"
   ],
   "id": "50182ad9226bf9a0",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accessing Feature Importance",
   "id": "f2473c06ac24a8f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:16.384303Z",
     "start_time": "2024-05-04T19:56:15.793173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# after data cleaning and feat engine, the feature may space changes\n",
    "data_cleaning_feat_eng_steps = 4 # how many data cleaning and feature engineering the pipeline has\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[best_regressor_pipeline['feat_selection'].get_support()],\n",
    "    'Importance': best_regressor_pipeline['model'].feature_importances_})\n",
    "                         .sort_values(by='Importance', ascending=False)\n",
    "                         )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()"
   ],
   "id": "df62143bc114f658",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating Model on Train and Test Sets",
   "id": "212be54b4767cc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:16.416011Z",
     "start_time": "2024-05-04T19:56:16.388151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    # Train plot\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0], color='blue')\n",
    "    axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')  # Red line y=x\n",
    "    axes[0].set_xlabel(\"Actual Values\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set Performance\")\n",
    "\n",
    "    # Test plot\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1], color='green')\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Red line y=x\n",
    "    axes[1].set_xlabel(\"Actual Values\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set Performance\")\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "a4307e48f8f882a6",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T19:56:18.060780Z",
     "start_time": "2024-05-04T19:56:16.420223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_regressor_pipeline)"
   ],
   "id": "c8183ba9e91bdceb",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## This model is not accurate enough, as mean score of test is just 0.448",
   "id": "30f57d56866133f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hypothesis 2: we need all features to predict Sale Price",
   "id": "d8b4af3e5d2831b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
