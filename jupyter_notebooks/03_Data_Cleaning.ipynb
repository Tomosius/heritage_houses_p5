{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7875c24c43c0d1e",
   "metadata": {},
   "source": [
    "# Notebook 03 - Data Cleaning\n",
    "\n",
    "## Objectives\n",
    "* Clean data\n",
    "* Evaluate and process missing data\n",
    "\n",
    "## Inputs\n",
    "* outputs/datasets/collection/HousePricesRecords.csv\n",
    "\n",
    "## Outputs\n",
    "* Create Clean dataset:\n",
    "    * all new datasets of cleaning will be stored in inputs/datasets/cleaning\n",
    "* Split created dataset in to 2 parts:\n",
    "    * Train\n",
    "    * Test\n",
    "* all new datasets (train and test) will be stored in outputs/datasets/cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a32a78833a8ea4",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "id": "441b1d998870df76",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af8ce4b5f364e90",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8c62acadc6c7d2",
   "metadata": {},
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59f6db5b4d3b9e3b",
   "metadata": {},
   "source": [
    "Confirm new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d1302e4d5192730",
   "metadata": {},
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "891d66d7edd5b491",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "d983f77e553443e0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46a82af219cc1b93",
   "metadata": {},
   "source": [
    "Will make a copy of dataset, so later it can be compared with cleaned one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957218d717e97c5c",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "\n",
    "We will get all features that are missing data as a list"
   ]
  },
  {
   "cell_type": "code",
   "id": "d450551279e1eeb9",
   "metadata": {},
   "source": [
    "features_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "features_with_missing_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f92e45858086afa",
   "metadata": {},
   "source": [
    "### Visualizing Missing Data\n",
    "\n",
    "Visualize the missing data to better understand the pattern of missing values."
   ]
  },
  {
   "cell_type": "code",
   "id": "977c7f5b71a393e8",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the background style\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Prepare the data for heatmap, converting True/False to integers for color mapping\n",
    "heatmap_data = df.isnull().astype(int)\n",
    "\n",
    "# Visualize missing values as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Using `cbar=True` to show the color bar\n",
    "ax = sns.heatmap(heatmap_data, yticklabels=False, cbar=True, cmap='viridis',\n",
    "                 cbar_kws={'label': 'Missing Data Indicator'})\n",
    "\n",
    "# Customize the plot with titles and labels as needed\n",
    "plt.title('Missing Data Heatmap', fontsize=16, color='navy')\n",
    "plt.xticks(fontsize=12, color='darkred')  # Set x-tick colors to dark red\n",
    "plt.yticks(fontsize=12, color='darkred')  # Set y-tick colors to dark red\n",
    "\n",
    "# Set color bar label and style\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label('Missing Data Indicator', rotation=270, labelpad=20)\n",
    "cbar.set_ticks([0.25, 0.75])  # Setting tick positions\n",
    "cbar.set_ticklabels(['Present', 'Missing'])  # Setting tick labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "386c56982028ba89",
   "metadata": {},
   "source": [
    "We will generate a profiling report for features with missing data, which will assist us in selecting the most effective method for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "id": "9843a601856f1124",
   "metadata": {},
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df=df[features_with_missing_data], minimal=True)\n",
    "profile.to_notebook_iframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6179bce92c628e0d",
   "metadata": {},
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## All Data Cleaning\n",
    "\n",
    "All Data cleaning with all steps, graphs, etc. will be performed in folder jupyter_notebooks/data_cleaning/\n",
    "\n",
    "* All this is because we have noticed quite a few missing data vales, also we need to perform more deep checking on data is it all correct and valid.\n",
    "* After All cleaning and fixing data in folder jupyter_notebooks/data_cleaning is completed, we will import inputs/datasets/cleaning/clean_finished.csv as current dataframe\n",
    "\n",
    "When jupyter_notebooks/data_cleaning is being processed, we will add cleaning code below"
   ],
   "id": "c0e3fed0608a12d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.loc[:, 'LotFrontage'] = df['LotFrontage'].fillna(70)\n",
    "\n",
    "# Lists of columns grouped by their fill values and type conversions\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageArea', 'GarageYrBlt',\n",
    "                         'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF', 'BedroomAbvGr']\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "df['LotFrontage'] = df['LotFrontage'].round().astype(int)\n",
    "\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"BsmtFinSF1\": 0, \"BsmtUnfSF\": 0,\n",
    "                       \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Dropping new created column Consistency\n",
    "df = df.drop(columns=['Consistency'])\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtUnfSF'] + df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']"
   ],
   "id": "faf716153a9b930",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb7256e19753fd31",
   "metadata": {},
   "source": [
    "During cleaning, we did not drop any features, even some of them were missing nearly 90% of data.\n",
    "We keep them to explore any potential correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f5fd9c5161802",
   "metadata": {},
   "source": "## Splitting data to train and test"
  },
  {
   "cell_type": "code",
   "id": "a88ea0efe7bf69f3",
   "metadata": {},
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure output directories exist\n",
    "output_dir = 'outputs/datasets/cleaned'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Creates the directory if it does not exist\n",
    "\n",
    "# No need to separate features and target just yet, keep the dataframe whole for the split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc0851c445c2e727",
   "metadata": {},
   "source": [
    "# Save the datasets to PARQUET files including 'SalePrice'\n",
    "df_train.to_parquet(f'{output_dir}/train.parquet.gzip', compression='gzip')\n",
    "df_test.to_parquet(f'{output_dir}/test.parquet.gzip', compression='gzip')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "968a6bf7e76a0283",
   "metadata": {},
   "source": [
    "## Correlation Study\n",
    "### Initial study of correlations between features\n",
    "Before we start checking correlations, we need to "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix with numeric_only set to True\n",
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(15, 15))  # Set the size of the figure\n",
    "plt.matshow(corr, cmap='coolwarm', fignum=1)  # Plot the correlation matrix as a heatmap\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels to the x and y axes\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha='left')\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "\n",
    "# Title for the heatmap\n",
    "plt.title('Correlation Heatmap with Coefficients', pad=20)\n",
    "\n",
    "# Add correlation coefficients on the heatmap\n",
    "for (i, j), val in np.ndenumerate(corr.values):\n",
    "    plt.text(j, i, f'{val:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "ca5dfa52040f1b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57e070a6d14f8fec",
   "metadata": {},
   "source": [
    "We will explore correlation (pearson and spearman methods) of Sales Price and all the rest features, sort values by absolute value and see most correlated features\n",
    "\n",
    "* Before we process to correlation, all data objects needs to be encoded"
   ]
  },
  {
   "cell_type": "code",
   "id": "407613fdc03d09dd",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "\n",
    "def plot_single_correlation(correlation, correlation_type, target):\n",
    "    \"\"\"\n",
    "    Plots a single correlation type.\n",
    "    correlation: DataFrame with correlation values\n",
    "    correlation_type: String, name of the correlation type\n",
    "    target: String, name of the target variable\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = correlation.plot(kind='bar', color='lightgreen')\n",
    "    plt.title(f'{correlation_type} Correlation Coefficients with {target}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel(f'{correlation_type} Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(p.get_x() + p.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_numerical_correlations(df, target):\n",
    "    \"\"\"Calculates and returns a dictionary of correlation DataFrames for numerical features.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    correlations = {}\n",
    "    for method in ['Pearson', 'Spearman', 'Kendall']:\n",
    "        corrs = []\n",
    "        for col in numerical_cols:\n",
    "            if col != target:\n",
    "                if method == 'Pearson':\n",
    "                    corr_value = ss.pearsonr(df[col], df[target])[0]\n",
    "                elif method == 'Spearman':\n",
    "                    corr_value = ss.spearmanr(df[col], df[target])[0]\n",
    "                elif method == 'Kendall':\n",
    "                    corr_value = ss.kendalltau(df[col], df[target])[0]\n",
    "                corrs.append(corr_value)\n",
    "        correlations[method] = pd.Series(corrs, index=[col for col in numerical_cols if col != target])\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def calculate_categorical_correlations(df, target):\n",
    "    \"\"\"Calculates and returns a dictionary of correlation DataFrames for categorical features.\"\"\"\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    correlations = {'Cramer\\'s V': {}, 'Mutual Information': {}}\n",
    "    for col in categorical_cols:\n",
    "        if col != target:\n",
    "            table = pd.crosstab(df[col], df[target])\n",
    "            chi2 = ss.chi2_contingency(table)[0]\n",
    "            correlations['Cramer\\'s V'][col] = np.sqrt(chi2 / (table.sum().sum() * (min(table.shape) - 1)))\n",
    "            correlations['Mutual Information'][col] = mutual_info_score(df[col], df[target])\n",
    "    for key in correlations:\n",
    "        correlations[key] = pd.Series(correlations[key])\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def plot_top_correlations(df, target, top_n=10):\n",
    "    num_corr = calculate_numerical_correlations(df, target)\n",
    "    cat_corr = calculate_categorical_correlations(df, target)\n",
    "    all_correlations = {**num_corr, **cat_corr}\n",
    "\n",
    "    # Plot individual correlation types\n",
    "    for corr_type, corr_values in all_correlations.items():\n",
    "        plot_single_correlation(corr_values, corr_type, target)\n",
    "\n",
    "    # Convert dictionary of Series to DataFrame\n",
    "    correlation_df = pd.DataFrame(all_correlations)\n",
    "\n",
    "    # Calculate the maximum correlation value for each feature and determine the corresponding method\n",
    "    max_values = correlation_df.max(axis=1)\n",
    "    max_methods = correlation_df.idxmax(axis=1)\n",
    "\n",
    "    # Create a DataFrame to store these maximum values along with their corresponding methods\n",
    "    result_df = pd.DataFrame({\n",
    "        'Feature': max_values.index,\n",
    "        'Method': max_methods.values,\n",
    "        'Value': max_values.values\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the 'Value' column in descending order to find the highest correlations\n",
    "    result_df = result_df.sort_values(by='Value', ascending=False).head(top_n)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(result_df['Feature'] + \" (\" + result_df['Method'] + \")\", result_df['Value'], color='skyblue')\n",
    "    plt.title('Top Correlation Coefficients Across All Methods')\n",
    "    plt.xlabel('Feature (Method)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Annotating the bars with their correlation values\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.2f}\", va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "list_of_top = plot_top_correlations(df_train, 'SalePrice', top_n=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56315963ef047277",
   "metadata": {},
   "source": "We will print out our top 10 correlations:"
  },
  {
   "cell_type": "code",
   "id": "89f060c8dc990401",
   "metadata": {},
   "source": [
    "list_of_top"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4f614ecb9675f59",
   "metadata": {},
   "source": [
    "We have a list of features we will be investigating:\n",
    "* BsmtFinType1\n",
    "* KitchenQual\n",
    "* OverallQual\n",
    "* BsmtExposure\n",
    "* GrLivArea\n",
    "* GarageFinish\n",
    "* 1stFlrSF\n",
    "* TotalBsmtSF\n",
    "* GarageArea\n",
    "* YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cb4d95456e11b9",
   "metadata": {},
   "source": [
    "# Converting to list all top features\n",
    "top_correlations_list_hypothesis_1 = list_of_top.values.tolist()\n",
    "# Making a list of features + SalePrice, so later it is easier to manage dataframe.\n",
    "features_list = [item[0] for item in top_correlations_list_hypothesis_1]\n",
    "features_list.append('SalePrice')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eeb16cfb95622c1c",
   "metadata": {},
   "source": [
    "## EDA on selected features\n",
    "\n",
    "We will create separate dataframe just with selected features + SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ed89bb1c6d3667e",
   "metadata": {},
   "source": [
    "df = df_train[features_list]\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "82bc4f2c7dbd4b3c",
   "metadata": {},
   "source": "We will check how each feature is distributed against the Price, so we can see correlations in more details"
  },
  {
   "cell_type": "code",
   "id": "e0273db2c314cd07",
   "metadata": {},
   "source": [
    "numeric_columns = df.columns\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot each feature against SalePrice\n",
    "for column in numeric_columns[:-1]:\n",
    "    sns.scatterplot(x=column, y='SalePrice', data=df)\n",
    "    plt.title(f'{column} vs. SalePrice')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1707db0016b22579",
   "metadata": {},
   "source": [
    "## Outcomes\n",
    "\n",
    "### Hypothesis - 1\n",
    "\n",
    "Based on Plots above we can conclude these points:\n",
    "\n",
    "* BsmtFinType1 - With Basement Quality increasing, house price tends to increase, although there is some high prices in Unfinished category\n",
    "* KitchenQual - Kitchen Quality is very clear, with its quality going up, price also goes up.\n",
    "* OverallQual - Overall Quality is most correlated feature from all, and when it increases, SalePrice also increases\n",
    "* BsmtExposure - If There is basement in building, With better exposure price tends to increase\n",
    "* GrLivArea - Increasing Ground Living Area SalePrice also increases\n",
    "* GarageFinish - Just having garage, price increases, and the more it is Finished, the better house price\n",
    "* 1stFlrSF - Similar to Ground Living Area, when it goes up, SalePrice also increases\n",
    "* TotalBsmtSF - By increasing Basement we are increasing SalePrice\n",
    "* GarageArea - When Garage Area goes up, Sale Price also increase. Most of the houses without garages are 150.000 or less\n",
    "* YearBuilt - By increasing Garage Year Built we are increasing SalePrice, but it looks more exponential after 1980 ish..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f56f90eb6269e",
   "metadata": {},
   "source": [
    "## Next is Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
