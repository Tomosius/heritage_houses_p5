{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ],
   "id": "3aaf88d26c42c1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "75a72d0651596244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "84e53adb39c40ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "624018c3aea597b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "d0307cd1e623595e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "b4e467105bdd1e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "45512d37254c92ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "3789f66254e08f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "id": "9dd310e3dcb6619f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ],
   "id": "ce222f2402f6e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drop_features = ['Unnamed: 0']\n",
    "df.drop(columns=drop_features, inplace=True)"
   ],
   "id": "f80b313366f049f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Dataset",
   "id": "f742e59c8c0be550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.loc[:, 'LotFrontage'] = df['LotFrontage'].fillna(70)\n",
    "\n",
    "# Lists of columns grouped by their fill values and type conversions\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageArea', 'GarageYrBlt',\n",
    "                         'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF', 'BedroomAbvGr']\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "df['LotFrontage'] = df['LotFrontage'].round().astype(int)\n",
    "\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"BsmtFinSF1\": 0, \"BsmtUnfSF\": 0,\n",
    "                       \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Dropping new created column Consistency\n",
    "df = df.drop(columns=['Consistency'])\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtUnfSF'] + df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']"
   ],
   "id": "4afe74b356e994b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting to data and test dataframe",
   "id": "27600268ed420431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='SalePrice')\n",
    "y = df['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "id": "7b05199b68ab479a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Pre-Transformations"
   ],
   "id": "f3077ebeb6a8d911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "        \n",
    "        # New features descriptions to be presented in Pipeline i\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalBsmtSF_mul_BsmtExposure': 'TotalBsmtSF * BsmtExposure',\n",
    "            'NF_TotalBsmtSF_mul_BsmtFinType1': 'TotalBsmtSF * BsmtFinSF1',\n",
    "            'NF_BsmtFinSF1_mul_BsmtFinType1': 'BsmtFinType1 * BsmtFinSF1',\n",
    "            'NF_GarageFinish_mul_GarageArea': 'GarageFinish * GarageArea',\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "            'NF_2ndFlrSF_mul_OverallQual': '2ndFlrSF * OverallQual',\n",
    "            'NF_Age_Garage': '2010 - GarageYrBlt',\n",
    "            'NF_Age_Build': '2010 - YearBuilt',\n",
    "            'NF_Age_Remod': '2010 - YearRemodAdd',\n",
    "            'NF_Remod_TEST': '0 if NF_Age_Build == NF_Age_Remod else NF_Age_Remod',\n",
    "            'NF_Has_2nd_floor': '1 if 2ndFlrSF > 0 else 0',\n",
    "            'NF_Has_basement': '1 if TotalBsmtSF > 0 else 0',\n",
    "            'NF_Has_garage': '1 if GarageArea > 0 else 0',\n",
    "            'NF_Has_Masonry_Veneer': '1 if MasVnrArea > 0 else 0',\n",
    "            'NF_Has_Enclosed_Porch': '1 if EnclosedPorch > 0 else 0',\n",
    "            'NF_Has_Open_Porch': '1 if OpenPorchSF > 0 else 0',\n",
    "            'NF_Has_ANY_Porch': 'NF_Has_Enclosed_Porch | NF_Has_Open_Porch',\n",
    "            'NF_Has_Wooden_Deck': '1 if WoodDeckSF > 0 else 0'\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalBsmtSF_mul_BsmtExposure'] = X['TotalBsmtSF'] * X['BsmtExposure']\n",
    "        X['NF_TotalBsmtSF_mul_BsmtFinType1'] = X['TotalBsmtSF'] * X['BsmtFinSF1']\n",
    "        X['NF_BsmtFinSF1_mul_BsmtFinType1'] = X['BsmtFinType1'] * X['BsmtFinSF1']\n",
    "        X['NF_GarageFinish_mul_GarageArea'] = X['GarageFinish'] * X['GarageArea']\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "        X['NF_2ndFlrSF_mul_OverallQual'] = X['2ndFlrSF'] * X['OverallQual']\n",
    "        X['NF_Age_Garage'] = 2010 - X['GarageYrBlt']\n",
    "        X['NF_Age_Build'] = 2010 - X['YearBuilt']\n",
    "        X['NF_Age_Remod'] = 2010 - X['YearRemodAdd']\n",
    "        X['NF_Remod_TEST'] = X.apply(\n",
    "            lambda row: 0 if row['NF_Age_Build'] == row['NF_Age_Remod'] else row['NF_Age_Remod'], axis=1)\n",
    "        X[('NF_Has_2nd_floor')] = X.apply(lambda row: False if row['2ndFlrSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_basement')] = X.apply(lambda row: False if row['TotalBsmtSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_garage')] = X.apply(lambda row: False if row['GarageArea'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_Masonry_Veneer')] = X.apply(lambda row: False if row['MasVnrArea'] == 0 else True, axis=1).astype(\n",
    "            int)\n",
    "        X[('NF_Has_Enclosed_Porch')] = X.apply(lambda row: False if row['EnclosedPorch'] == 0 else True,\n",
    "                                               axis=1).astype(int)\n",
    "        X[('NF_Has_Open_Porch')] = X.apply(lambda row: False if row['OpenPorchSF'] == 0 else True, axis=1).astype(int)\n",
    "        X['NF_Has_ANY_Porch'] = X['NF_Has_Enclosed_Porch'] | X['NF_Has_Open_Porch'].astype(int)\n",
    "        X[('NF_Has_Wooden_Deck')] = X.apply(lambda row: False if row['WoodDeckSF'] == 0 else True, axis=1).astype(int)\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join([f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])"
   ],
   "id": "1733f60ad69b2844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features - Columns transformations",
   "id": "daecbf2114709a09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, BoxCoxTransformer, PowerTransformer\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtUnfSF', 'EnclosedPorch',\n",
    "                        'GarageArea', 'GarageFinish', 'GrLivArea', 'KitchenQual', 'LotArea', 'MasVnrArea',\n",
    "                        'OpenPorchSF', 'OverallCond', 'TotalBsmtSF', 'WoodDeckSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                        'NF_BsmtFinSF1_mul_BsmtFinType1', 'NF_GarageFinish_mul_GarageArea', 'NF_TotalLivingArea',\n",
    "                        'NF_Age_Garage', 'NF_Age_Remod', 'NF_Remod_TEST', 'NF_TotalLivingArea_mul_OverallQual',\n",
    "                        'NF_TotalLivingArea_mul_OverallCond', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                        'NF_2ndFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt', 'LotFrontage', 'YearRemodAdd', 'NF_TotalBsmtSF_mul_BsmtFinType1', 'NF_Age_Build']\n",
    "box_cox_features = []\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "box_cox_transformer = BoxCoxTransformer(variables=box_cox_features)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])\n"
   ],
   "id": "115707dff3fb8f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features-Columns Post Transformations",
   "id": "a1b0b4a3f77735cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['GarageArea', 'LotArea', 'LotFrontage', 'TotalBsmtSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                      'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "# We will apply Winsorizer to features from table in jupyter_notebooks/08_Feature_Engineering_hypothesis_3.ipynb\n",
    "# The ones which gad high or above outliers\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n"
   ],
   "id": "e9422024e2a178f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Target Transformations",
   "id": "fc2526c4423ef565"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add a small constant to avoid log(0)\n",
    "        return np.log1p(np.clip(X, 0, None))\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Use expm1 for numerical stability, clip to avoid overflow\n",
    "        return np.expm1(np.clip(X, None, 700))  # 700 is chosen to avoid overflow in expm1\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline for transforming the target variable\n",
    "target_transformation_pipeline = Pipeline([\n",
    "    ('log_transform', LogTransformer()),  # Log transformation\n",
    "])\n",
    "\n",
    "\n",
    "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"A transformer that passes through the data without changing it.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for passthrough\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "\n",
    "# Create a pipeline for passthrough transformation\n",
    "passthrough_transformation_pipeline = Pipeline([\n",
    "    ('passthrough', PassthroughTransformer())  # Passthrough transformer\n",
    "])\n",
    "\n"
   ],
   "id": "6a79ddf1fab5c7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pipeline ",
   "id": "8e8e43659c64b622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "7a58bc2bddf07de0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ],
   "id": "84dc27d100d55d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class grid_cv_search_hp:\n",
    "    \"\"\"\n",
    "    Class to perform hyperparameter optimization across multiple machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        params (dict): Dictionary of hyperparameters for the models.\n",
    "        grid_searches (dict): Dictionary to store the results of GridSearchCV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params, target_transformer):\n",
    "        \"\"\"\n",
    "        Initializes the GridCvSearchHP with models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            models (dict): A dictionary of model names and instances.\n",
    "            params (dict): A dictionary of model names and their hyperparameters.\n",
    "            target_transformer: Transformer to apply to the target variable.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.grid_searches = {}\n",
    "        self.target_transformer = target_transformer\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=10, scoring='r2', refit=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization using GridSearchCV for each model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data features.\n",
    "            y (array-like): Training data target values.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "            n_jobs (int): Number of jobs to run in parallel.\n",
    "            verbose (int): Controls the verbosity of the output.\n",
    "            scoring (str): Scoring metric for model evaluation.\n",
    "            refit (bool): Whether to refit the best model on the whole dataset after searching.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for key in self.models:\n",
    "            print(f\"\\nOptimizing hyperparameters for {key}...\\n\")\n",
    "            model = create_pipeline(self.models[key], self.target_transformer)\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        \"\"\"\n",
    "        Summarize the grid search results.\n",
    "        \n",
    "        Args:\n",
    "            sort_by (str): The column to sort the results by.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A pandas DataFrame containing the summary of grid search results.\n",
    "            dict: The grid search results.\n",
    "        \"\"\"\n",
    "\n",
    "        def row(key, scores, params):\n",
    "            \"\"\"Creates a row for the summary dataframe.\"\"\"\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = f\"split{i}_test_score\"\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append(row(k, s, p))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns += [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches\n"
   ],
   "id": "bebb2cba05dcd497",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ],
   "id": "f9f113541180514c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, QuantileRegressor, \\\n",
    "    RANSACRegressor, Lars, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Initializing regression models with default parameters\n",
    "\n",
    "# Linear Models\n",
    "linear_models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(max_iter=100000),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'QuantileRegressor': QuantileRegressor(),\n",
    "    'RANSACRegressor': RANSACRegressor(),\n",
    "    'PLSRegression': PLSRegression(),\n",
    "    'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    'TheilSenRegressor': TheilSenRegressor()\n",
    "}\n",
    "\n",
    "# Tree-Based Models\n",
    "tree_based_models = {\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Gradient Boosting Frameworks\n",
    "gradient_boosting_models = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "    'LGBMRegressor': LGBMRegressor(),\n",
    "    'CatBoostRegressor': CatBoostRegressor()\n",
    "}\n",
    "\n",
    "# Support Vector Machines\n",
    "svm_models = {\n",
    "    'SVR': SVR(),\n",
    "    'NuSVR': NuSVR(),\n",
    "    'LinearSVR': LinearSVR(max_iter=100000)\n",
    "}\n",
    "\n",
    "# Nearest Neighbors\n",
    "nearest_neighbors_models = {\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'RadiusNeighborsRegressor': RadiusNeighborsRegressor(radius=20.0) # Uncomment if needed\n",
    "}\n",
    "\n",
    "# Bayesian Methods\n",
    "bayesian_models = {\n",
    "    'GaussianProcessRegressor': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Combining all models into a single dictionary for quick search\n",
    "models_quick_search = {\n",
    "    **linear_models,\n",
    "    **tree_based_models,\n",
    "    **gradient_boosting_models,\n",
    "    **svm_models,\n",
    "    **nearest_neighbors_models,\n",
    "    **bayesian_models\n",
    "}\n",
    "\n",
    "# Define an empty hyperparameter dictionary as all models will use default parameters initially\n",
    "params_quick_search = {model_name: {} for model_name in models_quick_search}\n"
   ],
   "id": "10c625d779cae244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running Grid Search CV",
   "id": "e61feca22f472f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_search = grid_cv_search_hp(models=models_quick_search, params=params_quick_search,\n",
    "                                   target_transformer=target_transformation_pipeline)\n",
    "initial_search.fit(X_train, y_train, cv=5, n_jobs=-1, scoring='r2', refit=False)"
   ],
   "id": "b7dde5fb9f8e9143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = initial_search.score_summary()\n",
    "grid_search_summary"
   ],
   "id": "f892ab114b9ea019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of Regressors by Category\n",
    "\n",
    "1. **Linear Models**\n",
    "   * BayesianRidge, 0.860079\n",
    "   * PLSRegression, 0.853426\n",
    "   * Ridge, 0.851585\n",
    "   * HuberRegressor, 0.845978\n",
    "   * LinearRegression, 0.842141\n",
    "   * RANSACRegressor, 0.795119\n",
    "   * TheilSenRegressor, 0.285762\n",
    "   * Lasso, -0.036293\n",
    "   * ElasticNet, -0.036293\n",
    "   * QuantileRegressor, -0.055394\n",
    "\n",
    "2. **Tree-Based Models**\n",
    "   * HistGradientBoostingRegressor, 0.858713\n",
    "   * RandomForestRegressor, 0.853213\n",
    "   * ExtraTreesRegressor, 0.843201\n",
    "   * GradientBoostingRegressor, 0.833988\n",
    "   * AdaBoostRegressor, 0.793819\n",
    "   * DecisionTreeRegressor, 0.698089\n",
    "\n",
    "3. **Gradient Boosting Frameworks**\n",
    "   * CatBoostRegressor, 0.856161\n",
    "   * LGBMRegressor, 0.851544\n",
    "   * XGBRegressor, 0.828049\n",
    "\n",
    "4. **Support Vector Machines**\n",
    "   * LinearSVR, 0.846725\n",
    "   * NuSVR, 0.842756\n",
    "   * SVR, 0.827245\n",
    "\n",
    "5. **Nearest Neighbors**\n",
    "   * KNeighborsRegressor, 0.800393\n",
    "\n",
    "6. **Bayesian Methods**\n",
    "   * GaussianProcessRegressor, -6.150232\n",
    "\n",
    "We can see that the top-performing models are from the BayesianRidge, HistGradientBoostingRegressor, and CatBoostRegressor categories.\n",
    "\n",
    "\n",
    "## Regressors Exploration\n",
    "\n",
    "### BayesianRidge"
   ],
   "id": "951d2801cb8ac903"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": {\n",
    "        'model__regressor__max_iter': [300, 600, 1000],  # Number of iterations for convergence. Increasing iterations can improve accuracy but increase computation time.\n",
    "        'model__regressor__tol': [1e-3, 1e-4, 1e-5],  # Tolerance for the stopping criteria. Lower values lead to more precise convergence but longer training times.\n",
    "        'model__regressor__alpha_1': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__alpha_2': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_1': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_2': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "    }\n",
    "}\n"
   ],
   "id": "be6a96de159576e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_br = grid_cv_search_hp(models=models_tune_search_bayesianridge, params=params_tune_search_bayesianridge,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_br.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "728e5a031f37dbfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_br, models_tune_pipelines_br = search_tuned_br.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_br"
   ],
   "id": "df377f860c1c1371",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_br, 'models/hypothesis_3/models_tune_pipelines_br.pkl')\n",
    "models_tune_summary_br.to_csv('models/hypothesis_3/models_tune_summary_br.csv', index=False)"
   ],
   "id": "40853e6cacf6776f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_br = models_tune_summary_br.iloc[0]['estimator']\n",
    "best_model_br"
   ],
   "id": "57f9b1393eff327e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_br = models_tune_pipelines_br[best_model_br].best_params_\n",
    "best_parameters_br"
   ],
   "id": "3e20e0cf811668cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__alpha_1': 1e-06,\n",
    " 'model__regressor__alpha_2': 0.0001,\n",
    " 'model__regressor__lambda_1': 0.0001,\n",
    " 'model__regressor__lambda_2': 1e-06,\n",
    " 'model__regressor__max_iter': 300,\n",
    " 'model__regressor__tol': 1e-05}"
   ],
   "id": "5d546372437c8c9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_br = models_tune_pipelines_br[best_model_br].best_estimator_\n",
    "best_pipeline_br"
   ],
   "id": "6e96b3a92a692f63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accessing Feature Importance",
   "id": "706eb93fbbf0958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def plot_feature_importance_absolute(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Plot the absolute feature importance from a given pipeline.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the sub-pipeline does not support the transform operation.\n",
    "        ValueError: If there is a mismatch in feature support mask length and transformed features.\n",
    "    \"\"\"\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances\n",
    "        }).sort_values(by=importance_type, ascending=False)\n",
    "\n",
    "        # Plotting the feature importances or coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=importance_type, y='Feature', data=feature_importances_df)\n",
    "        plt.xlabel(importance_type.capitalize())\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Feature {importance_type.capitalize()}s')\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ],
   "id": "caadadf3af8b501f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_br)",
   "id": "1c1415ea6682ca73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's get all Features sorted by their importance",
   "id": "1c5ebd0aeed212e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_sorted_feature_importances(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Retrieve and return the feature importances or coefficients from a given pipeline, sorted by absolute value.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features and their absolute importances or coefficients, sorted by absolute value.\n",
    "    \"\"\"\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances,\n",
    "            f'abs_{importance_type}': np.abs(importances)\n",
    "        }).sort_values(by=f'abs_{importance_type}', ascending=False)\n",
    "\n",
    "        return feature_importances_df[['Feature', importance_type, f'abs_{importance_type}']]\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "id": "77baf6cf9a876bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_br)",
   "id": "a9b3443a54712d8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a regression model on both the training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "    return (r2_train, mae_train, mse_train, rmse_train, msle_train), (r2_test, mae_test, mse_test, rmse_test, msle_test)\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates a regression model on a given dataset and prints key metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Data features.\n",
    "        y (array-like): Data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    prediction = pipeline.predict(X)\n",
    "    r2 = r2_score(y, prediction)\n",
    "    mae = mean_absolute_error(y, prediction)\n",
    "    mse = mean_squared_error(y, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    msle = mean_squared_log_error(y, prediction)\n",
    "\n",
    "\n",
    "    return r2, mae, mse, rmse, msle\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    \"\"\"\n",
    "    Plots actual vs predicted values for both training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "        alpha_scatter (float): Transparency of the scatter plot points.\n",
    "    \"\"\"\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "    # Train set evaluation\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    # Test set evaluation\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "    # Train plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0, 0], color='blue')\n",
    "    axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 0].set_ylabel(\"Predictions\")\n",
    "    axes[0, 0].set_title(\"Train Set: Actual vs Predicted\")\n",
    "    train_metrics_text = (f'R2: {round(r2_train, 3)}\\n'\n",
    "                          f'MAE: {round(mae_train, 3)}\\n'\n",
    "                          f'MSE: {round(mse_train, 3)}\\n'\n",
    "                          f'RMSE: {round(rmse_train, 3)}\\n'\n",
    "                          f'MSLE: {round(msle_train, 3)}')\n",
    "    axes[0, 0].text(0.05, 0.95, train_metrics_text, transform=axes[0, 0].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Test plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[0, 1], color='green')\n",
    "    axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 1].set_ylabel(\"Predictions\")\n",
    "    axes[0, 1].set_title(\"Test Set: Actual vs Predicted\")\n",
    "    test_metrics_text = (f'R2: {round(r2_test, 3)}\\n'\n",
    "                         f'MAE: {round(mae_test, 3)}\\n'\n",
    "                         f'MSE: {round(mse_test, 3)}\\n'\n",
    "                         f'RMSE: {round(rmse_test, 3)}\\n'\n",
    "                         f'MSLE: {round(msle_test, 3)}')\n",
    "    axes[0, 1].text(0.05, 0.95, test_metrics_text, transform=axes[0, 1].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Residuals plot: Train\n",
    "    residuals_train = y_train - pred_train\n",
    "    sns.scatterplot(x=pred_train, y=residuals_train, alpha=alpha_scatter, ax=axes[1, 0], color='blue')\n",
    "    axes[1, 0].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel(\"Predictions\")\n",
    "    axes[1, 0].set_ylabel(\"Residuals\")\n",
    "    axes[1, 0].set_title(\"Train Set: Residuals\")\n",
    "\n",
    "    # Residuals plot: Test\n",
    "    residuals_test = y_test - pred_test\n",
    "    sns.scatterplot(x=pred_test, y=residuals_test, alpha=alpha_scatter, ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel(\"Predictions\")\n",
    "    axes[1, 1].set_ylabel(\"Residuals\")\n",
    "    axes[1, 1].set_title(\"Test Set: Residuals\")\n",
    "\n",
    "    # Error distribution plot: Train\n",
    "    sns.histplot(residuals_train, kde=True, ax=axes[1, 2], color='blue')\n",
    "    axes[1, 2].set_xlabel(\"Residuals\")\n",
    "    axes[1, 2].set_ylabel(\"Frequency\")\n",
    "    axes[1, 2].set_title(\"Train Set: Error Distribution\")\n",
    "\n",
    "    # Error distribution plot: Test\n",
    "    sns.histplot(residuals_test, kde=True, ax=axes[0, 2], color='green')\n",
    "    axes[0, 2].set_xlabel(\"Residuals\")\n",
    "    axes[0, 2].set_ylabel(\"Frequency\")\n",
    "    axes[0, 2].set_title(\"Test Set: Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "5c06f06129ade337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_br)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_br)"
   ],
   "id": "376a2219a4d56ad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OK, we have some quote promising results:\n",
    "* Train R<sup>2</sup> = 0.861\n",
    "* Test R<sup>2</sup> = 0.778\n",
    "\n",
    "### PLSRegression"
   ],
   "id": "2a190acb5efe8c53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_pls = {\n",
    "    \"PLSRegression\": PLSRegression(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_pls = {\n",
    "    \"PLSRegression\": {\n",
    "        'model__regressor__n_components': [2, 5, 10, 15, 20],  # Number of components to keep.\n",
    "        'model__regressor__max_iter': [300, 600, 1000, 2000],  # Maximum number of iterations for the algorithm to converge.\n",
    "        'model__regressor__tol': [1e-6, 1e-7, 1e-8, 1e-9],  # Tolerance for the stopping criteria.\n",
    "    }\n",
    "}\n"
   ],
   "id": "7281141623cc9caa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_pls = grid_cv_search_hp(models=models_tune_search_pls, params=params_tune_search_pls,\n",
    "                                    target_transformer=target_transformation_pipeline)\n",
    "search_tuned_pls.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "48352152bfa6c27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_pls, models_tune_pipelines_pls = search_tuned_pls.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_pls"
   ],
   "id": "6645f0af35b752da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_pls, 'models/hypothesis_3/models_tune_pipelines_pls.pkl')\n",
    "models_tune_summary_pls.to_csv('models/hypothesis_3/models_tune_summary_pls.csv', index=False)"
   ],
   "id": "ad8a212f36487ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_pls = models_tune_summary_pls.iloc[0]['estimator']\n",
    "best_model_pls"
   ],
   "id": "802f7770878c0e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_pls = models_tune_pipelines_pls[best_model_pls].best_params_\n",
    "best_parameters_pls"
   ],
   "id": "ecd2823ffb39f7de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__max_iter': 300,\n",
    " 'model__regressor__n_components': 10,\n",
    " 'model__regressor__tol': 1e-06}"
   ],
   "id": "52841eee2a68fe75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_pls = models_tune_pipelines_pls[best_model_pls].best_estimator_\n",
    "best_pipeline_pls"
   ],
   "id": "5d67fe2e11ca84f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_pls)",
   "id": "e6ffc3aefe2342e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_pls)",
   "id": "4a7ef12801ad6054",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_pls)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_pls)"
   ],
   "id": "f85458ab30fbdc0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have Results:\n",
    "\n",
    "* Train R<sup>2</sup> = 0.878\n",
    "* Test R<sup>2</sup> = 0.74\n",
    "\n",
    "### HistGradientBoostingRegressor"
   ],
   "id": "15d92712e1bdbf8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_hgb = {\n",
    "    \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_hgb = {\n",
    "    \"HistGradientBoostingRegressor\": {\n",
    "        'model__regressor__learning_rate': [0.01, 0.1, 0.3],  # Learning rate shrinks the contribution of each tree. Lower values make the model more robust to overfitting, but require more trees.\n",
    "        'model__regressor__max_iter': [100, 250, 600],  # Maximum number of iterations (trees). More iterations can improve performance but increase computation time and risk of overfitting.\n",
    "        'model__regressor__max_leaf_nodes': [31, 63],  # Maximum number of leaves per tree. More leaves can capture more information but increase risk of overfitting and computation cost.\n",
    "        'model__regressor__max_depth': [None, 10, 20],  # Maximum depth of each tree. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__min_samples_leaf': [20, 50, 100],  # Minimum number of samples required at a leaf node. Higher values prevent overfitting by ensuring leaves contain more samples.\n",
    "        'model__regressor__l2_regularization': [0.0, 0.1, 1.0],  # L2 regularization term. Higher values penalize large weights, helping to prevent overfitting.\n",
    "        'model__regressor__max_bins': [255, 511],  # Maximum number of bins for discretizing continuous features. More bins can capture more detail but increase memory and computation cost.\n",
    "    }\n",
    "}\n"
   ],
   "id": "27971eb779e64d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_hgb = grid_cv_search_hp(models=models_tune_search_hgb, params=params_tune_search_hgb,\n",
    "                                    target_transformer=target_transformation_pipeline)\n",
    "search_tuned_hgb.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "1d094e630682f276",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_hgb, models_tune_pipelines_hgb = search_tuned_hgb.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_hgb"
   ],
   "id": "5082e7505cef3398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_hgb, 'models/hypothesis_3/models_tune_pipelines_hgb.pkl')\n",
    "models_tune_summary_hgb.to_csv('models/hypothesis_3/models_tune_summary_hgb.csv', index=False)"
   ],
   "id": "a774830578c1c6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_hgb = models_tune_summary_hgb.iloc[0]['estimator']\n",
    "best_model_hgb"
   ],
   "id": "adea51a03ab55589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_hgb = models_tune_pipelines_hgb[best_model_hgb].best_params_\n",
    "best_parameters_hgb"
   ],
   "id": "44d1648c1c88e62a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__l2_regularization': 0.1,\n",
    " 'model__regressor__learning_rate': 0.1,\n",
    " 'model__regressor__max_bins': 255,\n",
    " 'model__regressor__max_depth': None,\n",
    " 'model__regressor__max_iter': 100,\n",
    " 'model__regressor__max_leaf_nodes': 31,\n",
    " 'model__regressor__min_samples_leaf': 50}"
   ],
   "id": "741d2bb34a6c6bcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_hgb = models_tune_pipelines_hgb[best_model_hgb].best_estimator_\n",
    "best_pipeline_hgb"
   ],
   "id": "6ff036b9f77b529b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_hgb)",
   "id": "363a3845f9ee995f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_hgb)",
   "id": "d01e8408ba50f369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_hgb)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_hgb)"
   ],
   "id": "c72535b2d1de4e0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have Results:\n",
    "* Train R<sup>2</sup> = 0.944\n",
    "* Test R<sup>2</sup> = 0.824\n",
    "\n",
    "### RandomForestRegressor"
   ],
   "id": "977edcde32da81ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_rf = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_rf = {\n",
    "    \"RandomForestRegressor\": {\n",
    "        'model__regressor__n_estimators': [100, 200, 500],  # The number of trees in the forest. More trees can improve performance but increase computation time.\n",
    "        'model__regressor__max_features': ['auto', 'sqrt', 'log2'],  # The number of features to consider when looking for the best split. 'auto' uses sqrt(n_features).\n",
    "        'model__regressor__max_depth': [None, 10, 20, 30],  # Maximum depth of the tree. None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\n",
    "        'model__regressor__min_samples_split': [2, 10, 20],  # Minimum number of samples required to split an internal node. Higher values can prevent overfitting.\n",
    "        'model__regressor__min_samples_leaf': [1, 5, 10],  # Minimum number of samples required to be at a leaf node. Higher values can help prevent overfitting.\n",
    "        'model__regressor__random_state': [42],  # Seed used by the random number generator. Ensures reproducibility.\n",
    "    }\n",
    "}"
   ],
   "id": "db2663a76bdf82fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_rf = grid_cv_search_hp(models=models_tune_search_rf, params=params_tune_search_rf,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_rf.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "98b23469a1e52c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_rf, models_tune_pipelines_rf = search_tuned_rf.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_rf"
   ],
   "id": "3e99a12400af238",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_rf, 'models/hypothesis_3/models_tune_pipelines_rf.pkl')\n",
    "models_tune_summary_rf.to_csv('models/hypothesis_3/models_tune_summary_rf.csv', index=False)"
   ],
   "id": "856685e72442ad8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_rf = models_tune_summary_rf.iloc[0]['estimator']\n",
    "best_model_rf"
   ],
   "id": "5aca9684f0ae7ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_rf = models_tune_pipelines_rf[best_model_rf].best_params_\n",
    "best_parameters_rf"
   ],
   "id": "70de9451444c9992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_rf = models_tune_pipelines_rf[best_model_rf].best_estimator_\n",
    "best_pipeline_rf"
   ],
   "id": "ef49572e7cbb254c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_rf)",
   "id": "5d6a619b98a66e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_rf)",
   "id": "780748876a852126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_rf)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_rf)"
   ],
   "id": "6aaffbe888bd7b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup> = 0.978\n",
    "* Test R<sup>2</sup> = 0.84\n",
    "\n",
    "### CatBoostRegressor"
   ],
   "id": "39ef46f98888e995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": {\n",
    "        'model__regressor__iterations': [500, 1000, 1500],  # Number of trees. More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__learning_rate': [0.001, 0.01, 0.1],  # Learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__depth': [4, 6, 8],  # Depth of the trees. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__l2_leaf_reg': [1, 3, 5],  # L2 regularization term. Higher values prevent overfitting by penalizing large weights.\n",
    "        'model__regressor__bagging_temperature': [0.0, 0.5, 1.0],  # Controls the variance of bagging. Higher values increase the randomization.\n",
    "        #'model__regressor__border_count': [32, 64, 128],  # Number of splits for numerical features. Higher values capture more detail but increase computation.\n",
    "        'model__regressor__random_strength': [1, 2, 5],  # Amount of randomness for scoring splits. Higher values add more randomness, helping to prevent overfitting.\n",
    "        #'model__regressor__rsm': [0.8, 0.9, 1.0],  # Random subspace method. Fraction of features to use at each split. Lower values reduce overfitting.\n",
    "    }\n",
    "}\n"
   ],
   "id": "e21d9d9500c06e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "1e75c0f058d92bc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "id": "fc62b85d28924d3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_catboost, 'models/hypothesis_3/models_tune_pipelines_catboost.pkl')\n",
    "models_tune_summary_catboost.to_csv('models/hypothesis_3/models_tune_summary_catboost.csv', index=False)"
   ],
   "id": "ccc1b5aaa5ace792",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_catboost = models_tune_summary_catboost.iloc[0]['estimator']\n",
    "best_model_catboost"
   ],
   "id": "d2a695cc168c7c4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "id": "ab3f00a75393c65f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "id": "468f05f5bea7249e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_catboost)",
   "id": "2520af2a8bf02d66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_catboost)",
   "id": "c22edf0f53756014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "id": "1df9033d13f7aff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "*Train R<sup>2</sup > =\n",
    "*Test\n",
    "R <sup>2</sup> = \n",
    "\n",
    "### LGBMRegressor\n"
   ],
   "id": "21292daecd6ccbe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_lgbm = {\n",
    "    \"LGBMRegressor\": LGBMRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with the selected hyperparameters\n",
    "params_tune_search_lgbm = {\n",
    "    \"LGBMRegressor\": {\n",
    "        'model__regressor__num_leaves': [31, 50, 70],  # Maximum number of leaves in one tree. More leaves can increase model complexity and accuracy.\n",
    "        'model__regressor__max_depth': [-1, 10, 20],  # Maximum depth of the tree. -1 means no limit. Controls the complexity of the model.\n",
    "        'model__regressor__learning_rate': [0.01, 0.05, 0.1],  # Boosting learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__n_estimators': [100, 200, 500],  # Number of boosting iterations (trees). More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__min_child_samples': [20, 50, 100],  # Minimum number of data points needed in a leaf. Helps prevent overfitting.\n",
    "        'model__regressor__subsample': [0.6, 0.8, 1.0],  # Subsample ratio of the training instance. Prevents overfitting by randomly sampling a subset of data.\n",
    "    }\n",
    "}\n"
   ],
   "id": "d3cb90f017ba1c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_lgbm = grid_cv_search_hp(models=models_tune_search_lgbm, params=params_tune_search_lgbm,\n",
    "                                      target_transformer=target_transformation_pipeline)\n",
    "search_tuned_lgbm.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "6e0c26ee9983fe3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_lgbm, models_tune_pipelines_lgbm = search_tuned_lgbm.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_lgbm"
   ],
   "id": "3d21fb7d49be9514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_lgbm, 'models/hypothesis_3/models_tune_pipelines_lgbm.pkl')\n",
    "models_tune_summary_lgbm.to_csv('models/hypothesis_3/models_tune_summary_lgbm.csv', index=False)"
   ],
   "id": "f5c48d0605a18060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_lgbm = models_tune_summary_lgbm.iloc[0]['estimator']\n",
    "best_model_lgbm"
   ],
   "id": "60e7a0b683968ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_lgbm = models_tune_pipelines_lgbm[best_model_lgbm].best_params_\n",
    "best_parameters_lgbm"
   ],
   "id": "40a2b377cfa4cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__learning_rate': 0.05,\n",
    " 'model__regressor__max_depth': 10,\n",
    " 'model__regressor__min_child_samples': 50,\n",
    " 'model__regressor__n_estimators': 200,\n",
    " 'model__regressor__num_leaves': 31,\n",
    " 'model__regressor__subsample': 0.6}"
   ],
   "id": "66b91443a2fafd56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_lgbm = models_tune_pipelines_lgbm[best_model_lgbm].best_estimator_\n",
    "best_pipeline_lgbm"
   ],
   "id": "9d5ed34b4ba5fe0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_lgbm)",
   "id": "76b65e0bafbec041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_lgbm)",
   "id": "367e93d07d003c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_lgbm)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_lgbm)"
   ],
   "id": "26d6e08a45fa6066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "*Train R <sup>2</sup> =0.942\n",
    "*Test R<sup>2</sup> =0.827\n",
    "\n",
    "### LinearSVR"
   ],
   "id": "c215bd46caea4efd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_linearsvr = {\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_linearsvr = {\n",
    "    \"LinearSVR\": {\n",
    "        'model__regressor__C': [0.1, 1, 10],  # Regularization parameter. Higher values mean less regularization.\n",
    "        'model__regressor__epsilon': [0.1, 0.2, 0.5],  # Epsilon-tube within which no penalty is associated with the training loss.\n",
    "        'model__regressor__tol': [1e-4, 1e-5, 1e-6],  # Tolerance for stopping criteria. Lower values mean more precise convergence.\n",
    "        'model__regressor__max_iter': [100000, 200000, 300000],  # Maximum number of iterations. Higher values ensure convergence but increase computation time.\n",
    "        'model__regressor__loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],  # Specifies the loss function.\n",
    "        'model__regressor__dual': [True, False],  # Dual or primal formulation. Dual is preferred when n_samples > n_features.\n",
    "    }\n",
    "}\n"
   ],
   "id": "2f358aa50295a7d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_linearsvr = grid_cv_search_hp(models=models_tune_search_linearsvr, params=params_tune_search_linearsvr,\n",
    "                                           target_transformer=target_transformation_pipeline)\n",
    "search_tuned_linearsvr.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "edc4198f0645d1b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_linearsvr, models_tune_pipelines_linearsvr = search_tuned_linearsvr.score_summary(\n",
    "    sort_by='mean_score')\n",
    "models_tune_summary_linearsvr"
   ],
   "id": "14b44ca5ed55e45c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_linearsvr, 'models/hypothesis_3/models_tune_pipelines_linearsvr.pkl')\n",
    "models_tune_summary_linearsvr.to_csv('models/hypothesis_3/models_tune_summary_linearsvr.csv', index=False)"
   ],
   "id": "ed5332c6f5586071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_linearsvr = models_tune_summary_linearsvr.iloc[0]['estimator']\n",
    "best_model_linearsvr"
   ],
   "id": "9bbd1f27a0ba73d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_linearsvr = models_tune_pipelines_linearsvr[best_model_linearsvr].best_params_\n",
    "best_parameters_linearsvr"
   ],
   "id": "9a5fe91f2f2aec47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__C': 1,\n",
    " 'model__regressor__dual': True,\n",
    " 'model__regressor__epsilon': 0.1,\n",
    " 'model__regressor__loss': 'epsilon_insensitive',\n",
    " 'model__regressor__max_iter': 300000,\n",
    " 'model__regressor__tol': 1e-06}"
   ],
   "id": "d3107ce426d80579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_linearsvr = models_tune_pipelines_linearsvr[best_model_linearsvr].best_estimator_\n",
    "best_pipeline_linearsvr"
   ],
   "id": "b4328a1177a95bc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_linearsvr)",
   "id": "3fd3f21d8147fdfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_linearsvr)",
   "id": "35a1e161a620e040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_linearsvr)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_linearsvr)"
   ],
   "id": "5af848eb43644ee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup> =0.8869\n",
    "* Test R<sup>2</sup> =0.53\n",
    "\n",
    "### NuSVFR"
   ],
   "id": "408af04eb3837717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import NuSVR\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_nusvr = {\n",
    "    \"NuSVR\": NuSVR(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_nusvr = {\n",
    "    \"NuSVR\": {\n",
    "        'model__regressor__nu': [0.1, 0.5, 0.9],  # An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors.\n",
    "        'model__regressor__C': [0.1, 1, 10],  # Regularization parameter. Higher values mean less regularization.\n",
    "        'model__regressor__kernel': ['linear', 'poly', 'rbf'],  # Specifies the kernel type to be used in the algorithm.\n",
    "        'model__regressor__degree': [2, 3, 4],  # Degree of the polynomial kernel function (poly). Ignored by all other kernels.\n",
    "        'model__regressor__tol': [1e-3, 1e-4, 1e-5],  # Tolerance for stopping criteria. Lower values mean more precise convergence.\n",
    "        'model__regressor__max_iter': [100000, 200000, 300000],  # Maximum number of iterations. Higher values ensure convergence but increase computation time.\n",
    "    }\n",
    "}\n"
   ],
   "id": "d074bb68ebd5b35a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_nusvr = grid_cv_search_hp(models=models_tune_search_nusvr, params=params_tune_search_nusvr,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_nusvr.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "4eab9c41990c47ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_nusvr, models_tune_pipelines_nusvr = search_tuned_nusvr.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_nusvr"
   ],
   "id": "bd4f7df8604eaacd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_nusvr, 'models/hypothesis_3/models_tune_pipelines_nusvr.pkl')\n",
    "models_tune_summary_nusvr.to_csv('models/hypothesis_3/models_tune_summary_nusvr.csv', index=False)"
   ],
   "id": "2573e68fec7d6e6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_nusvr = models_tune_summary_nusvr.iloc[0]['estimator']\n",
    "best_model_nusvr"
   ],
   "id": "daf0be4992272345",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_nusvr = models_tune_pipelines_nusvr[best_model_nusvr].best_params_\n",
    "best_parameters_nusvr"
   ],
   "id": "115f83542f51106d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__C': 0.1,\n",
    " 'model__regressor__degree': 2,\n",
    " 'model__regressor__kernel': 'linear',\n",
    " 'model__regressor__max_iter': 100000,\n",
    " 'model__regressor__nu': 0.1,\n",
    " 'model__regressor__tol': 1e-05}"
   ],
   "id": "c44aa255758c3b29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_nusvr = models_tune_pipelines_nusvr[best_model_nusvr].best_estimator_\n",
    "best_pipeline_nusvr"
   ],
   "id": "ec08ebb9d6618e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_nusvr)",
   "id": "8f58a5793ce4b9eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_nusvr)",
   "id": "6772f1ea21be7b3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_nusvr)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_nusvr)"
   ],
   "id": "e556bc4086d02f1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup > =0.862\n",
    "* Test R<sup>2</sup> = 0.825\n",
    "\n",
    "### KNeighborsRegressor"
   ],
   "id": "ce3fbc9a22d4de09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_knn = {\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_knn = {\n",
    "    \"KNeighborsRegressor\": {\n",
    "        'model__regressor__n_neighbors': [3, 5, 7],  # Number of neighbors to use. More neighbors can smooth out predictions but might include more noise.\n",
    "        'model__regressor__weights': ['uniform', 'distance'],  # Weight function used in prediction. 'uniform' uses equal weights, 'distance' uses the inverse of distance.\n",
    "        'model__regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors.\n",
    "        'model__regressor__leaf_size': [30, 50, 70],  # Leaf size passed to BallTree or KDTree. Affects the speed of the construction and query.\n",
    "        'model__regressor__p': [1, 2],  # Power parameter for the Minkowski metric. 1 is equivalent to Manhattan distance, 2 is equivalent to Euclidean distance.\n",
    "        'model__regressor__metric': ['minkowski', 'euclidean', 'manhattan'],  # The distance metric to use for the tree.\n",
    "    }\n",
    "}\n"
   ],
   "id": "d8e0fad06dfce3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_knn = grid_cv_search_hp(models=models_tune_search_knn, params=params_tune_search_knn,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_knn.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "f3511813feb3cdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_knn, models_tune_pipelines_knn = search_tuned_knn.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_knn"
   ],
   "id": "34ab51248737bcd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_knn, 'models/hypothesis_3/models_tune_pipelines_knn.pkl')\n",
    "models_tune_summary_knn.to_csv('models/hypothesis_3/models_tune_summary_knn.csv', index=False)"
   ],
   "id": "b5eb846a1985ee16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_knn = models_tune_summary_knn.iloc[0]['estimator']\n",
    "best_model_knn"
   ],
   "id": "c5ae287b79ea98cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_knn = models_tune_pipelines_knn[best_model_knn].best_params_\n",
    "best_parameters_knn"
   ],
   "id": "81b7bf5273ab2460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_knn = models_tune_pipelines_knn[best_model_knn].best_estimator_\n",
    "best_pipeline_knn"
   ],
   "id": "d097b9f3bdac7f61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_knn)",
   "id": "e4d3c5670911672c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_knn)",
   "id": "516013313a51c693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_knn)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_knn)"
   ],
   "id": "a4f58cda8b989b1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have results:\n",
    "*Train R<sup>2</sup > =\n",
    "*Test R<sup>2</sup> = \n",
    "\n",
    "## Regressors Review"
   ],
   "id": "59178ff2d383bbdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d6a76cd10e4dd159",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
