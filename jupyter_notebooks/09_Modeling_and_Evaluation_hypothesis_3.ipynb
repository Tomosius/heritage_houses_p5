{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aaf88d26c42c1f5",
   "metadata": {},
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a72d0651596244",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "id": "84e53adb39c40ff0",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "624018c3aea597b3",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0307cd1e623595e",
   "metadata": {},
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4e467105bdd1e91",
   "metadata": {},
   "source": [
    "Confirm new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "45512d37254c92ce",
   "metadata": {},
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3789f66254e08f44",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dd310e3dcb6619f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce222f2402f6e5f",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f80b313366f049f6",
   "metadata": {},
   "source": [
    "drop_features = ['Unnamed: 0']\n",
    "df.drop(columns=drop_features, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f742e59c8c0be550",
   "metadata": {},
   "source": [
    "### Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "4afe74b356e994b8",
   "metadata": {},
   "source": [
    "df.loc[:, 'LotFrontage'] = df['LotFrontage'].fillna(70)\n",
    "\n",
    "# Lists of columns grouped by their fill values and type conversions\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageArea', 'GarageYrBlt',\n",
    "                         'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF', 'BedroomAbvGr']\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "df['LotFrontage'] = df['LotFrontage'].round().astype(int)\n",
    "\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"BsmtFinSF1\": 0, \"BsmtUnfSF\": 0,\n",
    "                       \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Dropping new created column Consistency\n",
    "df = df.drop(columns=['Consistency'])\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtUnfSF'] + df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27600268ed420431",
   "metadata": {},
   "source": [
    "## Splitting to data and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b05199b68ab479a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='SalePrice')\n",
    "y = df['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3077ebeb6a8d911",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Pre-Transformations"
   ]
  },
  {
   "cell_type": "code",
   "id": "1733f60ad69b2844",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize the feature creation descriptions\n",
    "        self.feature_creation_descriptions_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # New features descriptions to be presented in Pipeline i\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalBsmtSF_mul_BsmtExposure': 'TotalBsmtSF * BsmtExposure',\n",
    "            'NF_TotalBsmtSF_mul_BsmtFinType1': 'TotalBsmtSF * BsmtFinType1',\n",
    "            'NF_BsmtFinSF1_mul_BsmtFinType1': 'BsmtFinType1 * BsmtFinType1',\n",
    "            'NF_GarageFinish_mul_GarageArea': 'GarageFinish * GarageArea',\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "            'NF_2ndFlrSF_mul_OverallQual': '2ndFlrSF * OverallQual',\n",
    "            'NF_Age_Garage': '2010 - GarageYrBlt',\n",
    "            'NF_Age_Build': '2010 - YearBuilt',\n",
    "            'NF_Age_Remod': '2010 - YearRemodAdd',\n",
    "            'NF_Remod_TEST': '0 if NF_Age_Build == NF_Age_Remod else NF_Age_Remod',\n",
    "            'NF_Has_2nd_floor': '1 if 2ndFlrSF > 0 else 0',\n",
    "            'NF_Has_basement': '1 if TotalBsmtSF > 0 else 0',\n",
    "            'NF_Has_garage': '1 if GarageArea > 0 else 0',\n",
    "            'NF_Has_Masonry_Veneer': '1 if MasVnrArea > 0 else 0',\n",
    "            'NF_Has_Enclosed_Porch': '1 if EnclosedPorch > 0 else 0',\n",
    "            'NF_Has_Open_Porch': '1 if OpenPorchSF > 0 else 0',\n",
    "            'NF_Has_ANY_Porch': 'NF_Has_Enclosed_Porch | NF_Has_Open_Porch',\n",
    "            'NF_Has_Wooden_Deck': '1 if WoodDeckSF > 0 else 0'\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalBsmtSF_mul_BsmtExposure'] = X['TotalBsmtSF'] * X['BsmtExposure']\n",
    "        X['NF_TotalBsmtSF_mul_BsmtFinType1'] = X['TotalBsmtSF'] * X['BsmtFinType1']\n",
    "        X['NF_BsmtFinSF1_mul_BsmtFinType1'] = X['BsmtFinType1'] * X['BsmtFinType1']\n",
    "        X['NF_GarageFinish_mul_GarageArea'] = X['GarageFinish'] * X['GarageArea']\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "        X['NF_2ndFlrSF_mul_OverallQual'] = X['2ndFlrSF'] * X['OverallQual']\n",
    "        X['NF_Age_Garage'] = 2010 - X['GarageYrBlt']\n",
    "        X['NF_Age_Build'] = 2010 - X['YearBuilt']\n",
    "        X['NF_Age_Remod'] = 2010 - X['YearRemodAdd']\n",
    "        X['NF_Remod_TEST'] = X.apply(\n",
    "            lambda row: 0 if row['NF_Age_Build'] == row['NF_Age_Remod'] else row['NF_Age_Remod'], axis=1)\n",
    "        X[('NF_Has_2nd_floor')] = X.apply(lambda row: False if row['2ndFlrSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_basement')] = X.apply(lambda row: False if row['TotalBsmtSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_garage')] = X.apply(lambda row: False if row['GarageArea'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_Masonry_Veneer')] = X.apply(lambda row: False if row['MasVnrArea'] == 0 else True, axis=1).astype(\n",
    "            int)\n",
    "        X[('NF_Has_Enclosed_Porch')] = X.apply(lambda row: False if row['EnclosedPorch'] == 0 else True,\n",
    "                                               axis=1).astype(int)\n",
    "        X[('NF_Has_Open_Porch')] = X.apply(lambda row: False if row['OpenPorchSF'] == 0 else True, axis=1).astype(int)\n",
    "        X['NF_Has_ANY_Porch'] = X['NF_Has_Enclosed_Porch'] | X['NF_Has_Open_Porch'].astype(int)\n",
    "        X[('NF_Has_Wooden_Deck')] = X.apply(lambda row: False if row['WoodDeckSF'] == 0 else True, axis=1).astype(int)\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join(\n",
    "            [f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "daecbf2114709a09",
   "metadata": {},
   "source": [
    "### Features - Columns transformations"
   ]
  },
  {
   "cell_type": "code",
   "id": "115707dff3fb8f59",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, BoxCoxTransformer, PowerTransformer\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtUnfSF', 'EnclosedPorch',\n",
    "                        'GarageArea', 'GarageFinish', 'GrLivArea', 'KitchenQual', 'LotArea', 'MasVnrArea',\n",
    "                        'OpenPorchSF', 'OverallCond', 'TotalBsmtSF', 'WoodDeckSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                        'NF_BsmtFinSF1_mul_BsmtFinType1', 'NF_GarageFinish_mul_GarageArea', 'NF_TotalLivingArea',\n",
    "                        'NF_Age_Garage', 'NF_Age_Remod', 'NF_Remod_TEST', 'NF_TotalLivingArea_mul_OverallQual',\n",
    "                        'NF_TotalLivingArea_mul_OverallCond', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                        'NF_2ndFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt', 'LotFrontage', 'YearRemodAdd', 'NF_TotalBsmtSF_mul_BsmtFinType1', 'NF_Age_Build']\n",
    "box_cox_features = []\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "box_cox_transformer = BoxCoxTransformer(variables=box_cox_features)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1b0b4a3f77735cf",
   "metadata": {},
   "source": [
    "### Features-Columns Post Transformations"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9422024e2a178f8",
   "metadata": {},
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['GarageArea', 'LotArea', 'LotFrontage', 'TotalBsmtSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                      'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "# We will apply Winsorizer to features from table in jupyter_notebooks/08_Feature_Engineering_hypothesis_3.ipynb\n",
    "# The ones which gad high or above outliers\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc2526c4423ef565",
   "metadata": {},
   "source": [
    "### Target Transformations"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a79ddf1fab5c7be",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add a small constant to avoid log(0)\n",
    "        return np.log1p(np.clip(X, 0, None))\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Use expm1 for numerical stability, clip to avoid overflow\n",
    "        return np.expm1(np.clip(X, None, 700))  # 700 is chosen to avoid overflow in expm1\n",
    "\n",
    "\n",
    "# Create a pipeline for transforming the target variable\n",
    "target_transformation_pipeline = Pipeline([\n",
    "    ('log_transform', LogTransformer()),  # Log transformation\n",
    "])\n",
    "\n",
    "\n",
    "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"A transformer that passes through the data without changing it.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for passthrough\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "\n",
    "# Create a pipeline for passthrough transformation\n",
    "passthrough_transformation_pipeline = Pipeline([\n",
    "    ('passthrough', PassthroughTransformer())  # Passthrough transformer\n",
    "])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e8e43659c64b622",
   "metadata": {},
   "source": [
    "### Main Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "id": "7a58bc2bddf07de0",
   "metadata": {},
   "source": [
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84dc27d100d55d1d",
   "metadata": {},
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "bebb2cba05dcd497",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class grid_cv_search_hp:\n",
    "    \"\"\"\n",
    "    Class to perform hyperparameter optimization across multiple machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        params (dict): Dictionary of hyperparameters for the models.\n",
    "        grid_searches (dict): Dictionary to store the results of GridSearchCV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params, target_transformer):\n",
    "        \"\"\"\n",
    "        Initializes the GridCvSearchHP with models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            models (dict): A dictionary of model names and instances.\n",
    "            params (dict): A dictionary of model names and their hyperparameters.\n",
    "            target_transformer: Transformer to apply to the target variable.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.grid_searches = {}\n",
    "        self.target_transformer = target_transformer\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=10, scoring='r2', refit=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization using GridSearchCV for each model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data features.\n",
    "            y (array-like): Training data target values.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "            n_jobs (int): Number of jobs to run in parallel.\n",
    "            verbose (int): Controls the verbosity of the output.\n",
    "            scoring (str): Scoring metric for model evaluation.\n",
    "            refit (bool): Whether to refit the best model on the whole dataset after searching.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for key in self.models:\n",
    "            print(f\"\\nOptimizing hyperparameters for {key}...\\n\")\n",
    "            model = create_pipeline(self.models[key], self.target_transformer)\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        \"\"\"\n",
    "        Summarize the grid search results.\n",
    "        \n",
    "        Args:\n",
    "            sort_by (str): The column to sort the results by.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A pandas DataFrame containing the summary of grid search results.\n",
    "            dict: The grid search results.\n",
    "        \"\"\"\n",
    "\n",
    "        def row(key, scores, params):\n",
    "            \"\"\"Creates a row for the summary dataframe.\"\"\"\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = f\"split{i}_test_score\"\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append(row(k, s, p))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns += [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9f113541180514c",
   "metadata": {},
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ]
  },
  {
   "cell_type": "code",
   "id": "10c625d779cae244",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, \\\n",
    "    HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, QuantileRegressor, \\\n",
    "    RANSACRegressor, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Initializing regression models with default parameters\n",
    "\n",
    "# Linear Models\n",
    "linear_models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(max_iter=100000),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'QuantileRegressor': QuantileRegressor(),\n",
    "    'RANSACRegressor': RANSACRegressor(),\n",
    "    'PLSRegression': PLSRegression(),\n",
    "    'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    'TheilSenRegressor': TheilSenRegressor()\n",
    "}\n",
    "\n",
    "# Tree-Based Models\n",
    "tree_based_models = {\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Gradient Boosting Frameworks\n",
    "gradient_boosting_models = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "    'LGBMRegressor': LGBMRegressor(),\n",
    "    'CatBoostRegressor': CatBoostRegressor(verbose=False)\n",
    "}\n",
    "\n",
    "# Support Vector Machines\n",
    "svm_models = {\n",
    "    'SVR': SVR(),\n",
    "    'NuSVR': NuSVR(),\n",
    "    'LinearSVR': LinearSVR(max_iter=100000)\n",
    "}\n",
    "\n",
    "# Nearest Neighbors\n",
    "nearest_neighbors_models = {\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'RadiusNeighborsRegressor': RadiusNeighborsRegressor(radius=20.0)  # Uncomment if needed\n",
    "}\n",
    "\n",
    "# Bayesian Methods\n",
    "bayesian_models = {\n",
    "    'GaussianProcessRegressor': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Combining all models into a single dictionary for quick search\n",
    "models_quick_search = {\n",
    "    **linear_models,\n",
    "    **tree_based_models,\n",
    "    **gradient_boosting_models,\n",
    "    **svm_models,\n",
    "    **nearest_neighbors_models,\n",
    "    **bayesian_models\n",
    "}\n",
    "\n",
    "# Define an empty hyperparameter dictionary as all models will use default parameters initially\n",
    "params_quick_search = {model_name: {} for model_name in models_quick_search}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e61feca22f472f5a",
   "metadata": {},
   "source": [
    "### Running Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7dde5fb9f8e9143",
   "metadata": {},
   "source": [
    "initial_search = grid_cv_search_hp(models=models_quick_search, params=params_quick_search,\n",
    "                                   target_transformer=target_transformation_pipeline)\n",
    "initial_search.fit(X_train, y_train, cv=5, n_jobs=-1, scoring='r2', refit=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f892ab114b9ea019",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = initial_search.score_summary()\n",
    "grid_search_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "951d2801cb8ac903",
   "metadata": {},
   "source": [
    "### Summary of Regressors by Category\n",
    "\n",
    "1. **Linear Models**\n",
    "   * BayesianRidge, 0.860079\n",
    "   * PLSRegression, 0.853426\n",
    "   * Ridge, 0.851585\n",
    "   * HuberRegressor, 0.845978\n",
    "   * LinearRegression, 0.842141\n",
    "   * RANSACRegressor, 0.795119\n",
    "   * TheilSenRegressor, 0.285762\n",
    "   * Lasso, -0.036293\n",
    "   * ElasticNet, -0.036293\n",
    "   * QuantileRegressor, -0.055394\n",
    "\n",
    "2. **Tree-Based Models**\n",
    "   * HistGradientBoostingRegressor, 0.858713\n",
    "   * RandomForestRegressor, 0.853213\n",
    "   * ExtraTreesRegressor, 0.843201\n",
    "   * GradientBoostingRegressor, 0.833988\n",
    "   * AdaBoostRegressor, 0.793819\n",
    "   * DecisionTreeRegressor, 0.698089\n",
    "\n",
    "3. **Gradient Boosting Frameworks**\n",
    "   * CatBoostRegressor, 0.856161\n",
    "   * LGBMRegressor, 0.851544\n",
    "   * XGBRegressor, 0.828049\n",
    "\n",
    "4. **Support Vector Machines**\n",
    "   * LinearSVR, 0.846725\n",
    "   * NuSVR, 0.842756\n",
    "   * SVR, 0.827245\n",
    "\n",
    "5. **Nearest Neighbors**\n",
    "   * KNeighborsRegressor, 0.800393\n",
    "\n",
    "6. **Bayesian Methods**\n",
    "   * GaussianProcessRegressor, -6.150232\n",
    "\n",
    "We can see that the top-performing models are from the BayesianRidge, HistGradientBoostingRegressor, and CatBoostRegressor categories.\n",
    "\n",
    "\n",
    "## Regressors Exploration\n",
    "\n",
    "### BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "id": "be6a96de159576e4",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": {\n",
    "        'model__regressor__max_iter': [300, 600, 1000],\n",
    "        # Number of iterations for convergence. Increasing iterations can improve accuracy but increase computation time.\n",
    "        'model__regressor__tol': [1e-3, 1e-4, 1e-5],\n",
    "        # Tolerance for the stopping criteria. Lower values lead to more precise convergence but longer training times.\n",
    "        'model__regressor__alpha_1': [1e-6, 1e-5, 1e-4],\n",
    "        # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__alpha_2': [1e-6, 1e-5, 1e-4],\n",
    "        # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_1': [1e-6, 1e-5, 1e-4],\n",
    "        # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_2': [1e-6, 1e-5, 1e-4],\n",
    "        # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "728e5a031f37dbfc",
   "metadata": {},
   "source": [
    "search_tuned_br = grid_cv_search_hp(models=models_tune_search_bayesianridge, params=params_tune_search_bayesianridge,\n",
    "                                    target_transformer=target_transformation_pipeline)\n",
    "search_tuned_br.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df377f860c1c1371",
   "metadata": {},
   "source": [
    "models_tune_summary_br, models_tune_pipelines_br = search_tuned_br.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_br"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57f9b1393eff327e",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_br = models_tune_summary_br.iloc[0]['estimator']\n",
    "best_model_br"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e20e0cf811668cc",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_br = models_tune_pipelines_br[best_model_br].best_params_\n",
    "best_parameters_br"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e96b3a92a692f63",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_br = models_tune_pipelines_br[best_model_br].best_estimator_\n",
    "best_pipeline_br"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "706eb93fbbf0958",
   "metadata": {},
   "source": [
    "## Accessing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "id": "caadadf3af8b501f",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "def plot_feature_importance_absolute(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Plot the absolute feature importance from a given pipeline.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the sub-pipeline does not support the transform operation.\n",
    "        ValueError: If there is a mismatch in feature support mask length and transformed features.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(\n",
    "                        step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances\n",
    "        }).sort_values(by=importance_type, ascending=False)\n",
    "\n",
    "        # Plotting the feature importances or coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=importance_type, y='Feature', data=feature_importances_df)\n",
    "        plt.xlabel(importance_type.capitalize())\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Feature {importance_type.capitalize()}s')\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c1415ea6682ca73",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_br)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c5ebd0aeed212e8",
   "metadata": {},
   "source": [
    "Let's get all Features sorted by their importance"
   ]
  },
  {
   "cell_type": "code",
   "id": "77baf6cf9a876bef",
   "metadata": {},
   "source": [
    "def get_sorted_feature_importances(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Retrieve and return the feature importances or coefficients from a given pipeline, sorted by absolute value.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features and their absolute importances or coefficients, sorted by absolute value.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(\n",
    "                        step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances,\n",
    "            f'abs_{importance_type}': np.abs(importances)\n",
    "        }).sort_values(by=f'abs_{importance_type}', ascending=False)\n",
    "\n",
    "        return feature_importances_df[['Feature', importance_type, f'abs_{importance_type}']]\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9b3443a54712d8a",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_br)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c06f06129ade337",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a regression model on both the training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "    return (r2_train, mae_train, mse_train, rmse_train, msle_train), (r2_test, mae_test, mse_test, rmse_test, msle_test)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates a regression model on a given dataset and prints key metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Data features.\n",
    "        y (array-like): Data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    prediction = pipeline.predict(X)\n",
    "    r2 = r2_score(y, prediction)\n",
    "    mae = mean_absolute_error(y, prediction)\n",
    "    mse = mean_squared_error(y, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    msle = mean_squared_log_error(y, prediction)\n",
    "\n",
    "    return r2, mae, mse, rmse, msle\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    \"\"\"\n",
    "    Plots actual vs predicted values for both training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "        alpha_scatter (float): Transparency of the scatter plot points.\n",
    "    \"\"\"\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "    # Train set evaluation\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    # Test set evaluation\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "    # Train plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0, 0], color='blue')\n",
    "    axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 0].set_ylabel(\"Predictions\")\n",
    "    axes[0, 0].set_title(\"Train Set: Actual vs Predicted\")\n",
    "    train_metrics_text = (f'R2: {round(r2_train, 3)}\\n'\n",
    "                          f'MAE: {round(mae_train, 3)}\\n'\n",
    "                          f'MSE: {round(mse_train, 3)}\\n'\n",
    "                          f'RMSE: {round(rmse_train, 3)}\\n'\n",
    "                          f'MSLE: {round(msle_train, 3)}')\n",
    "    axes[0, 0].text(0.05, 0.95, train_metrics_text, transform=axes[0, 0].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Test plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[0, 1], color='green')\n",
    "    axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 1].set_ylabel(\"Predictions\")\n",
    "    axes[0, 1].set_title(\"Test Set: Actual vs Predicted\")\n",
    "    test_metrics_text = (f'R2: {round(r2_test, 3)}\\n'\n",
    "                         f'MAE: {round(mae_test, 3)}\\n'\n",
    "                         f'MSE: {round(mse_test, 3)}\\n'\n",
    "                         f'RMSE: {round(rmse_test, 3)}\\n'\n",
    "                         f'MSLE: {round(msle_test, 3)}')\n",
    "    axes[0, 1].text(0.05, 0.95, test_metrics_text, transform=axes[0, 1].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Residuals plot: Train\n",
    "    residuals_train = y_train - pred_train\n",
    "    sns.scatterplot(x=pred_train, y=residuals_train, alpha=alpha_scatter, ax=axes[1, 0], color='blue')\n",
    "    axes[1, 0].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel(\"Predictions\")\n",
    "    axes[1, 0].set_ylabel(\"Residuals\")\n",
    "    axes[1, 0].set_title(\"Train Set: Residuals\")\n",
    "\n",
    "    # Residuals plot: Test\n",
    "    residuals_test = y_test - pred_test\n",
    "    sns.scatterplot(x=pred_test, y=residuals_test, alpha=alpha_scatter, ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel(\"Predictions\")\n",
    "    axes[1, 1].set_ylabel(\"Residuals\")\n",
    "    axes[1, 1].set_title(\"Test Set: Residuals\")\n",
    "\n",
    "    # Error distribution plot: Train\n",
    "    sns.histplot(residuals_train, kde=True, ax=axes[1, 2], color='blue')\n",
    "    axes[1, 2].set_xlabel(\"Residuals\")\n",
    "    axes[1, 2].set_ylabel(\"Frequency\")\n",
    "    axes[1, 2].set_title(\"Train Set: Error Distribution\")\n",
    "\n",
    "    # Error distribution plot: Test\n",
    "    sns.histplot(residuals_test, kde=True, ax=axes[0, 2], color='green')\n",
    "    axes[0, 2].set_xlabel(\"Residuals\")\n",
    "    axes[0, 2].set_ylabel(\"Frequency\")\n",
    "    axes[0, 2].set_title(\"Test Set: Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "376a2219a4d56ad4",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_br)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_br)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a190acb5efe8c53",
   "metadata": {},
   "source": [
    "OK, we have some quote promising results:\n",
    "* Train R<sup>2</sup> = 0.861\n",
    "* Test R<sup>2</sup> = 0.778\n",
    "\n",
    "### PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "id": "7281141623cc9caa",
   "metadata": {},
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_pls = {\n",
    "    \"PLSRegression\": PLSRegression(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_pls = {\n",
    "    \"PLSRegression\": {\n",
    "        'model__regressor__n_components': [2, 5, 10, 15, 20],  # Number of components to keep.\n",
    "        'model__regressor__max_iter': [300, 600, 1000, 2000],\n",
    "        # Maximum number of iterations for the algorithm to converge.\n",
    "        'model__regressor__tol': [1e-6, 1e-7, 1e-8, 1e-9],  # Tolerance for the stopping criteria.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48352152bfa6c27e",
   "metadata": {},
   "source": [
    "search_tuned_pls = grid_cv_search_hp(models=models_tune_search_pls, params=params_tune_search_pls,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_pls.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6645f0af35b752da",
   "metadata": {},
   "source": [
    "models_tune_summary_pls, models_tune_pipelines_pls = search_tuned_pls.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_pls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "802f7770878c0e99",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_pls = models_tune_summary_pls.iloc[0]['estimator']\n",
    "best_model_pls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecd2823ffb39f7de",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_pls = models_tune_pipelines_pls[best_model_pls].best_params_\n",
    "best_parameters_pls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d67fe2e11ca84f1",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_pls = models_tune_pipelines_pls[best_model_pls].best_estimator_\n",
    "best_pipeline_pls"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e6ffc3aefe2342e2",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_pls)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a7ef12801ad6054",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_pls)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f85458ab30fbdc0c",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_pls)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_pls)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15d92712e1bdbf8b",
   "metadata": {},
   "source": [
    "We have Results:\n",
    "\n",
    "* Train R<sup>2</sup> = 0.878\n",
    "* Test R<sup>2</sup> = 0.74\n",
    "\n",
    "### HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "id": "27971eb779e64d94",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_hgb = {\n",
    "    \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_hgb = {\n",
    "    \"HistGradientBoostingRegressor\": {\n",
    "        'model__regressor__learning_rate': [0.01, 0.1, 0.3],\n",
    "        # Learning rate shrinks the contribution of each tree. Lower values make the model more robust to overfitting, but require more trees.\n",
    "        'model__regressor__max_iter': [100, 250, 600],\n",
    "        # Maximum number of iterations (trees). More iterations can improve performance but increase computation time and risk of overfitting.\n",
    "        'model__regressor__max_leaf_nodes': [31, 63],\n",
    "        # Maximum number of leaves per tree. More leaves can capture more information but increase risk of overfitting and computation cost.\n",
    "        'model__regressor__max_depth': [None, 10, 20],\n",
    "        # Maximum depth of each tree. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__min_samples_leaf': [20, 50, 100],\n",
    "        # Minimum number of samples required at a leaf node. Higher values prevent overfitting by ensuring leaves contain more samples.\n",
    "        'model__regressor__l2_regularization': [0.0, 0.1, 1.0],\n",
    "        # L2 regularization term. Higher values penalize large weights, helping to prevent overfitting.\n",
    "        'model__regressor__max_bins': [255, 511],\n",
    "        # Maximum number of bins for discretizing continuous features. More bins can capture more detail but increase memory and computation cost.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d094e630682f276",
   "metadata": {},
   "source": [
    "search_tuned_hgb = grid_cv_search_hp(models=models_tune_search_hgb, params=params_tune_search_hgb,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_hgb.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5082e7505cef3398",
   "metadata": {},
   "source": [
    "models_tune_summary_hgb, models_tune_pipelines_hgb = search_tuned_hgb.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_hgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "adea51a03ab55589",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_hgb = models_tune_summary_hgb.iloc[0]['estimator']\n",
    "best_model_hgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44d1648c1c88e62a",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_hgb = models_tune_pipelines_hgb[best_model_hgb].best_params_\n",
    "best_parameters_hgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ff036b9f77b529b",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_hgb = models_tune_pipelines_hgb[best_model_hgb].best_estimator_\n",
    "best_pipeline_hgb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "363a3845f9ee995f",
   "metadata": {},
   "source": "# We can not plot Feature Importance for HistGradientBoostingRegressor, as it does not support it",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c72535b2d1de4e0f",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_hgb)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_hgb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "977edcde32da81ec",
   "metadata": {},
   "source": [
    "We have Results:\n",
    "* Train R<sup>2</sup> = 0.944\n",
    "* Test R<sup>2</sup> = 0.824\n",
    "\n",
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "id": "db2663a76bdf82fa",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_rf = {\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_rf = {\n",
    "    \"RandomForestRegressor\": {\n",
    "        'model__regressor__n_estimators': [100, 200, 500],\n",
    "        # The number of trees in the forest. More trees can improve performance but increase computation time.\n",
    "        'model__regressor__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        # The number of features to consider when looking for the best split. 'auto' uses sqrt(n_features).\n",
    "        'model__regressor__max_depth': [None, 10, 20, 30],\n",
    "        # Maximum depth of the tree. None means nodes are expanded until all leaves are pure or contain less than min_samples_split samples.\n",
    "        'model__regressor__min_samples_split': [2, 10, 20],\n",
    "        # Minimum number of samples required to split an internal node. Higher values can prevent overfitting.\n",
    "        'model__regressor__min_samples_leaf': [1, 5, 10],\n",
    "        # Minimum number of samples required to be at a leaf node. Higher values can help prevent overfitting.\n",
    "        'model__regressor__random_state': [42],  # Seed used by the random number generator. Ensures reproducibility.\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98b23469a1e52c6",
   "metadata": {},
   "source": [
    "search_tuned_rf = grid_cv_search_hp(models=models_tune_search_rf, params=params_tune_search_rf,\n",
    "                                    target_transformer=target_transformation_pipeline)\n",
    "search_tuned_rf.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e99a12400af238",
   "metadata": {},
   "source": [
    "models_tune_summary_rf, models_tune_pipelines_rf = search_tuned_rf.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_rf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aca9684f0ae7ceb",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_rf = models_tune_summary_rf.iloc[0]['estimator']\n",
    "best_model_rf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70de9451444c9992",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_rf = models_tune_pipelines_rf[best_model_rf].best_params_\n",
    "best_parameters_rf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef49572e7cbb254c",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_rf = models_tune_pipelines_rf[best_model_rf].best_estimator_\n",
    "best_pipeline_rf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d6a619b98a66e49",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_rf)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "780748876a852126",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_rf)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6aaffbe888bd7b58",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_rf)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_rf)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39ef46f98888e995",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup> = 0.978\n",
    "* Test R<sup>2</sup> = 0.852\n",
    "\n",
    "### CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "id": "e21d9d9500c06e09",
   "metadata": {},
   "source": [
    "models_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(logging_level='Silent')\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": {\n",
    "        'model__regressor__iterations': [100, 500, 1000],\n",
    "        # Number of trees. More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "        # Learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__depth': [6, 8, 10],\n",
    "        # Depth of the trees. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__l2_leaf_reg': [1, 3, 5],\n",
    "        # L2 regularization term. Higher values prevent overfitting by penalizing large weights.\n",
    "        'model__regressor__bagging_temperature': [0.0, 0.5, 1.0],\n",
    "        # Controls the variance of bagging. Higher values increase the randomization.\n",
    "        'model__regressor__random_strength': [1, 2, 5],\n",
    "        # Amount of randomness for scoring splits. Higher values add more randomness, helping to prevent overfitting.\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e75c0f058d92bc7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc62b85d28924d3c",
   "metadata": {},
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2a695cc168c7c4a",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_catboost = models_tune_summary_catboost.iloc[0]['estimator']\n",
    "best_model_catboost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab3f00a75393c65f",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "468f05f5bea7249e",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2520af2a8bf02d66",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_catboost)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c22edf0f53756014",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_catboost)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1df9033d13f7aff7",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21292daecd6ccbe0",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup> = 0.943\n",
    "* Test R<sup>2</sup> = 0.855\n",
    "\n",
    "### LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3cb90f017ba1c50",
   "metadata": {},
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_lgbm = {\n",
    "    \"LGBMRegressor\": LGBMRegressor(verbose=-1),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with the selected hyperparameters\n",
    "params_tune_search_lgbm = {\n",
    "    \"LGBMRegressor\": {\n",
    "        'model__regressor__num_leaves': [31, 50, 70],\n",
    "        # Maximum number of leaves in one tree. More leaves can increase model complexity and accuracy.\n",
    "        'model__regressor__max_depth': [-1, 10, 20],\n",
    "        # Maximum depth of the tree. -1 means no limit. Controls the complexity of the model.\n",
    "        'model__regressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "        # Boosting learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__n_estimators': [100, 200, 500],\n",
    "        # Number of boosting iterations (trees). More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__min_child_samples': [20, 50, 100],\n",
    "        # Minimum number of data points needed in a leaf. Helps prevent overfitting.\n",
    "        'model__regressor__subsample': [0.6, 0.8, 1.0],\n",
    "        # Subsample ratio of the training instance. Prevents overfitting by randomly sampling a subset of data.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e0c26ee9983fe3d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "search_tuned_lgbm = grid_cv_search_hp(models=models_tune_search_lgbm, params=params_tune_search_lgbm,\n",
    "                                      target_transformer=target_transformation_pipeline)\n",
    "search_tuned_lgbm.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d21fb7d49be9514",
   "metadata": {},
   "source": [
    "models_tune_summary_lgbm, models_tune_pipelines_lgbm = search_tuned_lgbm.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_lgbm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60e7a0b683968ff0",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_lgbm = models_tune_summary_lgbm.iloc[0]['estimator']\n",
    "best_model_lgbm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40a2b377cfa4cb0",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_lgbm = models_tune_pipelines_lgbm[best_model_lgbm].best_params_\n",
    "best_parameters_lgbm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d5ed34b4ba5fe0b",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_lgbm = models_tune_pipelines_lgbm[best_model_lgbm].best_estimator_\n",
    "best_pipeline_lgbm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76b65e0bafbec041",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_lgbm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "367e93d07d003c68",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_lgbm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26d6e08a45fa6066",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_lgbm)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_lgbm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c215bd46caea4efd",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R <sup>2</sup> =0.942\n",
    "* Test R<sup>2</sup> =0.827\n",
    "\n",
    "### LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f358aa50295a7d2",
   "metadata": {},
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_linearsvr = {\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_linearsvr = {\n",
    "    \"LinearSVR\": {\n",
    "        'model__regressor__C': [0.1, 1, 10],  # Regularization parameter. Higher values mean less regularization.\n",
    "        'model__regressor__epsilon': [0.1, 0.2, 0.5],\n",
    "        # Epsilon-tube within which no penalty is associated with the training loss.\n",
    "        'model__regressor__tol': [1e-4, 1e-5, 1e-6],\n",
    "        # Tolerance for stopping criteria. Lower values mean more precise convergence.\n",
    "        'model__regressor__max_iter': [100000, 200000, 300000],\n",
    "        # Maximum number of iterations. Higher values ensure convergence but increase computation time.\n",
    "        'model__regressor__loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        # Specifies the loss function.\n",
    "        'model__regressor__dual': [True, False],\n",
    "        # Dual or primal formulation. Dual is preferred when n_samples > n_features.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edc4198f0645d1b2",
   "metadata": {},
   "source": [
    "search_tuned_linearsvr = grid_cv_search_hp(models=models_tune_search_linearsvr, params=params_tune_search_linearsvr,\n",
    "                                           target_transformer=target_transformation_pipeline)\n",
    "search_tuned_linearsvr.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14b44ca5ed55e45c",
   "metadata": {},
   "source": [
    "models_tune_summary_linearsvr, models_tune_pipelines_linearsvr = search_tuned_linearsvr.score_summary(\n",
    "    sort_by='mean_score')\n",
    "models_tune_summary_linearsvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bbd1f27a0ba73d",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_linearsvr = models_tune_summary_linearsvr.iloc[0]['estimator']\n",
    "best_model_linearsvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a5fe91f2f2aec47",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_linearsvr = models_tune_pipelines_linearsvr[best_model_linearsvr].best_params_\n",
    "best_parameters_linearsvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4328a1177a95bc6",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_linearsvr = models_tune_pipelines_linearsvr[best_model_linearsvr].best_estimator_\n",
    "best_pipeline_linearsvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fd3f21d8147fdfa",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_linearsvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35a1e161a620e040",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_linearsvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5af848eb43644ee4",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_linearsvr)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_linearsvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "408af04eb3837717",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup> =0.869\n",
    "* Test R<sup>2</sup> =0.526\n",
    "\n",
    "### NuSVFR"
   ]
  },
  {
   "cell_type": "code",
   "id": "d074bb68ebd5b35a",
   "metadata": {},
   "source": [
    "from sklearn.svm import NuSVR\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_nusvr = {\n",
    "    \"NuSVR\": NuSVR(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_nusvr = {\n",
    "    \"NuSVR\": {\n",
    "        'model__regressor__nu': [0.1, 0.5, 0.9],\n",
    "        # An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors.\n",
    "        'model__regressor__C': [0.1, 1, 10],  # Regularization parameter. Higher values mean less regularization.\n",
    "        'model__regressor__kernel': ['linear', 'poly', 'rbf'],  # Specifies the kernel type to be used in the algorithm.\n",
    "        'model__regressor__degree': [2, 3, 4],\n",
    "        # Degree of the polynomial kernel function (poly). Ignored by all other kernels.\n",
    "        'model__regressor__tol': [1e-3, 1e-4, 1e-5],\n",
    "        # Tolerance for stopping criteria. Lower values mean more precise convergence.\n",
    "        'model__regressor__max_iter': [100000, 200000, 300000],\n",
    "        # Maximum number of iterations. Higher values ensure convergence but increase computation time.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4eab9c41990c47ff",
   "metadata": {},
   "source": [
    "search_tuned_nusvr = grid_cv_search_hp(models=models_tune_search_nusvr, params=params_tune_search_nusvr,\n",
    "                                       target_transformer=target_transformation_pipeline)\n",
    "search_tuned_nusvr.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd4f7df8604eaacd",
   "metadata": {},
   "source": [
    "models_tune_summary_nusvr, models_tune_pipelines_nusvr = search_tuned_nusvr.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_nusvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "daf0be4992272345",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_nusvr = models_tune_summary_nusvr.iloc[0]['estimator']\n",
    "best_model_nusvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "115f83542f51106d",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_nusvr = models_tune_pipelines_nusvr[best_model_nusvr].best_params_\n",
    "best_parameters_nusvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec08ebb9d6618e9e",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_nusvr = models_tune_pipelines_nusvr[best_model_nusvr].best_estimator_\n",
    "best_pipeline_nusvr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f58a5793ce4b9eb",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_nusvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6772f1ea21be7b3b",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_nusvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e556bc4086d02f1f",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_nusvr)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_nusvr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce3fbc9a22d4de09",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup > =0.862\n",
    "* Test R<sup>2</sup> = 0.825\n",
    "\n",
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8e0fad06dfce3a",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_knn = {\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_knn = {\n",
    "    \"KNeighborsRegressor\": {\n",
    "        'model__regressor__n_neighbors': [3, 5, 7],\n",
    "        # Number of neighbors to use. More neighbors can smooth out predictions but might include more noise.\n",
    "        'model__regressor__weights': ['uniform', 'distance'],\n",
    "        # Weight function used in prediction. 'uniform' uses equal weights, 'distance' uses the inverse of distance.\n",
    "        'model__regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        # Algorithm used to compute the nearest neighbors.\n",
    "        'model__regressor__leaf_size': [30, 50, 70],\n",
    "        # Leaf size passed to BallTree or KDTree. Affects the speed of the construction and query.\n",
    "        'model__regressor__p': [1, 2],\n",
    "        # Power parameter for the Minkowski metric. 1 is equivalent to Manhattan distance, 2 is equivalent to Euclidean distance.\n",
    "        'model__regressor__metric': ['minkowski', 'euclidean', 'manhattan'],  # The distance metric to use for the tree.\n",
    "    }\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3511813feb3cdc",
   "metadata": {},
   "source": [
    "search_tuned_knn = grid_cv_search_hp(models=models_tune_search_knn, params=params_tune_search_knn,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_knn.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34ab51248737bcd2",
   "metadata": {},
   "source": [
    "models_tune_summary_knn, models_tune_pipelines_knn = search_tuned_knn.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_knn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5ae287b79ea98cb",
   "metadata": {},
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_knn = models_tune_summary_knn.iloc[0]['estimator']\n",
    "best_model_knn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81b7bf5273ab2460",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_knn = models_tune_pipelines_knn[best_model_knn].best_params_\n",
    "best_parameters_knn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d097b9f3bdac7f61",
   "metadata": {},
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_knn = models_tune_pipelines_knn[best_model_knn].best_estimator_\n",
    "best_pipeline_knn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4d3c5670911672c",
   "metadata": {},
   "source": [
    "plot_feature_importance_absolute(best_pipeline_knn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "516013313a51c693",
   "metadata": {},
   "source": [
    "get_sorted_feature_importances(best_pipeline_knn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f58cda8b989b1d",
   "metadata": {},
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_knn)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_knn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59178ff2d383bbdf",
   "metadata": {},
   "source": [
    "We have results:\n",
    "* Train R<sup>2</sup > =1.0\n",
    "* Test R<sup>2</sup> = 0.79\n",
    "\n",
    "## Model Performance Summary\n",
    "\n",
    "### Table of Model Metrics\n",
    "\n",
    "| Model                          | Train R<sup>2</sup> | Test R<sup>2</sup> | Overfitting (Train - Test) |\n",
    "|------------------------------- |---------------------|--------------------|----------------------------|\n",
    "| BayesianRidge                 | 0.861               | 0.778              | 0.083                      |\n",
    "| PLSRegression                 | 0.878               | 0.74               | 0.138                      |\n",
    "| HistGradientBoostingRegressor | 0.944               | 0.824              | 0.12                       |\n",
    "| RandomForestRegressor         | 0.978               | 0.852              | 0.126                      |\n",
    "| CatBoostRegressor             | 0.943               | 0.855              | 0.088                      |\n",
    "| LGBMRegressor                 | 0.942               | 0.827              | 0.115                      |\n",
    "| LinearSVR                     | 0.869               | 0.526              | 0.43                       |\n",
    "| NuSVR                         | 0.882               | 0.825              | 0.057                      |\n",
    "| KNeighborsRegressor           | 1.0                 | 0.79               | 0.21                       |\n",
    "\n",
    "### Analysis and Recommendations\n",
    "\n",
    "To determine the best regressors for trading and modeling, we should consider both the test R value and the overfitting metric. High test R values indicate good predictive performance on new data, while low overfitting values indicate good generalization.\n",
    "\n",
    "#### Key Metrics Summary\n",
    "\n",
    "1. **CatBoostRegressor**:\n",
    "   - Test R: 0.855\n",
    "   - Overfitting: 0.088\n",
    "\n",
    "2. **RandomForestRegressor**:\n",
    "   - Test R: 0.852\n",
    "   - Overfitting: 0.126\n",
    "\n",
    "3. **LGBMRegressor**:\n",
    "   - Test R: 0.827\n",
    "   - Overfitting: 0.115\n",
    "\n",
    "4. **NuSVR**:\n",
    "   - Test R: 0.882\n",
    "   - Overfitting: 0.057\n",
    "\n",
    "5. **HistGradientBoostingRegressor**:\n",
    "   - Test R: 0.824\n",
    "   - Overfitting: 0.12\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- **CatBoostRegressor** has the highest test R and low overfitting, making it an excellent choice for predictive accuracy and generalization.\n",
    "- **RandomForestRegressor** also shows strong performance but has slightly higher overfitting than CatBoost.\n",
    "- **LGBMRegressor** performs well with a slightly higher overfitting than CatBoost but still maintains a high test R.\n",
    "- **NuSVR** has the smallest overfitting value, indicating excellent generalization, but its test R is slightly lower than CatBoost and RandomForest.\n",
    "- **HistGradientBoostingRegressor** has good performance but is slightly less favorable than CatBoost and RandomForest in terms of test R and overfitting.\n",
    "\n",
    "#### Recommendation\n",
    "\n",
    "- **Best Regressors:**\n",
    "  - **CatBoostRegressor**: Best overall balance of high test R and low overfitting.\n",
    "  - **RandomForestRegressor**: Very strong performance with slightly higher overfitting than CatBoost.\n",
    "  - **NuSVR**: Exceptional generalization with very low overfitting, making it a reliable model despite slightly lower test R.\n",
    "\n",
    "- **Keep for Trading and Modeling:**\n",
    "  - **CatBoostRegressor** and **RandomForestRegressor** for their high predictive accuracy.\n",
    "  - **NuSVR** for scenarios where minimal overfitting is critical, even with slightly lower predictive power.\n",
    "\n",
    "These models provide a strong foundation for trading and modeling, balancing accuracy and generalization effectively.\n",
    "\n",
    "CatBoost has it's own feature selector.\n",
    " But this does not mean we can not experiment applying PCA"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PCA\n",
    "\n",
    "We will add PCA to main Pipeline and remove Feature selection"
   ],
   "id": "b93aa0289eb01095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# Assume the necessary imports and other classes like FeatureCreator are already defined\n",
    "\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer, n_components=None):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    - n_components: Number of components to keep for PCA. If None, PCA is skipped.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Placeholder for additional transformations if any\n",
    "        ('post_transformations', post_feature_transformer),  # Placeholder for additional post-transformations if any\n",
    "        ('pca', PCA(n_components=n_components))  # Ensure PCA is always included\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "d8bb5389f6831d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PCA Catboost Regressor Grid Search\n",
    "\n",
    "Creating Custom Gridsearch CV to get R<sup>2</sup> for Train and Test sets"
   ],
   "id": "15b23166a9cbfed4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class grid_cv_search_hp:\n",
    "    \"\"\"\n",
    "    Class to perform hyperparameter optimization across multiple machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        params (dict): Dictionary of hyperparameters for the models.\n",
    "        grid_searches (dict): Dictionary to store the results of GridSearchCV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params, target_transformer):\n",
    "        \"\"\"\n",
    "        Initializes the GridCvSearchHP with models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            models (dict): A dictionary of model names and instances.\n",
    "            params (dict): A dictionary of model names and their hyperparameters.\n",
    "            target_transformer: Transformer to apply to the target variable.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.grid_searches = {}\n",
    "        self.target_transformer = target_transformer\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=10, scoring='r2', refit=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization using GridSearchCV for each model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data features.\n",
    "            y (array-like): Training data target values.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "            n_jobs (int): Number of jobs to run in parallel.\n",
    "            verbose (int): Controls the verbosity of the output.\n",
    "            scoring (str): Scoring metric for model evaluation.\n",
    "            refit (bool): Whether to refit the best model on the whole dataset after searching.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for key in self.models:\n",
    "            print(f\"\\nOptimizing hyperparameters for {key}...\\n\")\n",
    "            model = create_pipeline(self.models[key], self.target_transformer)\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, X_train, y_train, X_test, y_test, sort_by='mean_score'):\n",
    "        \"\"\"\n",
    "        Summarize the grid search results and score on train and test sets.\n",
    "        \n",
    "        Args:\n",
    "            X_train (array-like): Training data features.\n",
    "            y_train (array-like): Training data target values.\n",
    "            X_test (array-like): Test data features.\n",
    "            y_test (array-like): Test data target values.\n",
    "            sort_by (str): The column to sort the results by.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A pandas DataFrame containing the summary of grid search results.\n",
    "            dict: The grid search results.\n",
    "        \"\"\"\n",
    "\n",
    "        def row(key, scores, params, train_score, test_score):\n",
    "            \"\"\"Creates a row for the summary dataframe.\"\"\"\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "                'train_score': train_score,\n",
    "                'test_score': test_score\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = f\"split{i}_test_score\"\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                # Refit the model with the best parameters\n",
    "                self.grid_searches[k].best_estimator_.fit(X_train, y_train)\n",
    "                train_score = self.grid_searches[k].best_estimator_.score(X_train, y_train)\n",
    "                test_score = self.grid_searches[k].best_estimator_.score(X_test, y_test)\n",
    "                rows.append(row(k, s, p, train_score, test_score))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score', 'train_score', 'test_score']\n",
    "        columns += [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ],
   "id": "e7334556acbab499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating Catboost Regressor Hyper Parameters with PCA",
   "id": "ec275eb0e5818e50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit and transform the pre_feature_transformations pipeline on the training data\n",
    "X_transformed = pre_feature_transformations.fit_transform(X_train)\n",
    "\n",
    "# Get the number of features after the transformation\n",
    "num_features_after_transformation = X_transformed.shape[1]\n",
    "\n",
    "# Define the parameter grid with the appropriate PCA components range\n",
    "params_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": {\n",
    "        'pca__n_components': list(range(1, num_features_after_transformation + 1)),  # Updated PCA components range\n",
    "        'model__regressor__iterations': [1000],\n",
    "        'model__regressor__learning_rate': [0.01],\n",
    "        'model__regressor__depth': [6],\n",
    "        'model__regressor__l2_leaf_reg': [3],\n",
    "        'model__regressor__bagging_temperature': [0.0],\n",
    "        'model__regressor__random_strength': [1],\n",
    "    }\n",
    "}\n"
   ],
   "id": "f191e6b4dd4eef56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)\n"
   ],
   "id": "b56329f64ac19fd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluating Model Performance with PCA",
   "id": "4b803234b08f1533"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "def evaluate_pca_performance(grid_search, X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "\n",
    "    for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "        # Clone the pipeline with current parameters\n",
    "        pipeline = clone(grid_search.estimator).set_params(**params)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Get the PCA components for the current setting\n",
    "        pca_n_components = params.get('pca__n_components', None)\n",
    "\n",
    "        # Evaluate the performance of the current model\n",
    "        (r2_train, mae_train, mse_train, rmse_train, msle_train), \\\n",
    "            (r2_test, mae_test, mse_test, rmse_test, msle_test) = regression_performance(X_train, y_train, X_test,\n",
    "                                                                                         y_test, pipeline)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'PCA Components': pca_n_components,\n",
    "            'Train_R2': r2_train,\n",
    "            'Test_R2': r2_test,\n",
    "            'Train_MAE': mae_train,\n",
    "            'Test_MAE': mae_test,\n",
    "            'Train_MSE': mse_train,\n",
    "            'Test_MSE': mse_test,\n",
    "            'Train_RMSE': rmse_train,\n",
    "            'Test_RMSE': rmse_test,\n",
    "            'Train_MSLE': msle_train,\n",
    "            'Test_MSLE': msle_test\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame with all results\n",
    "    all_results_df = pd.DataFrame(results)\n",
    "\n",
    "    return all_results_df\n"
   ],
   "id": "5efbfbd48a5d8164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_pca_results_df = evaluate_pca_performance(search_tuned_catboost.grid_searches[\"CatBoostRegressor\"], X_train,\n",
    "                                              y_train, X_test, y_test)"
   ],
   "id": "192e68baecb60ff9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_pca_results_df",
   "id": "4d3ca6edc6cf7cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_scores(results_df, dataset='Train'):\n",
    "    \"\"\"\n",
    "    Plots the scores for different PCA components.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing the results.\n",
    "        dataset (str): 'Train' or 'Test' to specify which dataset scores to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    score_types = ['R2']\n",
    "\n",
    "    for score in score_types:\n",
    "        sns.lineplot(x='PCA Components', y=f'{dataset}_{score}', data=results_df, label=f'{dataset} {score}')\n",
    "\n",
    "    plt.title(f'{dataset} Scores vs PCA Components')\n",
    "    plt.xlabel('PCA Components')\n",
    "    plt.ylabel('Score Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "8e74c7d81f643e22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Plot scores for train dataset\n",
    "plot_scores(all_pca_results_df, dataset='Train')"
   ],
   "id": "942a8dfb36b2603b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Plot scores for test dataset\n",
    "plot_scores(all_pca_results_df, dataset='Test')\n"
   ],
   "id": "edca46cbdfd889b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PCA evaluation\n",
    "\n",
    "As it was though, PCA is not compatible with our model.\n",
    "\n",
    "Catboost Regression, works better with Feature Selector\n",
    "\n",
    "In code above we saw that CatBoost Regressor scored 0.855 based just on 12 features "
   ],
   "id": "40298a4b18341189"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CatBoost Regressor best model Features Selector\n",
    "\n",
    "We know that the best hyper_parameters for CatBoostRegressor\n",
    "\n",
    "* bagging_temperature: 0.0,\n",
    "* depth: 6,\n",
    "* iterations: 1000,\n",
    "* l2_leaf_reg: 3,\n",
    "* learning_rate: 0.01,\n",
    "* random_strength: 1\n",
    "\n",
    "We know that model was build based on these Features:\n",
    "\n",
    "\n",
    "| Index | Feature                             | Importance |\n",
    "|-------|-------------------------------------|------------|\n",
    "| 1     | NF_TotalLivingArea_mul_OverallQual  | 18.150305  |\n",
    "| 2     | NF_TotalLivingArea_mul_OverallCond  | 12.228521  |\n",
    "| 3     | OverallQual                         | 10.386984  |\n",
    "| 4     | YearBuilt                           | 9.574680   |\n",
    "| 5     | NF_1stFlrSF_mul_OverallQual         | 9.127873   |\n",
    "| 6     | NF_TotalBsmtSF_mul_BsmtFinType1     | 7.524968   |\n",
    "| 7     | GarageYrBlt                         | 5.885639   |\n",
    "| 8     | LotArea                             | 5.812481   |\n",
    "| 9     | KitchenQual                         | 5.613071   |\n",
    "| 10    | GrLivArea                           | 5.108714   |\n",
    "| 11    | TotalBsmtSF                         | 4.225762   |\n",
    "| 12    | 1stFlrSF                            | 3.473703   |\n",
    "| 13    | NF_TotalBsmtSF_mul_BsmtExposure     | 2.887298   |\n",
    "\n",
    "Based on these Features, we can make a list of features we will need to build a model:\n",
    "1. OverallQual\n",
    "2. GrLivArea\n",
    "3. 1stFlrSF\n",
    "4. 2ndFlrSF\n",
    "5. OverallCond\n",
    "6. YearBuilt\n",
    "7. TotalBsmtSF\n",
    "8. BsmtFinType1\n",
    "9. GarageYrBlt\n",
    "10. LotArea\n",
    "11. KitchenQual\n",
    "12. BsmtExposure"
   ],
   "id": "f3e6dc89d6797327"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Based on These Features, now we can adjust cleaning code and add it to ML pipeline\n",
    "\n",
    "#### Cleaning Data code\n",
    "\n",
    "We will need just this part of code based on features:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF', 'GarageYrBlt']\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1']\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "\n",
    "# Adjust 'BedroomAbvGr' based on '2ndFlrSF' (assuming it might be necessary for 'OverallCond')\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Dropping the newly created column 'Consistency'\n",
    "df = df.drop(columns=['Consistency'])\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtFinSF1'] + df['BsmtUnfSF']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']\n",
    "\n",
    "# Drop columns not in the specified list of necessary features\n",
    "necessary_features = ['OverallQual', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'OverallCond',\n",
    "                      'YearBuilt', 'TotalBsmtSF', 'BsmtFinType1', 'GarageYrBlt', 'LotArea',\n",
    "                      'KitchenQual', 'BsmtExposure']\n",
    "\n",
    "df = df[necessary_features]\n",
    "```\n",
    "\n",
    "\n",
    "Based on that we will create extra functionality for data cleaning in pre_transformations Pipeline"
   ],
   "id": "79908450f08f3e42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "class DatasetCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom dataset cleaner for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom data\n",
    "    cleaning during preprocessing pipelines. It handles various data cleaning \n",
    "    tasks such as filling missing values, adjusting feature inconsistencies, \n",
    "    and dropping unnecessary columns explicitly detailed within the transform \n",
    "    method, ensuring all features are appropriately processed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cleaning_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for cleaning, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of data cleaning steps to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The cleaned dataframe.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # Fill missing values with zero and convert to integers for numerical columns\n",
    "        fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF', 'GarageYrBlt']\n",
    "        X[fill_zero_and_convert] = X[fill_zero_and_convert].fillna(0).astype(int)\n",
    "        self.cleaning_descriptions_['fill_zero_and_convert'] = (\n",
    "            f\"Filled missing values in {fill_zero_and_convert} with 0 and converted to int.\"\n",
    "        )\n",
    "\n",
    "        # Fill missing values with 'None' for categorical columns\n",
    "        fill_none = ['BsmtExposure', 'BsmtFinType1']\n",
    "        X[fill_none] = X[fill_none].fillna('None')\n",
    "        self.cleaning_descriptions_['fill_none'] = f\"Filled missing values in {fill_none} with 'None'.\"\n",
    "\n",
    "        # Adjust 'BedroomAbvGr' based on '2ndFlrSF'\n",
    "        X.loc[X['2ndFlrSF'] == 0, 'BedroomAbvGr'] = X['BedroomAbvGr'].replace(0, 2)\n",
    "        X.loc[X['2ndFlrSF'] > 0, 'BedroomAbvGr'] = X['BedroomAbvGr'].replace(0, 3)\n",
    "        self.cleaning_descriptions_['adjust_BedroomAbvGr'] = \"Adjusted 'BedroomAbvGr' based on '2ndFlrSF'.\"\n",
    "\n",
    "        # Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "        swap_idx = X['2ndFlrSF'] > X['1stFlrSF']\n",
    "        X.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = X.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "        self.cleaning_descriptions_['swap_flrsf'] = \"Swapped values where '2ndFlrSF' is greater than '1stFlrSF'.\"\n",
    "\n",
    "        # Define features and their 'no presence' values\n",
    "        basement_features = ['BsmtExposure', 'BsmtFinType1', 'TotalBsmtSF']\n",
    "        features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"TotalBsmtSF\": 0}\n",
    "\n",
    "        # Check and update inconsistencies for each feature\n",
    "        for feature in basement_features:\n",
    "            primary_value = features_and_values[feature]\n",
    "            X['Consistency'] = X.apply(\n",
    "                lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[\n",
    "                                                                                            feature] == primary_value else True,\n",
    "                axis=1\n",
    "            )\n",
    "            inconsistent_idx = X[~X['Consistency']].index\n",
    "            if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "                correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "                X.loc[inconsistent_idx, feature] = correction\n",
    "            self.cleaning_descriptions_[\n",
    "                f'check_inconsistency_{feature}'] = f\"Checked and updated inconsistencies for {feature}.\"\n",
    "\n",
    "        # Dropping the newly created column 'Consistency'\n",
    "        X = X.drop(columns=['Consistency'])\n",
    "        self.cleaning_descriptions_['drop_consistency'] = \"Dropped the 'Consistency' column.\"\n",
    "\n",
    "        # Correct zero values and adjust inconsistent records using vectorized operations\n",
    "        X.loc[X['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = X['BsmtFinSF1'] + X['BsmtUnfSF']\n",
    "        X.loc[X['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = X['TotalBsmtSF'] - X['BsmtUnfSF']\n",
    "        X.loc[X['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = X['TotalBsmtSF'] - X['BsmtFinSF1']\n",
    "        self.cleaning_descriptions_['correct_zero_values'] = \"Corrected zero values and adjusted inconsistent records.\"\n",
    "\n",
    "        # Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "        mask = X['BsmtFinSF1'] + X['BsmtUnfSF'] != X['TotalBsmtSF']\n",
    "        X.loc[mask, 'BsmtUnfSF'] = (X.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "        X.loc[mask, 'BsmtFinSF1'] = X.loc[mask, 'TotalBsmtSF'] - X.loc[mask, 'BsmtUnfSF']\n",
    "        self.cleaning_descriptions_[\n",
    "            'adjust_inconsistent_basement'] = \"Adjusted records with inconsistent basement measurements.\"\n",
    "\n",
    "        # Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "        features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "        def check_consistency(X, primary_feature):\n",
    "            primary_value = features_and_values[primary_feature]\n",
    "            return X.apply(\n",
    "                lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "                if row[primary_feature] == primary_value else True, axis=1\n",
    "            )\n",
    "\n",
    "        # Apply consistency check and correct 'GarageFinish'\n",
    "        consistency_mask = check_consistency(X, 'GarageFinish')\n",
    "        X.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "        self.cleaning_descriptions_['check_garage_finish'] = \"Checked and corrected 'GarageFinish'.\"\n",
    "\n",
    "        # Correct garage years that are earlier than the house build year\n",
    "        X.loc[X['GarageYrBlt'] < X['YearBuilt'], 'GarageYrBlt'] = X['YearBuilt']\n",
    "        self.cleaning_descriptions_[\n",
    "            'correct_garage_year'] = \"Corrected garage years that are earlier than the house build year.\"\n",
    "\n",
    "        # Drop columns not in the specified list of necessary features\n",
    "        necessary_features = ['OverallQual', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'OverallCond',\n",
    "                              'YearBuilt', 'TotalBsmtSF', 'BsmtFinType1', 'GarageYrBlt', 'LotArea',\n",
    "                              'KitchenQual', 'BsmtExposure']\n",
    "        X = X[necessary_features]\n",
    "        self.cleaning_descriptions_['drop_unnecessary_features'] = (\n",
    "            f\"Dropped columns not in the list of necessary features: {necessary_features}.\"\n",
    "        )\n",
    "\n",
    "        self.output_features_ = necessary_features  # Set the output features\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_cleaning_descriptions(self):\n",
    "        \"\"\"Return the descriptions of the cleaning steps.\"\"\"\n",
    "        return self.cleaning_descriptions_\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the cleaned features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        cleaning_descriptions = \"\\n\".join(\n",
    "            [f\"{step}: {desc}\" for step, desc in self.cleaning_descriptions_.items()])\n",
    "        return f\"DatasetCleaner(cleaning_steps:\\n{cleaning_descriptions})\"\n",
    "\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_creation_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # New features descriptions to be presented in Pipeline\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalBsmtSF_mul_BsmtExposure': 'TotalBsmtSF * BsmtExposure',\n",
    "            'NF_TotalBsmtSF_mul_BsmtFinType1': 'TotalBsmtSF * BsmtFinType1',\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalBsmtSF_mul_BsmtExposure'] = X['TotalBsmtSF'] * X['BsmtExposure']\n",
    "        X['NF_TotalBsmtSF_mul_BsmtFinType1'] = X['TotalBsmtSF'] * X['BsmtFinType1']\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()  # Set the output features\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join(\n",
    "            [f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('dataset_cleaner', DatasetCleaner()),  # Custom dataset cleaning\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])\n"
   ],
   "id": "55136cb54d5c502c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Adjusting Transformations Pipeline",
   "id": "c573ef48687157ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, BoxCoxTransformer, PowerTransformer\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['1stFlrSF', 'GrLivArea', 'KitchenQual', 'LotArea', 'TotalBsmtSF',\n",
    "                        'NF_TotalBsmtSF_mul_BsmtExposure', 'NF_TotalLivingArea_mul_OverallQual',\n",
    "                        'NF_TotalLivingArea_mul_OverallCond', 'NF_1stFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt', 'NF_TotalBsmtSF_mul_BsmtFinType1']\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])"
   ],
   "id": "a9ff8fd7e2f1ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Adjusting Post_Transformations Pipeline",
   "id": "a47f230c3206caa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['LotArea', 'TotalBsmtSF', 'NF_TotalBsmtSF_mul_BsmtExposure', 'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "# We will apply Winsorizer to features from table in jupyter_notebooks/08_Feature_Engineering_hypothesis_3.ipynb\n",
    "# The ones which had high or above outliers\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])"
   ],
   "id": "111ece942698de9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Adjusting Target Transformation Pipeline\n",
    "\n",
    "\n",
    "We simply remove Passthrough Function at is not needed anymore"
   ],
   "id": "281c8351e765bfdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add a small constant to avoid log(0)\n",
    "        return np.log1p(np.clip(X, 0, None))\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Use expm1 for numerical stability, clip to avoid overflow\n",
    "        return np.expm1(np.clip(X, None, 700))  # 700 is chosen to avoid overflow in expm1\n",
    "\n",
    "\n",
    "# Create a pipeline for transforming the target variable\n",
    "target_transformation_pipeline = Pipeline([\n",
    "    ('log_transform', LogTransformer()),  # Log transformation\n",
    "])"
   ],
   "id": "4287df45527da63e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Removing PCA from main Pipeline",
   "id": "38fa0a694c0a55c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "62b7197b1f665583",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating Catboost Regressor with just selected Features",
   "id": "97000f2b6d05b73e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(logging_level='Silent')\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": {\n",
    "        'model__regressor__iterations': [1000],\n",
    "        # Number of trees. More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__learning_rate': [0.01],\n",
    "        # Learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__depth': [6],\n",
    "        # Depth of the trees. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__l2_leaf_reg': [3],\n",
    "        # L2 regularization term. Higher values prevent overfitting by penalizing large weights.\n",
    "        'model__regressor__bagging_temperature': [0.0],\n",
    "        # Controls the variance of bagging. Higher values increase the randomization.\n",
    "        'model__regressor__random_strength': [1],\n",
    "        # Amount of randomness for scoring splits. Higher values add more randomness, helping to prevent overfitting.\n",
    "    }\n",
    "}"
   ],
   "id": "c8a3e4d03742e045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "72d5d6da38e511cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "id": "47ed788c088f370b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_catboost = models_tune_summary_catboost.iloc[0]['estimator']\n",
    "best_model_catboost"
   ],
   "id": "ba4f5d8bf76e4a4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "id": "83903451e981dc29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "id": "3d46d51706c28c53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_catboost)",
   "id": "a8350950ac5369d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that this time it used just 7 Features, Lets get scored for them",
   "id": "18c2cae5af970706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_catboost)",
   "id": "49908ab28436b154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluating with given selection of features\n",
    "\n",
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "id": "3baa13988d73ed92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that performance even increased from 0.855 to 0.856, what is a good sign, and model is less overfitted, as train now is just 0.915 from previous being 0.943",
   "id": "ebdeffb774dfd53e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "65212ace46f5eccd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(get_sorted_feature_importances(best_pipeline_catboost)\n",
    "      )"
   ],
   "id": "5b85d4f7f855e9b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing Catboost just with latest 7 Features\n",
    "\n",
    "#### Adjusting pre_transformations\n",
    "\n",
    "We will not change cleaning code, just add in Feature Creator extra line:\n",
    "``` python\n",
    "# Drop columns not in the specified list of necessary features\n",
    "        necessary_features = ['OverallQual', 'YearBuilt', 'GarageYrBlt', 'LotArea', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                              'NF_TotalLivingArea_mul_OverallCond', 'NF_TotalLivingArea_mul_OverallQual']\n",
    "        X = X[necessary_features]\n",
    "```\n"
   ],
   "id": "d6950245f7243d29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "class DatasetCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom dataset cleaner for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom data\n",
    "    cleaning during preprocessing pipelines. It handles various data cleaning \n",
    "    tasks such as filling missing values, adjusting feature inconsistencies, \n",
    "    and dropping unnecessary columns explicitly detailed within the transform \n",
    "    method, ensuring all features are appropriately processed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cleaning_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for cleaning, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of data cleaning steps to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The cleaned dataframe.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # Fill missing values with zero and convert to integers for numerical columns\n",
    "        fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'TotalBsmtSF', 'GarageYrBlt']\n",
    "        X[fill_zero_and_convert] = X[fill_zero_and_convert].fillna(0).astype(int)\n",
    "        self.cleaning_descriptions_['fill_zero_and_convert'] = (\n",
    "            f\"Filled missing values in {fill_zero_and_convert} with 0 and converted to int.\"\n",
    "        )\n",
    "\n",
    "        # Fill missing values with 'None' for categorical columns\n",
    "        fill_none = ['BsmtExposure', 'BsmtFinType1']\n",
    "        X[fill_none] = X[fill_none].fillna('None')\n",
    "        self.cleaning_descriptions_['fill_none'] = f\"Filled missing values in {fill_none} with 'None'.\"\n",
    "\n",
    "        # Adjust 'BedroomAbvGr' based on '2ndFlrSF'\n",
    "        X.loc[X['2ndFlrSF'] == 0, 'BedroomAbvGr'] = X['BedroomAbvGr'].replace(0, 2)\n",
    "        X.loc[X['2ndFlrSF'] > 0, 'BedroomAbvGr'] = X['BedroomAbvGr'].replace(0, 3)\n",
    "        self.cleaning_descriptions_['adjust_BedroomAbvGr'] = \"Adjusted 'BedroomAbvGr' based on '2ndFlrSF'.\"\n",
    "\n",
    "        # Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "        swap_idx = X['2ndFlrSF'] > X['1stFlrSF']\n",
    "        X.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = X.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "        self.cleaning_descriptions_['swap_flrsf'] = \"Swapped values where '2ndFlrSF' is greater than '1stFlrSF'.\"\n",
    "\n",
    "        # Define features and their 'no presence' values\n",
    "        basement_features = ['BsmtExposure', 'BsmtFinType1', 'TotalBsmtSF']\n",
    "        features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"TotalBsmtSF\": 0}\n",
    "\n",
    "        # Check and update inconsistencies for each feature\n",
    "        for feature in basement_features:\n",
    "            primary_value = features_and_values[feature]\n",
    "            X['Consistency'] = X.apply(\n",
    "                lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[\n",
    "                                                                                            feature] == primary_value else True,\n",
    "                axis=1\n",
    "            )\n",
    "            inconsistent_idx = X[~X['Consistency']].index\n",
    "            if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "                correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "                X.loc[inconsistent_idx, feature] = correction\n",
    "            self.cleaning_descriptions_[\n",
    "                f'check_inconsistency_{feature}'] = f\"Checked and updated inconsistencies for {feature}.\"\n",
    "\n",
    "        # Dropping the newly created column 'Consistency'\n",
    "        X = X.drop(columns=['Consistency'])\n",
    "        self.cleaning_descriptions_['drop_consistency'] = \"Dropped the 'Consistency' column.\"\n",
    "\n",
    "        # Correct zero values and adjust inconsistent records using vectorized operations\n",
    "        X.loc[X['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = X['BsmtFinSF1'] + X['BsmtUnfSF']\n",
    "        X.loc[X['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = X['TotalBsmtSF'] - X['BsmtUnfSF']\n",
    "        X.loc[X['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = X['TotalBsmtSF'] - X['BsmtFinSF1']\n",
    "        self.cleaning_descriptions_['correct_zero_values'] = \"Corrected zero values and adjusted inconsistent records.\"\n",
    "\n",
    "        # Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "        mask = X['BsmtFinSF1'] + X['BsmtUnfSF'] != X['TotalBsmtSF']\n",
    "        X.loc[mask, 'BsmtUnfSF'] = (X.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "        X.loc[mask, 'BsmtFinSF1'] = X.loc[mask, 'TotalBsmtSF'] - X.loc[mask, 'BsmtUnfSF']\n",
    "        self.cleaning_descriptions_[\n",
    "            'adjust_inconsistent_basement'] = \"Adjusted records with inconsistent basement measurements.\"\n",
    "\n",
    "        # Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "        features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "        def check_consistency(X, primary_feature):\n",
    "            primary_value = features_and_values[primary_feature]\n",
    "            return X.apply(\n",
    "                lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "                if row[primary_feature] == primary_value else True, axis=1\n",
    "            )\n",
    "\n",
    "        # Apply consistency check and correct 'GarageFinish'\n",
    "        consistency_mask = check_consistency(X, 'GarageFinish')\n",
    "        X.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "        self.cleaning_descriptions_['check_garage_finish'] = \"Checked and corrected 'GarageFinish'.\"\n",
    "\n",
    "        # Correct garage years that are earlier than the house build year\n",
    "        X.loc[X['GarageYrBlt'] < X['YearBuilt'], 'GarageYrBlt'] = X['YearBuilt']\n",
    "        self.cleaning_descriptions_[\n",
    "            'correct_garage_year'] = \"Corrected garage years that are earlier than the house build year.\"\n",
    "\n",
    "        # Drop columns not in the specified list of necessary features\n",
    "        necessary_features = ['OverallQual', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'OverallCond',\n",
    "                              'YearBuilt', 'TotalBsmtSF', 'BsmtFinType1', 'GarageYrBlt', 'LotArea',\n",
    "                              'KitchenQual', 'BsmtExposure']\n",
    "        X = X[necessary_features]\n",
    "        self.cleaning_descriptions_['drop_unnecessary_features'] = (\n",
    "            f\"Dropped columns not in the list of necessary features: {necessary_features}.\"\n",
    "        )\n",
    "\n",
    "        self.output_features_ = necessary_features  # Set the output features\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_cleaning_descriptions(self):\n",
    "        \"\"\"Return the descriptions of the cleaning steps.\"\"\"\n",
    "        return self.cleaning_descriptions_\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the cleaned features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        cleaning_descriptions = \"\\n\".join(\n",
    "            [f\"{step}: {desc}\" for step, desc in self.cleaning_descriptions_.items()])\n",
    "        return f\"DatasetCleaner(cleaning_steps:\\n{cleaning_descriptions})\"\n",
    "\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_creation_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # New features descriptions to be presented in Pipeline\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalBsmtSF_mul_BsmtExposure': 'TotalBsmtSF * BsmtExposure',\n",
    "            'NF_TotalBsmtSF_mul_BsmtFinType1': 'TotalBsmtSF * BsmtFinType1',\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "\n",
    "        # Drop columns not in the specified list of necessary features\n",
    "        necessary_features = ['OverallQual', 'YearBuilt', 'GarageYrBlt', 'LotArea', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                              'NF_TotalLivingArea_mul_OverallCond', 'NF_TotalLivingArea_mul_OverallQual']\n",
    "        X = X[necessary_features]\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()  # Set the output features\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join(\n",
    "            [f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('dataset_cleaner', DatasetCleaner()),  # Custom dataset cleaning\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])\n"
   ],
   "id": "88b4153ed8c35ba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, BoxCoxTransformer, PowerTransformer\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['LotArea', 'NF_TotalLivingArea_mul_OverallQual', 'NF_TotalLivingArea_mul_OverallCond',\n",
    "                        'NF_1stFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt']\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])"
   ],
   "id": "e4cb0cd5505e7b4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['LotArea', 'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "# We will apply Winsorizer to features from table in jupyter_notebooks/08_Feature_Engineering_hypothesis_3.ipynb\n",
    "# The ones which gad high or above outliers\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n"
   ],
   "id": "4c0ec0c97d527272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "f67ee32c10d41eae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "7269064b20e47cea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "id": "abde75233a3fae62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_catboost = models_tune_summary_catboost.iloc[0]['estimator']\n",
    "best_model_catboost"
   ],
   "id": "1e7a50f051272730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "id": "3c6fc1a01e5ae355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "id": "e3c82c9293575a32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_catboost)",
   "id": "eecdd655ce2b9382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "id": "60465bcc2582a08a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OK, now we can see, that Feature Selector from last 7 features selected just 2, and model dropped in performance.\n",
    "\n",
    "Let's Disable Feature Selector and test same model, so this time it should be only our selected 7 features, except we will drop OverallQual, as I believe it is not needed, as it is included in New Features"
   ],
   "id": "aa04156a69236ec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class DatasetCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom dataset cleaner for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom data\n",
    "    cleaning during preprocessing pipelines. It handles various data cleaning \n",
    "    tasks such as filling missing values, adjusting feature inconsistencies, \n",
    "    and dropping unnecessary columns explicitly detailed within the transform \n",
    "    method, ensuring all features are appropriately processed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cleaning_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for cleaning, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of data cleaning steps to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The cleaned dataframe.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # Fill missing values with zero and convert to integers for numerical columns\n",
    "        fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageYrBlt']\n",
    "        X[fill_zero_and_convert] = X[fill_zero_and_convert].fillna(0).astype(int)\n",
    "        self.cleaning_descriptions_['fill_zero_and_convert'] = (\n",
    "            f\"Filled missing values in {fill_zero_and_convert} with 0 and converted to int.\"\n",
    "        )\n",
    "\n",
    "        # Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "        swap_idx = X['2ndFlrSF'] > X['1stFlrSF']\n",
    "        X.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = X.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "        self.cleaning_descriptions_['swap_flrsf'] = \"Swapped values where '2ndFlrSF' is greater than '1stFlrSF'.\"\n",
    "\n",
    "        # Correct garage years that are earlier than the house build year\n",
    "        X.loc[X['GarageYrBlt'] < X['YearBuilt'], 'GarageYrBlt'] = X['YearBuilt']\n",
    "        self.cleaning_descriptions_[\n",
    "            'correct_garage_year'] = \"Corrected garage years that are earlier than the house build year.\"\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_cleaning_descriptions(self):\n",
    "        \"\"\"Return the descriptions of the cleaning steps.\"\"\"\n",
    "        return self.cleaning_descriptions_\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the cleaned features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        cleaning_descriptions = \"\\n\".join(\n",
    "            [f\"{step}: {desc}\" for step, desc in self.cleaning_descriptions_.items()])\n",
    "        return f\"DatasetCleaner(cleaning_steps:\\n{cleaning_descriptions})\"\n",
    "\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_creation_descriptions_ = {}\n",
    "        self.output_features_ = None  # Initialize output_features_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "\n",
    "        # New features descriptions to be presented in Pipeline\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "\n",
    "        # Drop columns not in the specified list of necessary features\n",
    "        necessary_features = ['YearBuilt', 'GarageYrBlt', 'LotArea', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                              'NF_TotalLivingArea_mul_OverallCond', 'NF_TotalLivingArea_mul_OverallQual']\n",
    "        X = X[necessary_features]\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()  # Set the output features\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join(\n",
    "            [f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('dataset_cleaner', DatasetCleaner()),  # Custom dataset cleaning\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])\n"
   ],
   "id": "b31d7f5977b851e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This module provides functionality to create a scikit-learn pipeline\n",
    "that includes preprocessing, feature transformation, selection, and modeling.\n",
    "\n",
    "Components:\n",
    "- create_pipeline: Function to create the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - pre_feature_transformations: A tuple of preprocessing steps before feature transformation.\n",
    "    - feature_transformer: The feature transformer to be applied.\n",
    "    - post_feature_transformer: A tuple of post-feature transformation steps.\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the steps of the pipeline\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "        # Add the model with target transformation step\n",
    "        ('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer))\n",
    "    ]\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "8e5b195410dc95aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "4bfbcfd5403157a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "id": "f3d50e528b48cda5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "id": "be7ea4cdf1d51d1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "id": "8cdd2bb60d4d027c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_catboost)",
   "id": "45944fe3cd00e6f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "id": "cd0ee4dd952fc90d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## We will Create extra Jupyter Notebook for FINAL MODEL\n",
    "\n",
    "10_Final_Model.ipynb\n",
    "\n",
    "In that model we will include all evaluations and necessary diagrams and plots as customer has requested"
   ],
   "id": "4ee3af1d7dec8751"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
