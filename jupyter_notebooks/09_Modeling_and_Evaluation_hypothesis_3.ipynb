{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ],
   "id": "3aaf88d26c42c1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "75a72d0651596244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "84e53adb39c40ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "624018c3aea597b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "d0307cd1e623595e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "b4e467105bdd1e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "45512d37254c92ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "3789f66254e08f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_parquet('outputs/datasets/cleaned/train.parquet.gzip')\n",
    "df_train.head()\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_parquet('outputs/datasets/cleaned/test.parquet.gzip')\n",
    "df_train.head()"
   ],
   "id": "9dd310e3dcb6619f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ],
   "id": "ce222f2402f6e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drop_features = ['Unnamed: 0']\n",
    "df_train.drop(columns=drop_features, inplace=True)\n",
    "df_test.drop(columns=drop_features, inplace=True)"
   ],
   "id": "f80b313366f049f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting to data and test dataframe",
   "id": "27600268ed420431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify the target variable column name\n",
    "target_column = 'SalePrice'\n",
    "\n",
    "# Extract the target variable\n",
    "y_train = df_train[target_column]\n",
    "y_test = df_test[target_column]\n",
    "\n",
    "# Remove the target variable from the DataFrame to create the feature DataFrame\n",
    "X_train = df_train.drop(columns=[target_column])\n",
    "X_test = df_test.drop(columns=[target_column])\n"
   ],
   "id": "7b05199b68ab479a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5f5cb59c8c7e169"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Pre-Transformations"
   ],
   "id": "f3077ebeb6a8d911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit method could calculate and store statistics, if needed\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()  # Work on a copy of the data\n",
    "        if 'BsmtFinType1' in X.columns and 'BsmtFinSF1' in X.columns:\n",
    "            X['BsmtFinType1_BsmtFinSF1'] = X['BsmtFinType1'] * X['BsmtFinSF1']\n",
    "        if 'BsmtExposure' in X.columns and 'TotalBsmtSF' in X.columns:\n",
    "            X['BsmtExposure_TotalBsmtSF'] = X['BsmtExposure'] * X['TotalBsmtSF']\n",
    "        if 'GarageArea' in X.columns and 'GarageFinish' in X.columns:\n",
    "            X['GarageArea_GarageFinish'] = X['GarageArea'] * X['GarageFinish']\n",
    "        return X\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])"
   ],
   "id": "1733f60ad69b2844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Testing transformations for errors and what is the return\n",
    "X_train_transformed = pre_feature_transformations.fit_transform(X_train)\n",
    "type(X_train_transformed)"
   ],
   "id": "71f4831ba6b95c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features - Columns transformations",
   "id": "daecbf2114709a09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from feature_engine.transformation import BoxCoxTransformer\n",
    "\n",
    "\n",
    "# Custom transformer for DataFrame that applies transformation to specified columns\n",
    "class DFColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer, columns):\n",
    "        self.transformer = transformer\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(X[self.columns], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X.loc[:, self.columns] = self.transformer.transform(X[self.columns])\n",
    "        return X\n",
    "\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtUnfSF',\n",
    "                        'EnclosedPorch', 'GarageFinish', 'LotArea', 'MasVnrArea', 'OpenPorchSF',\n",
    "                        'OverallCond', 'OverallQual', 'TotalBsmtSF', 'WoodDeckSF']\n",
    "power_features = ['BsmtFinSF1', 'LotFrontage']\n",
    "box_cox_features = ['GrLivArea']\n",
    "\n",
    "# Create transformers for each group of features\n",
    "yeo_johnson_transformer = DFColumnTransformer(PowerTransformer(method='yeo-johnson', standardize=True),\n",
    "                                              yeo_johnson_features)\n",
    "power_transformer = DFColumnTransformer(PowerTransformer(method='yeo-johnson', standardize=True),\n",
    "                                        power_features)  # Using Yeo-Johnson for simplicity\n",
    "box_cox_transformer = DFColumnTransformer(PowerTransformer(method='box-cox', standardize=True), box_cox_features)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "    ('box_cox', box_cox_transformer)\n",
    "])\n"
   ],
   "id": "115707dff3fb8f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features-Columns Post Transformations",
   "id": "a1b0b4a3f77735cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = [\n",
    "    '1stFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtUnfSF', 'GarageYrBlt',\n",
    "    'GrLivArea', 'OverallCond', 'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'LotArea', 'LotFrontage'\n",
    "]\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "# Create the pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "])\n",
    "\n"
   ],
   "id": "e9422024e2a178f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pipeline ",
   "id": "8e8e43659c64b622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine import transformation as vt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \"\"\"Creates a comprehensive pipeline including preprocessing, transformations, and model fitting.\"\"\"\n",
    "    main_pipeline = Pipeline([\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        ('model', model)  # Final model\n",
    "    ])\n",
    "\n",
    "    return main_pipeline"
   ],
   "id": "7a58bc2bddf07de0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ],
   "id": "84dc27d100d55d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=2, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model = create_pipeline(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ],
   "id": "bebb2cba05dcd497",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ],
   "id": "f9f113541180514c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ],
   "id": "10c625d779cae244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running Grid Search CV",
   "id": "e61feca22f472f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Results Inspection\n",
    "initial_search = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "initial_search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "b7dde5fb9f8e9143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = initial_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "8f27c51e2b115079",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that LinearRegression shows most promising results, mean = 0.78698, ExtraTreesRegressor is also high 0.779606\n",
    "Now we will add extra HyperParameters"
   ],
   "id": "951d2801cb8ac903"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dictionary containing the model instances\n",
    "models_tune_search_1 = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "}\n",
    "\n",
    "# Dictionary containing hyperparameters for tuning\n",
    "params_tune_search_1 = {\n",
    "    \"LinearRegression\": {\n",
    "        'model__fit_intercept': [True, False],  # Whether to calculate the intercept for this model\n",
    "        'model__n_jobs': [None, -1],  # Number of CPU cores used for the computations\n",
    "        'model__positive': [True, False],  # Forces the coefficients of the model to be positive\n",
    "        'model__copy_X': [True, False]  # If True, X will be copied; otherwise, it may be overwritten.\n",
    "    }\n",
    "}\n"
   ],
   "id": "a7dff9d6dc2a69ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_1 = HyperparameterOptimizationSearch(models=models_tune_search_1, params=params_tune_search_1)\n",
    "search_tuned_1.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "20ca6514b7633a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_1, models_tune_search_pipeline_1 = search_tuned_1.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_1"
   ],
   "id": "f04e2add8d9ab224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Selecting best model",
   "id": "363f8deb7b017bbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Parameters for best model",
   "id": "4b46f7f9e5737b6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_best_parameters = models_tune_search_pipeline_1[models_tune_search_summary_1.iloc[0, 0]].best_params_\n",
    "LinearRegression_best_parameters"
   ],
   "id": "55b3dc42b80b6d29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_regressor_pipeline = models_tune_search_pipeline_1[\n",
    "    models_tune_search_summary_1.iloc[0, 0]].best_estimator_\n",
    "LinearRegression_regressor_pipeline"
   ],
   "id": "50182ad9226bf9a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accessing Feature Importance",
   "id": "f2473c06ac24a8f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Assume 'lasso_best_regressor_pipeline' and 'X_train' are defined earlier in your code\n",
    "try:\n",
    "    data_cleaning_feat_eng_steps = 3  # Number of data cleaning and feature engineering steps\n",
    "    transformer_pipeline = Pipeline(LinearRegression_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "\n",
    "    # Ensure the pipeline up to this point consists only of transformers\n",
    "    if not hasattr(transformer_pipeline, 'transform'):\n",
    "        raise AttributeError(\"The sub-pipeline does not support transform operation.\")\n",
    "\n",
    "    X_transformed = transformer_pipeline.transform(X_train)\n",
    "    columns_after_data_cleaning_feat_eng = X_transformed.columns\n",
    "\n",
    "    feature_support_mask = LinearRegression_regressor_pipeline['feat_selection'].get_support()\n",
    "    best_features = columns_after_data_cleaning_feat_eng[feature_support_mask].to_list()\n",
    "\n",
    "    # DataFrame to display feature coefficients\n",
    "    df_feature_coefficients = pd.DataFrame({\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[feature_support_mask],\n",
    "        'Coefficient': LinearRegression_regressor_pipeline['model'].coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "          f\"The model was trained on them: \\n{df_feature_coefficients['Feature'].to_list()}\")\n",
    "\n",
    "    df_feature_coefficients.plot(kind='bar', x='Feature', y='Coefficient', color='blue', legend=None)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ],
   "id": "287e1d1ee058ddf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming lasso_best_regressor_pipeline is a Pipeline object containing the Lasso model\n",
    "LinearRegression_model = LinearRegression_regressor_pipeline['model']\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "feature_names = columns_after_data_cleaning_feat_eng[\n",
    "    LinearRegression_regressor_pipeline['feat_selection'].get_support()]\n",
    "\n",
    "# Get coefficients from the Lasso model\n",
    "coefficients = LinearRegression_model.coef_\n",
    "\n",
    "# Create a DataFrame to store feature names and coefficients\n",
    "df_coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort coefficients by absolute value\n",
    "df_coefficients['Abs_Coefficient'] = abs(df_coefficients['Coefficient'])\n",
    "df_coefficients_sorted = df_coefficients.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=df_coefficients_sorted)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Absolute Coefficients)')\n",
    "plt.show()\n"
   ],
   "id": "df62143bc114f658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluating Model on Train and Test Sets",
   "id": "212be54b4767cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"Model Evaluation \\n\")\n",
    "    print(\"* Train Set\")\n",
    "    regression_evaluation(X_train, y_train, pipeline)\n",
    "    print(\"* Test Set\")\n",
    "    regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    prediction = pipeline.predict(X)\n",
    "    print('R2 Score:', r2_score(y, prediction).round(3))\n",
    "    print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "    # Train plot\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0], color='blue')\n",
    "    axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')  # Red line y=x\n",
    "    axes[0].set_xlabel(\"Actual Values\")\n",
    "    axes[0].set_ylabel(\"Predictions\")\n",
    "    axes[0].set_title(\"Train Set Performance\")\n",
    "\n",
    "    # Test plot\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1], color='green')\n",
    "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Red line y=x\n",
    "    axes[1].set_xlabel(\"Actual Values\")\n",
    "    axes[1].set_ylabel(\"Predictions\")\n",
    "    axes[1].set_title(\"Test Set Performance\")\n",
    "\n",
    "    plt.show()\n"
   ],
   "id": "a4307e48f8f882a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)"
   ],
   "id": "c8183ba9e91bdceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results are way better than previous Hypothesis, but we can see model is Overfitted !\n",
    "\n",
    "Let's try ExtraTreesRegressor"
   ],
   "id": "30f57d56866133f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_2 = {\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "params_tune_search_2 = {\n",
    "    \"ExtraTreesRegressor\": {\n",
    "        'model__n_estimators': [100, 300, 600],\n",
    "        'model__max_depth': [3, 10, 20, None],\n",
    "        'model__min_samples_split': [8, 16],\n",
    "    }\n",
    "}"
   ],
   "id": "c6b0f5d76d09f396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_2 = HyperparameterOptimizationSearch(models=models_tune_search_2, params=params_tune_search_2)\n",
    "search_tuned_2.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "d640e544502c8235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_2, models_tune_search_pipeline_2 = search_tuned_2.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_2"
   ],
   "id": "d68f3a0c7db847d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_best_parameters = models_tune_search_pipeline_2[\n",
    "    models_tune_search_summary_2.iloc[0, 0]].best_params_\n",
    "ExtraTreesRegressor_best_parameters"
   ],
   "id": "ddad6f8a95aabb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_regressor_pipeline = models_tune_search_pipeline_2[\n",
    "    models_tune_search_summary_2.iloc[0, 0]].best_estimator_\n",
    "ExtraTreesRegressor_regressor_pipeline"
   ],
   "id": "2c5beb7b49b15721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Assume 'ExtraTreesRegressor_regressor_pipeline' and 'X_train' are defined earlier in your code\n",
    "try:\n",
    "    data_cleaning_feat_eng_steps = 3  # Number of data cleaning and feature engineering steps in the pipeline\n",
    "    transformer_pipeline = Pipeline(ExtraTreesRegressor_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "\n",
    "    # Ensure the pipeline up to this point consists only of transformers\n",
    "    if not hasattr(transformer_pipeline, 'transform'):\n",
    "        raise AttributeError(\"The sub-pipeline does not support transform operation.\")\n",
    "\n",
    "    X_transformed = transformer_pipeline.transform(X_train)\n",
    "    columns_after_data_cleaning_feat_eng = X_transformed.columns\n",
    "\n",
    "    feature_support_mask = ExtraTreesRegressor_regressor_pipeline['feat_selection'].get_support()\n",
    "    best_features = columns_after_data_cleaning_feat_eng[feature_support_mask].to_list()\n",
    "\n",
    "    # DataFrame to display feature importances\n",
    "    df_feature_importances = pd.DataFrame({\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[feature_support_mask],\n",
    "        'Importance': ExtraTreesRegressor_regressor_pipeline['model'].feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "          f\"The model was trained on them: \\n{df_feature_importances['Feature'].to_list()}\")\n",
    "\n",
    "    df_feature_importances.plot(kind='bar', x='Feature', y='Importance', color='blue', legend=None)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ],
   "id": "5f6d5c237e36bd69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ExtraTreesRegressor_regressor_pipeline is a Pipeline object containing the ExtraTreesRegressor model\n",
    "ExtraTreesRegressor_model = ExtraTreesRegressor_regressor_pipeline['model']\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "feature_names = columns_after_data_cleaning_feat_eng[\n",
    "    ExtraTreesRegressor_regressor_pipeline['feat_selection'].get_support()]\n",
    "\n",
    "# Get feature importances from the ExtraTreesRegressor model\n",
    "importances = ExtraTreesRegressor_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature names and their importances\n",
    "df_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort importances by their absolute values (though all will be positive here)\n",
    "df_importances_sorted = df_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=df_importances_sorted)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ],
   "id": "afa2ae84fe59ceca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)"
   ],
   "id": "3a0be6c8a5741217",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Strategies to Improve Model Performance and Reduce Overfitting\n",
    "\n",
    "### 1. Evaluate and Adjust Data Transformations\n",
    "- **Goal**: Ensure the model is not learning from noise or overly complex transformations.\n",
    "- **Actions**:\n",
    "  - Review current transformations for complexity and relevance.\n",
    "  - Simplify transformations to focus on significant features.\n",
    "  - Test removal or addition of transformations based on their impact on model performance.\n",
    "\n",
    "### 2. Implement Feature Scaling\n",
    "- **Goal**: Balance the influence of features in the model by ensuring features are on a comparable scale.\n",
    "- **Actions**:\n",
    "  - Apply `StandardScaler` for features with a normal distribution.\n",
    "  - Evaluate the impact of scaling on the model\n",
    "\n",
    "\n",
    "### 3. Monitor and Iterate\n",
    "- **Goal**: Achieve the best possible model performance through continuous improvement.\n",
    "- **Actions**:\n",
    "  - Regularly review model outputs and performance metrics.\n",
    "  - Adapt strategies as new data becomes available or as project requirements evolve.\n",
    "  - Keep abreast of new techniques or algorithms that might improve model performance.\n"
   ],
   "id": "723580887a4bbffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adjusting Pipeline",
   "id": "e2fab7929de7b148"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Removal of generating new sub_features, they did not improve model performance",
   "id": "1bc849dc0ee44fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "])"
   ],
   "id": "449a3044144538e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Changing Transformations for Features",
   "id": "cf898fc5c7ccdac4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom transformer for DataFrame that applies transformation to specified columns\n",
    "class DFColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, transformer, columns):\n",
    "        self.transformer = transformer\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(X[self.columns], y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X.loc[:, self.columns] = self.transformer.transform(X[self.columns])\n",
    "        return X\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "log_e_features = ['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage' ]\n",
    "yeo_johnson_features = ['BedroomAbvGr', 'BsmtExposure', 'EnclosedPorch', 'GarageFinish', 'MasVnrArea', 'OpenPorchSF',\n",
    "                        'OverallCond', 'OverallQual', 'TotalBsmtSF', 'WoodDeckSF']\n",
    "power_features = ['BsmtFinSF1', 'LotFrontage', '2ndFlrSF', 'BsmtUnfSF']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create transformers for each group of features\n",
    "yeo_johnson_transformer = DFColumnTransformer(PowerTransformer(method='yeo-johnson', standardize=True), yeo_johnson_features)\n",
    "power_transformer = DFColumnTransformer(PowerTransformer(method='yeo-johnson', standardize=True), power_features)  # Using Yeo-Johnson for simplicity\n",
    "log_transformer = DFColumnTransformer(FunctionTransformer(np.log1p, validate=False), log_e_features)\n",
    "\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('log', log_transformer),\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])"
   ],
   "id": "a0b5df94967d29e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding feature scaling to all features in post_transformations",
   "id": "f8c18a5e1901c6fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('features_scaler', StandardScaler())\n",
    "])\n",
    "\n"
   ],
   "id": "8d92eaff7e608073",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running GridSearch CV 2",
   "id": "3ac0b4ff49daaee6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Results Inspection\n",
    "initial_search_2 = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "initial_search_2.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "f89b38438a47aedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary_2, grid_search_pipelines_2 = initial_search_2.score_summary(sort_by='mean_score')\n",
    "grid_search_summary_2"
   ],
   "id": "1605f794a89b6241",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_3 = HyperparameterOptimizationSearch(models=models_tune_search_1, params=params_tune_search_1)\n",
    "search_tuned_3.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "4363b1626dd42e86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_3, models_tune_search_pipeline_3 = search_tuned_3.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_3"
   ],
   "id": "1b3971cb3048d672",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_best_parameters = models_tune_search_pipeline_3[models_tune_search_summary_3.iloc[0, 0]].best_params_\n",
    "LinearRegression_best_parameters"
   ],
   "id": "60f12b4d59b7d3be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_regressor_pipeline = models_tune_search_pipeline_3[models_tune_search_summary_3.iloc[0, 0]].best_estimator_\n",
    "LinearRegression_regressor_pipeline"
   ],
   "id": "f0d26097eba2397b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Assume 'lasso_best_regressor_pipeline' and 'X_train' are defined earlier in your code\n",
    "try:\n",
    "    data_cleaning_feat_eng_steps = 2  # Number of data cleaning and feature engineering steps\n",
    "    transformer_pipeline = Pipeline(LinearRegression_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "\n",
    "    # Ensure the pipeline up to this point consists only of transformers\n",
    "    if not hasattr(transformer_pipeline, 'transform'):\n",
    "        raise AttributeError(\"The sub-pipeline does not support transform operation.\")\n",
    "\n",
    "    X_transformed = transformer_pipeline.transform(X_train)\n",
    "    columns_after_data_cleaning_feat_eng = X_transformed.columns\n",
    "\n",
    "    feature_support_mask = LinearRegression_regressor_pipeline['feat_selection'].get_support()\n",
    "    best_features = columns_after_data_cleaning_feat_eng[feature_support_mask].to_list()\n",
    "\n",
    "    # DataFrame to display feature coefficients\n",
    "    df_feature_coefficients = pd.DataFrame({\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[feature_support_mask],\n",
    "        'Coefficient': LinearRegression_regressor_pipeline['model'].coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "          f\"The model was trained on them: \\n{df_feature_coefficients['Feature'].to_list()}\")\n",
    "\n",
    "    df_feature_coefficients.plot(kind='bar', x='Feature', y='Coefficient', color='blue', legend=None)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "id": "196e5744cb0814d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming lasso_best_regressor_pipeline is a Pipeline object containing the Lasso model\n",
    "LinearRegression_model = LinearRegression_regressor_pipeline['model']\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "feature_names = columns_after_data_cleaning_feat_eng[LinearRegression_regressor_pipeline['feat_selection'].get_support()]\n",
    "\n",
    "# Get coefficients from the Lasso model\n",
    "coefficients = LinearRegression_model.coef_\n",
    "\n",
    "# Create a DataFrame to store feature names and coefficients\n",
    "df_coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort coefficients by absolute value\n",
    "df_coefficients['Abs_Coefficient'] = abs(df_coefficients['Coefficient'])\n",
    "df_coefficients_sorted = df_coefficients.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=df_coefficients_sorted)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Absolute Coefficients)')\n",
    "plt.show()"
   ],
   "id": "711b3b988cd685ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)"
   ],
   "id": "738640e60a24a7b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, we see it is still overfitted. Let's test again ExtraTreesRegressor",
   "id": "4b655cb1b95b161b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_4 = HyperparameterOptimizationSearch(models=models_tune_search_2, params=params_tune_search_2)\n",
    "search_tuned_4.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "8257633e1c7249af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_4, models_tune_search_pipeline_4 = search_tuned_4.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_4"
   ],
   "id": "b6b26355c1a29c1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_best_parameters = models_tune_search_pipeline_4[models_tune_search_summary_4.iloc[0, 0]].best_params_\n",
    "ExtraTreesRegressor_best_parameters"
   ],
   "id": "32b78fe908869903",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_regressor_pipeline = models_tune_search_pipeline_4[models_tune_search_summary_4.iloc[0, 0]].best_estimator_\n",
    "ExtraTreesRegressor_regressor_pipeline"
   ],
   "id": "8dffb1c478130b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Assume 'ExtraTreesRegressor_regressor_pipeline' and 'X_train' are defined earlier in your code\n",
    "try:\n",
    "    data_cleaning_feat_eng_steps = 2  # Number of data cleaning and feature engineering steps in the pipeline\n",
    "    transformer_pipeline = Pipeline(ExtraTreesRegressor_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "\n",
    "    # Ensure the pipeline up to this point consists only of transformers\n",
    "    if not hasattr(transformer_pipeline, 'transform'):\n",
    "        raise AttributeError(\"The sub-pipeline does not support transform operation.\")\n",
    "\n",
    "    X_transformed = transformer_pipeline.transform(X_train)\n",
    "    columns_after_data_cleaning_feat_eng = X_transformed.columns\n",
    "\n",
    "    feature_support_mask = ExtraTreesRegressor_regressor_pipeline['feat_selection'].get_support()\n",
    "    best_features = columns_after_data_cleaning_feat_eng[feature_support_mask].to_list()\n",
    "\n",
    "    # DataFrame to display feature importances\n",
    "    df_feature_importances = pd.DataFrame({\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[feature_support_mask],\n",
    "        'Importance': ExtraTreesRegressor_regressor_pipeline['model'].feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "          f\"The model was trained on them: \\n{df_feature_importances['Feature'].to_list()}\")\n",
    "\n",
    "    df_feature_importances.plot(kind='bar', x='Feature', y='Importance', color='blue', legend=None)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "id": "e9488e3ba42767c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ExtraTreesRegressor_regressor_pipeline is a Pipeline object containing the ExtraTreesRegressor model\n",
    "ExtraTreesRegressor_model = ExtraTreesRegressor_regressor_pipeline['model']\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "feature_names = columns_after_data_cleaning_feat_eng[ExtraTreesRegressor_regressor_pipeline['feat_selection'].get_support()]\n",
    "\n",
    "# Get feature importances from the ExtraTreesRegressor model\n",
    "importances = ExtraTreesRegressor_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature names and their importances\n",
    "df_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort importances by their absolute values (though all will be positive here)\n",
    "df_importances_sorted = df_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=df_importances_sorted)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ],
   "id": "740976bd86399fce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)"
   ],
   "id": "a73b3c2dbc4ebd11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### We can see model is still overfitted.\n",
    "Just for fun, lets test original values without transformations. It might be a win or a loose.\n"
   ],
   "id": "70cae15e0318f9e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine import transformation as vt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \"\"\"Creates a comprehensive pipeline including preprocessing, transformations, and model fitting.\"\"\"\n",
    "    main_pipeline = Pipeline([\n",
    "        ('pre_transformations', pre_feature_transformations),          # Preprocessing steps\n",
    "        ('post_transformations', post_feature_transformer),       # Post-transformations\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        ('model', model)                                      # Final model\n",
    "    ])\n",
    "\n",
    "    return main_pipeline"
   ],
   "id": "7ae6f9d411ab2c62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Results Inspection\n",
    "initial_search_3 = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "initial_search_3.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "634bb338de0a4c68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary_3, grid_search_pipelines_3 = initial_search_3.score_summary(sort_by='mean_score')\n",
    "grid_search_summary_3"
   ],
   "id": "f0ce8c2dd787ffa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_5 = HyperparameterOptimizationSearch(models=models_tune_search_1, params=params_tune_search_1)\n",
    "search_tuned_5.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "611f5f085eaaf389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_5, models_tune_search_pipeline_5 = search_tuned_5.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_5"
   ],
   "id": "3f72bfae6242149c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_best_parameters = models_tune_search_pipeline_5[models_tune_search_summary_5.iloc[0, 0]].best_params_\n",
    "LinearRegression_best_parameters"
   ],
   "id": "3a79ee6898ef554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LinearRegression_regressor_pipeline = models_tune_search_pipeline_5[models_tune_search_summary_5.iloc[0, 0]].best_estimator_\n",
    "LinearRegression_regressor_pipeline"
   ],
   "id": "4d9b89f29c209147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Assume 'lasso_best_regressor_pipeline' and 'X_train' are defined earlier in your code\n",
    "try:\n",
    "    data_cleaning_feat_eng_steps = 1  # Number of data cleaning and feature engineering steps\n",
    "    transformer_pipeline = Pipeline(LinearRegression_regressor_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "\n",
    "    # Ensure the pipeline up to this point consists only of transformers\n",
    "    if not hasattr(transformer_pipeline, 'transform'):\n",
    "        raise AttributeError(\"The sub-pipeline does not support transform operation.\")\n",
    "\n",
    "    X_transformed = transformer_pipeline.transform(X_train)\n",
    "    columns_after_data_cleaning_feat_eng = X_transformed.columns\n",
    "\n",
    "    feature_support_mask = LinearRegression_regressor_pipeline['feat_selection'].get_support()\n",
    "    best_features = columns_after_data_cleaning_feat_eng[feature_support_mask].to_list()\n",
    "\n",
    "    # DataFrame to display feature coefficients\n",
    "    df_feature_coefficients = pd.DataFrame({\n",
    "        'Feature': columns_after_data_cleaning_feat_eng[feature_support_mask],\n",
    "        'Coefficient': LinearRegression_regressor_pipeline['model'].coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "          f\"The model was trained on them: \\n{df_feature_coefficients['Feature'].to_list()}\")\n",
    "\n",
    "    df_feature_coefficients.plot(kind='bar', x='Feature', y='Coefficient', color='blue', legend=None)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "id": "f4af30066f436ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming lasso_best_regressor_pipeline is a Pipeline object containing the Lasso model\n",
    "LinearRegression_model = LinearRegression_regressor_pipeline['model']\n",
    "\n",
    "# Get feature names from the pipeline\n",
    "feature_names = columns_after_data_cleaning_feat_eng[LinearRegression_regressor_pipeline['feat_selection'].get_support()]\n",
    "\n",
    "# Get coefficients from the Lasso model\n",
    "coefficients = LinearRegression_model.coef_\n",
    "\n",
    "# Create a DataFrame to store feature names and coefficients\n",
    "df_coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort coefficients by absolute value\n",
    "df_coefficients['Abs_Coefficient'] = abs(df_coefficients['Coefficient'])\n",
    "df_coefficients_sorted = df_coefficients.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=df_coefficients_sorted)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Absolute Coefficients)')\n",
    "plt.show()"
   ],
   "id": "438fcba10ba34528",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, LinearRegression_regressor_pipeline)"
   ],
   "id": "251a5fb79a6e39c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ouch, this went even worse.\n",
    "Time for ExtraTreesRegressor"
   ],
   "id": "1a43734592a825eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_6 = HyperparameterOptimizationSearch(models=models_tune_search_2, params=params_tune_search_2)\n",
    "search_tuned_6.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "13b398b12e1a2705",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_search_summary_6, models_tune_search_pipeline_6 = search_tuned_6.score_summary(sort_by='mean_score')\n",
    "models_tune_search_summary_6"
   ],
   "id": "868bfd034da4fd30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_best_parameters = models_tune_search_pipeline_6[models_tune_search_summary_6.iloc[0, 0]].best_params_\n",
    "ExtraTreesRegressor_best_parameters"
   ],
   "id": "c8c59f5f87a2b311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ExtraTreesRegressor_regressor_pipeline = models_tune_search_pipeline_6[models_tune_search_summary_6.iloc[0, 0]].best_estimator_\n",
    "ExtraTreesRegressor_regressor_pipeline"
   ],
   "id": "d4437fdde4dc0a14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, ExtraTreesRegressor_regressor_pipeline)"
   ],
   "id": "9befd0491c3c6784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that model is still overfitting.\n",
    "\n",
    "Let's try implementing PCA.\n",
    "Before that we will restore all transformations for  Pipeline"
   ],
   "id": "5d662f5a85e271c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "def create_pipeline(model, n_components=None):\n",
    "    \n",
    "    # Define the steps of the pipeline\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),    # Preprocessing steps\n",
    "        ('transformations', feature_transformer),            # Transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Optionally add PCA to the pipeline based on n_components\n",
    "    if n_components is not None:\n",
    "        steps.append(('pca', PCA(n_components=n_components)))\n",
    "\n",
    "    steps.extend([\n",
    "        ('feat_selection', SelectFromModel(model)),          # Feature selection based on the provided model\n",
    "        ('model', model)                                     # Final model\n",
    "    ])\n",
    "\n",
    "    return Pipeline(steps)\n"
   ],
   "id": "fc5916feef7929e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models_with_pca(X_train, y_train, models, n_components_list):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        results[name] = []\n",
    "        for components in n_components_list:\n",
    "            pipeline = create_pipeline(model, n_components=components)\n",
    "            # Using cross-validation to evaluate the model\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "            results[name].append(np.mean(scores))\n",
    "            print(f\"Model: {name}, PCA Components: {components}, Score: {np.mean(scores):.4f}\")\n",
    "\n",
    "    return results\n",
    "\n"
   ],
   "id": "483cfacc2f8da7d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_components_list = [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]  # Adjust based on your total number of features\n",
    "results = evaluate_models_with_pca(X_train, y_train, models_quick_search, n_components_list)\n"
   ],
   "id": "e8d8d8197be6d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Loop through each model and plot their results\n",
    "for model_name, accuracies in results.items():\n",
    "    plt.plot(n_components_list, accuracies, label=model_name, marker='o')  # Mark each point\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Model Accuracy')  # Change to 'Model RMSE' or appropriate metric if needed\n",
    "plt.title('Model Performance vs. PCA Components')\n",
    "plt.legend()\n",
    "\n",
    "# Add a grid for easier reading\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "4952306d127d0186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Based on this Plot we will choose CA = 13, as an optimal number.",
   "id": "a69a2193104a866a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Creating Pipeline with PCA",
   "id": "4f3ace8e4d0b3729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine import transformation as vt\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from feature_engine.encoding import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def create_pipeline(model):\n",
    "    \"\"\"Creates a comprehensive pipeline including preprocessing, transformations, and model fitting.\"\"\"\n",
    "    main_pipeline = Pipeline([\n",
    "        ('pre_transformations', pre_feature_transformations),          # Preprocessing steps\n",
    "        ('transformations', feature_transformer),                  # Transformations\n",
    "        ('post_transformations', post_feature_transformer),       # Post-transformations\n",
    "        (\"PCA\", PCA(n_components=13, random_state=0) ),\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        ('model', model)                                      # Final model\n",
    "    ])\n",
    "\n",
    "    return main_pipeline"
   ],
   "id": "e5c30158f1963e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding cross validation for Regressors.\n",
   "id": "c603ae48f664a3f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=2, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model = create_pipeline(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ],
   "id": "594a2bf7b3fbf605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_search_refit = HyperparameterOptimizationSearch(models=models_quick_search, params=params_quick_search)\n",
    "initial_search_refit.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ],
   "id": "4214512f6a4e0cac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T22:10:57.978509Z",
     "start_time": "2024-05-11T22:10:57.939084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary_refit, grid_search_pipelines_refit = initial_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ],
   "id": "33df8ae943e67c64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   estimator min_score mean_score max_score std_score\n",
       "0           LinearRegression  0.748467   0.794122   0.82926  0.029366\n",
       "3        ExtraTreesRegressor  0.662153    0.79391  0.871718  0.071866\n",
       "4          AdaBoostRegressor  0.670377   0.756489  0.813824  0.047056\n",
       "2      RandomForestRegressor  0.662667   0.741491   0.80781  0.061743\n",
       "5  GradientBoostingRegressor  0.588485   0.726548  0.804878  0.078733\n",
       "6               XGBRegressor  0.624948   0.676783   0.74762  0.046555\n",
       "1      DecisionTreeRegressor  0.396348   0.552548  0.667506  0.101012"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimator</th>\n",
       "      <th>min_score</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>std_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.748467</td>\n",
       "      <td>0.794122</td>\n",
       "      <td>0.82926</td>\n",
       "      <td>0.029366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>0.662153</td>\n",
       "      <td>0.79391</td>\n",
       "      <td>0.871718</td>\n",
       "      <td>0.071866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>0.670377</td>\n",
       "      <td>0.756489</td>\n",
       "      <td>0.813824</td>\n",
       "      <td>0.047056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.662667</td>\n",
       "      <td>0.741491</td>\n",
       "      <td>0.80781</td>\n",
       "      <td>0.061743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.588485</td>\n",
       "      <td>0.726548</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.078733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.624948</td>\n",
       "      <td>0.676783</td>\n",
       "      <td>0.74762</td>\n",
       "      <td>0.046555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>0.396348</td>\n",
       "      <td>0.552548</td>\n",
       "      <td>0.667506</td>\n",
       "      <td>0.101012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7d92fad8dbccd9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
