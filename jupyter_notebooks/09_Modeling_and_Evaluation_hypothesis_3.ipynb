{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ],
   "id": "3aaf88d26c42c1f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "75a72d0651596244"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "84e53adb39c40ff0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "624018c3aea597b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "d0307cd1e623595e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "b4e467105bdd1e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "45512d37254c92ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "3789f66254e08f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "id": "9dd310e3dcb6619f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ],
   "id": "ce222f2402f6e5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "drop_features = ['Unnamed: 0']\n",
    "df.drop(columns=drop_features, inplace=True)"
   ],
   "id": "f80b313366f049f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Dataset",
   "id": "f742e59c8c0be550"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.loc[:, 'LotFrontage'] = df['LotFrontage'].fillna(70)\n",
    "\n",
    "# Lists of columns grouped by their fill values and type conversions\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageArea', 'GarageYrBlt',\n",
    "                         'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF', 'BedroomAbvGr']\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "df['LotFrontage'] = df['LotFrontage'].round().astype(int)\n",
    "\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"BsmtFinSF1\": 0, \"BsmtUnfSF\": 0,\n",
    "                       \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Dropping new created column Consistency\n",
    "df = df.drop(columns=['Consistency'])\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtUnfSF'] + df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']"
   ],
   "id": "4afe74b356e994b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting to data and test dataframe",
   "id": "27600268ed420431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='SalePrice')\n",
    "y = df['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "id": "7b05199b68ab479a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Pre-Transformations"
   ],
   "id": "f3077ebeb6a8d911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "# Define custom FeatureCreator class\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom feature creator for pipeline integration.\n",
    "\n",
    "    This class extends sklearn's TransformerMixin to allow for custom feature\n",
    "    creation during preprocessing pipelines. It handles various mathematical\n",
    "    transformations and feature interactions explicitly detailed within the\n",
    "    transform method, ensuring all features are appropriately processed and added.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The fit method is not used for adding features, it's just here for compatibility.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply a series of custom transformations to the dataframe.\n",
    "\n",
    "        Args:\n",
    "        X (pd.DataFrame): Input dataframe from which features are derived.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The dataframe with new features added.\n",
    "\n",
    "        \"\"\"\n",
    "        X = X.copy()  # Work on a copy of the data to prevent changes to the original dataframe\n",
    "        \n",
    "        # New features descriptions to be presented in Pipeline i\n",
    "        self.feature_creation_descriptions_ = {\n",
    "            'NF_TotalBsmtSF_mul_BsmtExposure': 'TotalBsmtSF * BsmtExposure',\n",
    "            'NF_TotalBsmtSF_mul_BsmtFinType1': 'TotalBsmtSF * BsmtFinSF1',\n",
    "            'NF_BsmtFinSF1_mul_BsmtFinType1': 'BsmtFinType1 * BsmtFinSF1',\n",
    "            'NF_GarageFinish_mul_GarageArea': 'GarageFinish * GarageArea',\n",
    "            'NF_TotalLivingArea': 'GrLivArea + 1stFlrSF + 2ndFlrSF',\n",
    "            'NF_TotalLivingArea_mul_OverallQual': 'NF_TotalLivingArea * OverallQual',\n",
    "            'NF_TotalLivingArea_mul_OverallCond': 'NF_TotalLivingArea * OverallCond',\n",
    "            'NF_1stFlrSF_mul_OverallQual': '1stFlrSF * OverallQual',\n",
    "            'NF_2ndFlrSF_mul_OverallQual': '2ndFlrSF * OverallQual',\n",
    "            'NF_Age_Garage': '2010 - GarageYrBlt',\n",
    "            'NF_Age_Build': '2010 - YearBuilt',\n",
    "            'NF_Age_Remod': '2010 - YearRemodAdd',\n",
    "            'NF_Remod_TEST': '0 if NF_Age_Build == NF_Age_Remod else NF_Age_Remod',\n",
    "            'NF_Has_2nd_floor': '1 if 2ndFlrSF > 0 else 0',\n",
    "            'NF_Has_basement': '1 if TotalBsmtSF > 0 else 0',\n",
    "            'NF_Has_garage': '1 if GarageArea > 0 else 0',\n",
    "            'NF_Has_Masonry_Veneer': '1 if MasVnrArea > 0 else 0',\n",
    "            'NF_Has_Enclosed_Porch': '1 if EnclosedPorch > 0 else 0',\n",
    "            'NF_Has_Open_Porch': '1 if OpenPorchSF > 0 else 0',\n",
    "            'NF_Has_ANY_Porch': 'NF_Has_Enclosed_Porch | NF_Has_Open_Porch',\n",
    "            'NF_Has_Wooden_Deck': '1 if WoodDeckSF > 0 else 0'\n",
    "        }\n",
    "        # Numeric and Boolean feature interactions and transformations\n",
    "        X['NF_TotalBsmtSF_mul_BsmtExposure'] = X['TotalBsmtSF'] * X['BsmtExposure']\n",
    "        X['NF_TotalBsmtSF_mul_BsmtFinType1'] = X['TotalBsmtSF'] * X['BsmtFinSF1']\n",
    "        X['NF_BsmtFinSF1_mul_BsmtFinType1'] = X['BsmtFinType1'] * X['BsmtFinSF1']\n",
    "        X['NF_GarageFinish_mul_GarageArea'] = X['GarageFinish'] * X['GarageArea']\n",
    "        X['NF_TotalLivingArea'] = X['GrLivArea'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "        X['NF_TotalLivingArea_mul_OverallQual'] = X['NF_TotalLivingArea'] * X['OverallQual']\n",
    "        X['NF_TotalLivingArea_mul_OverallCond'] = X['NF_TotalLivingArea'] * X['OverallCond']\n",
    "        X['NF_1stFlrSF_mul_OverallQual'] = X['1stFlrSF'] * X['OverallQual']\n",
    "        X['NF_2ndFlrSF_mul_OverallQual'] = X['2ndFlrSF'] * X['OverallQual']\n",
    "        X['NF_Age_Garage'] = 2010 - X['GarageYrBlt']\n",
    "        X['NF_Age_Build'] = 2010 - X['YearBuilt']\n",
    "        X['NF_Age_Remod'] = 2010 - X['YearRemodAdd']\n",
    "        X['NF_Remod_TEST'] = X.apply(\n",
    "            lambda row: 0 if row['NF_Age_Build'] == row['NF_Age_Remod'] else row['NF_Age_Remod'], axis=1)\n",
    "        X[('NF_Has_2nd_floor')] = X.apply(lambda row: False if row['2ndFlrSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_basement')] = X.apply(lambda row: False if row['TotalBsmtSF'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_garage')] = X.apply(lambda row: False if row['GarageArea'] == 0 else True, axis=1).astype(int)\n",
    "        X[('NF_Has_Masonry_Veneer')] = X.apply(lambda row: False if row['MasVnrArea'] == 0 else True, axis=1).astype(\n",
    "            int)\n",
    "        X[('NF_Has_Enclosed_Porch')] = X.apply(lambda row: False if row['EnclosedPorch'] == 0 else True,\n",
    "                                               axis=1).astype(int)\n",
    "        X[('NF_Has_Open_Porch')] = X.apply(lambda row: False if row['OpenPorchSF'] == 0 else True, axis=1).astype(int)\n",
    "        X['NF_Has_ANY_Porch'] = X['NF_Has_Enclosed_Porch'] | X['NF_Has_Open_Porch'].astype(int)\n",
    "        X[('NF_Has_Wooden_Deck')] = X.apply(lambda row: False if row['WoodDeckSF'] == 0 else True, axis=1).astype(int)\n",
    "\n",
    "        self.output_features_ = X.columns.tolist()\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Return feature names for the transformed features.\"\"\"\n",
    "        return self.output_features_\n",
    "    def __repr__(self):\n",
    "        feature_descriptions = \"\\n\".join([f\"{name}: {desc}\" for name, desc in self.feature_creation_descriptions_.items()])\n",
    "        return f\"FeatureCreator(created_features:\\n{feature_descriptions})\"\n",
    "\n",
    "\n",
    "\n",
    "# Mapping and encoder setup\n",
    "encoding_dict = {\n",
    "    'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
    "    'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n",
    "    'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n",
    "    'KitchenQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "}\n",
    "\n",
    "ordinal_encoder = ce.OrdinalEncoder(mapping=[\n",
    "    {'col': k, 'mapping': v} for k, v in encoding_dict.items()\n",
    "])\n",
    "\n",
    "# Pipeline setup\n",
    "pre_feature_transformations = Pipeline(steps=[\n",
    "    ('ordinal_encoder', ordinal_encoder),  # Custom categorical encoding\n",
    "    ('feature_creator', FeatureCreator())  # Custom feature creation\n",
    "])"
   ],
   "id": "1733f60ad69b2844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features - Columns transformations",
   "id": "daecbf2114709a09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, BoxCoxTransformer, PowerTransformer\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtExposure', 'BsmtUnfSF', 'EnclosedPorch',\n",
    "                        'GarageArea', 'GarageFinish', 'GrLivArea', 'KitchenQual', 'LotArea', 'MasVnrArea',\n",
    "                        'OpenPorchSF', 'OverallCond', 'TotalBsmtSF', 'WoodDeckSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                        'NF_BsmtFinSF1_mul_BsmtFinType1', 'NF_GarageFinish_mul_GarageArea', 'NF_TotalLivingArea',\n",
    "                        'NF_Age_Garage', 'NF_Age_Remod', 'NF_Remod_TEST', 'NF_TotalLivingArea_mul_OverallQual',\n",
    "                        'NF_TotalLivingArea_mul_OverallCond', 'NF_1stFlrSF_mul_OverallQual',\n",
    "                        'NF_2ndFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt', 'LotFrontage', 'YearRemodAdd', 'NF_TotalBsmtSF_mul_BsmtFinType1', 'NF_Age_Build']\n",
    "box_cox_features = []\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "box_cox_transformer = BoxCoxTransformer(variables=box_cox_features)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])\n"
   ],
   "id": "115707dff3fb8f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Features-Columns Post Transformations",
   "id": "a1b0b4a3f77735cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['GarageArea', 'LotArea', 'LotFrontage', 'TotalBsmtSF', 'NF_TotalBsmtSF_mul_BsmtExposure',\n",
    "                      'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "# We will apply Winsorizer to features from table in jupyter_notebooks/08_Feature_Engineering_hypothesis_3.ipynb\n",
    "# The ones which gad high or above outliers\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n"
   ],
   "id": "e9422024e2a178f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Target Transformations",
   "id": "fc2526c4423ef565"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Add a small constant to avoid log(0)\n",
    "        return np.log1p(np.clip(X, 0, None))\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Use expm1 for numerical stability, clip to avoid overflow\n",
    "        return np.expm1(np.clip(X, None, 700))  # 700 is chosen to avoid overflow in expm1\n",
    "\n",
    "\n",
    "\n",
    "# Create a pipeline for transforming the target variable\n",
    "target_transformation_pipeline = Pipeline([\n",
    "    ('log_transform', LogTransformer()),  # Log transformation\n",
    "])\n",
    "\n",
    "\n",
    "class PassthroughTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"A transformer that passes through the data without changing it.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for passthrough\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        # Return the data as is\n",
    "        return X\n",
    "\n",
    "\n",
    "# Create a pipeline for passthrough transformation\n",
    "passthrough_transformation_pipeline = Pipeline([\n",
    "    ('passthrough', PassthroughTransformer())  # Passthrough transformer\n",
    "])\n",
    "\n"
   ],
   "id": "6a79ddf1fab5c7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main Pipeline ",
   "id": "8e8e43659c64b622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import is_regressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def supports_feature_selection(model):\n",
    "    \"\"\"\n",
    "    Check if the model supports feature selection.\n",
    "    A model supports feature selection if it has 'coef_' or 'feature_importances_' attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The model to check.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the model supports feature selection, False otherwise.\n",
    "    \"\"\"\n",
    "    return hasattr(model, 'coef_') or hasattr(model, 'feature_importances_')\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "    ]\n",
    "\n",
    "    # Conditionally add feature selection step\n",
    "    if supports_feature_selection(model):\n",
    "        steps.append(('feat_selection', SelectFromModel(model)))  # Feature selection\n",
    "\n",
    "    # Add the model with target transformation step\n",
    "    steps.append(('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer)))\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n"
   ],
   "id": "7a58bc2bddf07de0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML Pipeline for Modeling and Hyperparameters Optimization\n",
    "\n",
    "This is custom Class Hyperparameter Optimization"
   ],
   "id": "84dc27d100d55d1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class grid_cv_search_hp:\n",
    "    \"\"\"\n",
    "    Class to perform hyperparameter optimization across multiple machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        params (dict): Dictionary of hyperparameters for the models.\n",
    "        grid_searches (dict): Dictionary to store the results of GridSearchCV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params, target_transformer):\n",
    "        \"\"\"\n",
    "        Initializes the GridCvSearchHP with models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            models (dict): A dictionary of model names and instances.\n",
    "            params (dict): A dictionary of model names and their hyperparameters.\n",
    "            target_transformer: Transformer to apply to the target variable.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.grid_searches = {}\n",
    "        self.target_transformer = target_transformer\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=10, scoring='r2', refit=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization using GridSearchCV for each model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data features.\n",
    "            y (array-like): Training data target values.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "            n_jobs (int): Number of jobs to run in parallel.\n",
    "            verbose (int): Controls the verbosity of the output.\n",
    "            scoring (str): Scoring metric for model evaluation.\n",
    "            refit (bool): Whether to refit the best model on the whole dataset after searching.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for key in self.models:\n",
    "            print(f\"\\nOptimizing hyperparameters for {key}...\\n\")\n",
    "            model = create_pipeline(self.models[key], self.target_transformer)\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        \"\"\"\n",
    "        Summarize the grid search results.\n",
    "        \n",
    "        Args:\n",
    "            sort_by (str): The column to sort the results by.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A pandas DataFrame containing the summary of grid search results.\n",
    "            dict: The grid search results.\n",
    "        \"\"\"\n",
    "\n",
    "        def row(key, scores, params):\n",
    "            \"\"\"Creates a row for the summary dataframe.\"\"\"\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = f\"split{i}_test_score\"\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append(row(k, s, p))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns += [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches\n"
   ],
   "id": "bebb2cba05dcd497",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grid Search CV\n",
    "\n",
    "For this time being we will use default hyperparameters, just to select best algorithms"
   ],
   "id": "f9f113541180514c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, QuantileRegressor, \\\n",
    "    RANSACRegressor, Lars, OrthogonalMatchingPursuit, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.svm import SVR, NuSVR, LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Initializing regression models with default parameters\n",
    "\n",
    "# Linear Models\n",
    "linear_models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(max_iter=100000),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'QuantileRegressor': QuantileRegressor(),\n",
    "    'RANSACRegressor': RANSACRegressor(),\n",
    "    'PLSRegression': PLSRegression(),\n",
    "    'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "    'TheilSenRegressor': TheilSenRegressor()\n",
    "}\n",
    "\n",
    "# Tree-Based Models\n",
    "tree_based_models = {\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Gradient Boosting Frameworks\n",
    "gradient_boosting_models = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "    'LGBMRegressor': LGBMRegressor(),\n",
    "    'CatBoostRegressor': CatBoostRegressor()\n",
    "}\n",
    "\n",
    "# Support Vector Machines\n",
    "svm_models = {\n",
    "    'SVR': SVR(),\n",
    "    'NuSVR': NuSVR(),\n",
    "    'LinearSVR': LinearSVR(max_iter=100000)\n",
    "}\n",
    "\n",
    "# Nearest Neighbors\n",
    "nearest_neighbors_models = {\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'RadiusNeighborsRegressor': RadiusNeighborsRegressor(radius=20.0) # Uncomment if needed\n",
    "}\n",
    "\n",
    "# Bayesian Methods\n",
    "bayesian_models = {\n",
    "    'GaussianProcessRegressor': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "# Combining all models into a single dictionary for quick search\n",
    "models_quick_search = {\n",
    "    **linear_models,\n",
    "    **tree_based_models,\n",
    "    **gradient_boosting_models,\n",
    "    **svm_models,\n",
    "    **nearest_neighbors_models,\n",
    "    **bayesian_models\n",
    "}\n",
    "\n",
    "# Define an empty hyperparameter dictionary as all models will use default parameters initially\n",
    "params_quick_search = {model_name: {} for model_name in models_quick_search}\n"
   ],
   "id": "10c625d779cae244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Running Grid Search CV",
   "id": "e61feca22f472f5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_search = grid_cv_search_hp(models=models_quick_search, params=params_quick_search,\n",
    "                                   target_transformer=target_transformation_pipeline)\n",
    "initial_search.fit(X_train, y_train, cv=5, n_jobs=-1, scoring='r2', refit=False)"
   ],
   "id": "b7dde5fb9f8e9143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "grid_search_summary, grid_search_pipelines = initial_search.score_summary()\n",
    "grid_search_summary"
   ],
   "id": "f892ab114b9ea019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of Regressors by Category\n",
    "\n",
    "1. **Linear Models**\n",
    "   * BayesianRidge, 0.860079\n",
    "   * PLSRegression, 0.853426\n",
    "   * Ridge, 0.851585\n",
    "   * HuberRegressor, 0.845978\n",
    "   * LinearRegression, 0.842141\n",
    "   * RANSACRegressor, 0.795119\n",
    "   * TheilSenRegressor, 0.285762\n",
    "   * Lasso, -0.036293\n",
    "   * ElasticNet, -0.036293\n",
    "   * QuantileRegressor, -0.055394\n",
    "\n",
    "2. **Tree-Based Models**\n",
    "   * HistGradientBoostingRegressor, 0.858713\n",
    "   * RandomForestRegressor, 0.853213\n",
    "   * ExtraTreesRegressor, 0.843201\n",
    "   * GradientBoostingRegressor, 0.833988\n",
    "   * AdaBoostRegressor, 0.793819\n",
    "   * DecisionTreeRegressor, 0.698089\n",
    "\n",
    "3. **Gradient Boosting Frameworks**\n",
    "   * CatBoostRegressor, 0.856161\n",
    "   * LGBMRegressor, 0.851544\n",
    "   * XGBRegressor, 0.828049\n",
    "\n",
    "4. **Support Vector Machines**\n",
    "   * LinearSVR, 0.846725\n",
    "   * NuSVR, 0.842756\n",
    "   * SVR, 0.827245\n",
    "\n",
    "5. **Nearest Neighbors**\n",
    "   * KNeighborsRegressor, 0.800393\n",
    "\n",
    "6. **Bayesian Methods**\n",
    "   * GaussianProcessRegressor, -6.150232\n",
    "\n",
    "We can see that the top-performing models are from the BayesianRidge, HistGradientBoostingRegressor, and CatBoostRegressor categories.\n",
    "\n",
    "Let's test the top 2 regressors from each category. Even though they show high scores, it does not mean they are the best for a given model.\n",
    "* We will skip GaussianProcessRegressor as it is way too low. This model does not work for the given dataset and transformations.\n",
    "\n",
    "## Regressors Exploration\n",
    "\n",
    "### BayesianRidge"
   ],
   "id": "951d2801cb8ac903"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": BayesianRidge(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_bayesianridge = {\n",
    "    \"BayesianRidge\": {\n",
    "        'model__regressor__max_iter': [300, 600, 1000],  # Number of iterations for convergence. Increasing iterations can improve accuracy but increase computation time.\n",
    "        'model__regressor__tol': [1e-3, 1e-4, 1e-5],  # Tolerance for the stopping criteria. Lower values lead to more precise convergence but longer training times.\n",
    "        'model__regressor__alpha_1': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__alpha_2': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the alpha parameter (prior distribution). Controls regularization strength; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_1': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "        'model__regressor__lambda_2': [1e-6, 1e-5, 1e-4],  # Hyperparameter for the lambda parameter (prior distribution). Controls regularization strength for weight precision; lower values reduce overfitting.\n",
    "    }\n",
    "}\n"
   ],
   "id": "be6a96de159576e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_br = grid_cv_search_hp(models=models_tune_search_bayesianridge, params=params_tune_search_bayesianridge,\n",
    "                                     target_transformer=target_transformation_pipeline)\n",
    "search_tuned_br.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "728e5a031f37dbfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_br, models_tune_pipelines_br = search_tuned_br.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_br"
   ],
   "id": "df377f860c1c1371",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_br, 'models/hypothesis_3/models_tune_pipelines_br.pkl')\n",
    "models_tune_summary_br.to_csv('models/hypothesis_3/models_tune_summary_br.csv', index=False)"
   ],
   "id": "40853e6cacf6776f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_br = models_tune_summary_br.iloc[0]['estimator']\n",
    "best_model_br"
   ],
   "id": "57f9b1393eff327e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_br = models_tune_pipelines_br[best_model_br].best_params_\n",
    "best_parameters_br"
   ],
   "id": "3e20e0cf811668cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__alpha_1': 1e-06,\n",
    " 'model__regressor__alpha_2': 0.0001,\n",
    " 'model__regressor__lambda_1': 0.0001,\n",
    " 'model__regressor__lambda_2': 1e-06,\n",
    " 'model__regressor__max_iter': 300,\n",
    " 'model__regressor__tol': 1e-05}"
   ],
   "id": "5d546372437c8c9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_br = models_tune_pipelines_br[best_model_br].best_estimator_\n",
    "best_pipeline_br"
   ],
   "id": "6e96b3a92a692f63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Accessing Feature Importance",
   "id": "706eb93fbbf0958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def plot_feature_importance_absolute(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Plot the absolute feature importance from a given pipeline.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the sub-pipeline does not support the transform operation.\n",
    "        ValueError: If there is a mismatch in feature support mask length and transformed features.\n",
    "    \"\"\"\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances\n",
    "        }).sort_values(by=importance_type, ascending=False)\n",
    "\n",
    "        # Plotting the feature importances or coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=importance_type, y='Feature', data=feature_importances_df)\n",
    "        plt.xlabel(importance_type.capitalize())\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Feature {importance_type.capitalize()}s')\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ],
   "id": "caadadf3af8b501f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_br)",
   "id": "1c1415ea6682ca73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's get all Features sorted by their importance",
   "id": "1c5ebd0aeed212e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_sorted_feature_importances(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Retrieve and return the feature importances or coefficients from a given pipeline, sorted by absolute value.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing features and their absolute importances or coefficients, sorted by absolute value.\n",
    "    \"\"\"\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances,\n",
    "            f'abs_{importance_type}': np.abs(importances)\n",
    "        }).sort_values(by=f'abs_{importance_type}', ascending=False)\n",
    "\n",
    "        return feature_importances_df[['Feature', importance_type, f'abs_{importance_type}']]\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ],
   "id": "77baf6cf9a876bef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_br)",
   "id": "a9b3443a54712d8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a regression model on both the training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "    return (r2_train, mae_train, mse_train, rmse_train, msle_train), (r2_test, mae_test, mse_test, rmse_test, msle_test)\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates a regression model on a given dataset and prints key metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Data features.\n",
    "        y (array-like): Data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    prediction = pipeline.predict(X)\n",
    "    r2 = r2_score(y, prediction)\n",
    "    mae = mean_absolute_error(y, prediction)\n",
    "    mse = mean_squared_error(y, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    msle = mean_squared_log_error(y, prediction)\n",
    "\n",
    "\n",
    "    return r2, mae, mse, rmse, msle\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    \"\"\"\n",
    "    Plots actual vs predicted values for both training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "        alpha_scatter (float): Transparency of the scatter plot points.\n",
    "    \"\"\"\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "    # Train set evaluation\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    # Test set evaluation\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "    # Train plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0, 0], color='blue')\n",
    "    axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 0].set_ylabel(\"Predictions\")\n",
    "    axes[0, 0].set_title(\"Train Set: Actual vs Predicted\")\n",
    "    train_metrics_text = (f'R2: {round(r2_train, 3)}\\n'\n",
    "                          f'MAE: {round(mae_train, 3)}\\n'\n",
    "                          f'MSE: {round(mse_train, 3)}\\n'\n",
    "                          f'RMSE: {round(rmse_train, 3)}\\n'\n",
    "                          f'MSLE: {round(msle_train, 3)}')\n",
    "    axes[0, 0].text(0.05, 0.95, train_metrics_text, transform=axes[0, 0].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Test plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[0, 1], color='green')\n",
    "    axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 1].set_ylabel(\"Predictions\")\n",
    "    axes[0, 1].set_title(\"Test Set: Actual vs Predicted\")\n",
    "    test_metrics_text = (f'R2: {round(r2_test, 3)}\\n'\n",
    "                         f'MAE: {round(mae_test, 3)}\\n'\n",
    "                         f'MSE: {round(mse_test, 3)}\\n'\n",
    "                         f'RMSE: {round(rmse_test, 3)}\\n'\n",
    "                         f'MSLE: {round(msle_test, 3)}')\n",
    "    axes[0, 1].text(0.05, 0.95, test_metrics_text, transform=axes[0, 1].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Residuals plot: Train\n",
    "    residuals_train = y_train - pred_train\n",
    "    sns.scatterplot(x=pred_train, y=residuals_train, alpha=alpha_scatter, ax=axes[1, 0], color='blue')\n",
    "    axes[1, 0].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel(\"Predictions\")\n",
    "    axes[1, 0].set_ylabel(\"Residuals\")\n",
    "    axes[1, 0].set_title(\"Train Set: Residuals\")\n",
    "\n",
    "    # Residuals plot: Test\n",
    "    residuals_test = y_test - pred_test\n",
    "    sns.scatterplot(x=pred_test, y=residuals_test, alpha=alpha_scatter, ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel(\"Predictions\")\n",
    "    axes[1, 1].set_ylabel(\"Residuals\")\n",
    "    axes[1, 1].set_title(\"Test Set: Residuals\")\n",
    "\n",
    "    # Error distribution plot: Train\n",
    "    sns.histplot(residuals_train, kde=True, ax=axes[1, 2], color='blue')\n",
    "    axes[1, 2].set_xlabel(\"Residuals\")\n",
    "    axes[1, 2].set_ylabel(\"Frequency\")\n",
    "    axes[1, 2].set_title(\"Train Set: Error Distribution\")\n",
    "\n",
    "    # Error distribution plot: Test\n",
    "    sns.histplot(residuals_test, kde=True, ax=axes[0, 2], color='green')\n",
    "    axes[0, 2].set_xlabel(\"Residuals\")\n",
    "    axes[0, 2].set_ylabel(\"Frequency\")\n",
    "    axes[0, 2].set_title(\"Test Set: Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "5c06f06129ade337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_br)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_br)"
   ],
   "id": "376a2219a4d56ad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "OK, we have some quote promising results:\n",
    "* Train R<sup>2</sup> = 0.861\n",
    "* Test R<sup>2</sup> = 0.778\n",
    "\n",
    "### PLSRegression"
   ],
   "id": "2a190acb5efe8c53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Define the models\n",
    "models_tune_search_pls = {\n",
    "    \"PLSRegression\": PLSRegression(),\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_pls = {\n",
    "    \"PLSRegression\": {\n",
    "        'model__regressor__n_components': [2, 5, 10, 15, 20],  # Number of components to keep.\n",
    "        'model__regressor__max_iter': [300, 600, 1000, 2000],  # Maximum number of iterations for the algorithm to converge.\n",
    "        'model__regressor__tol': [1e-6, 1e-7, 1e-8, 1e-9],  # Tolerance for the stopping criteria.\n",
    "    }\n",
    "}\n"
   ],
   "id": "7281141623cc9caa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_pls = grid_cv_search_hp(models=models_tune_search_pls, params=params_tune_search_pls,\n",
    "                                    target_transformer=target_transformation_pipeline)\n",
    "search_tuned_pls.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "48352152bfa6c27e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_pls, models_tune_pipelines_pls = search_tuned_pls.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_pls"
   ],
   "id": "6645f0af35b752da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(models_tune_pipelines_pls, 'models/hypothesis_3/models_tune_pipelines_pls.pkl')\n",
    "models_tune_summary_pls.to_csv('models/hypothesis_3/models_tune_summary_pls.csv', index=False)"
   ],
   "id": "ad8a212f36487ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_pls = models_tune_summary_pls.iloc[0]['estimator']\n",
    "best_model_pls"
   ],
   "id": "802f7770878c0e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_pls = models_tune_pipelines_pls[best_model_pls].best_params_\n",
    "best_parameters_pls"
   ],
   "id": "ecd2823ffb39f7de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{'model__regressor__max_iter': 300,\n",
    " 'model__regressor__n_components': 10,\n",
    " 'model__regressor__tol': 1e-06}"
   ],
   "id": "52841eee2a68fe75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best Pipeline\n",
    "\n",
    "best_pipeline_pls = models_tune_pipelines_pls[best_model_pls].best_estimator_\n",
    "best_pipeline_pls"
   ],
   "id": "5d67fe2e11ca84f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_pls)",
   "id": "e6ffc3aefe2342e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_sorted_feature_importances(best_pipeline_pls)",
   "id": "4a7ef12801ad6054",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_pls)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_pls)"
   ],
   "id": "f85458ab30fbdc0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have Results:\n",
    "\n",
    "* Train R<sup>2</sup> = 0.878\n",
    "* Test R<sup>2</sup> = 0.74\n",
    "\n",
    "### HistGradientBoostingRegressor"
   ],
   "id": "15d92712e1bdbf8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "27971eb779e64d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1d094e630682f276",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
