{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aaf88d26c42c1f5",
   "metadata": {},
   "source": [
    "# Predicting SalePrice\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Create and evaluate model to predict SalePrice of building\n",
    "\n",
    "## Inputs:\n",
    "* outputs/datasets/cleaned/test.parquet.gzip\n",
    "* outputs/datasets/cleaned/train.parquet.gzip\n",
    "* Conclusions from Feature Engineering jupyter_notebooks/04_Feature_Engineering.ipynb\n",
    "\n",
    "## Outputs\n",
    "* Train Set: Features and Target\n",
    "* Test Set: Features and Target\n",
    "* Feature Engineering Pipeline\n",
    "* Modeling Pipeline\n",
    "* Features Importance Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a72d0651596244",
   "metadata": {},
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "id": "84e53adb39c40ff0",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "624018c3aea597b3",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0307cd1e623595e",
   "metadata": {},
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4e467105bdd1e91",
   "metadata": {},
   "source": [
    "Confirm new current directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "45512d37254c92ce",
   "metadata": {},
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3789f66254e08f44",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dd310e3dcb6619f",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce222f2402f6e5f",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Before exploring data and doing transformations, as we decided earlier, we drop features:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f80b313366f049f6",
   "metadata": {},
   "source": [
    "drop_features = ['Unnamed: 0']\n",
    "df.drop(columns=drop_features, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27600268ed420431",
   "metadata": {},
   "source": [
    "## Splitting to data and test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b05199b68ab479a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='SalePrice')\n",
    "y = df['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3077ebeb6a8d911",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Main Pipeline"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from feature_engine.transformation import YeoJohnsonTransformer, PowerTransformer\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "from custom_transformers import DatasetCleaner, FeatureCreator, LogTransformer  # Import custom transformers\n",
    "\n",
    "# Pipelines\n",
    "pre_feature_transformations = Pipeline([\n",
    "    ('dataset_cleaner', DatasetCleaner()),\n",
    "    ('feature_creator', FeatureCreator())\n",
    "])\n",
    "\n",
    "# Define the columns for each transformation type\n",
    "yeo_johnson_features = ['LotArea', 'NF_TotalLivingArea_mul_OverallQual', 'NF_TotalLivingArea_mul_OverallCond',\n",
    "                        'NF_1stFlrSF_mul_OverallQual']\n",
    "power_features = ['GarageYrBlt']\n",
    "\n",
    "# Create transformers for each group of features using feature_engine transformers\n",
    "yeo_johnson_transformer = YeoJohnsonTransformer(variables=yeo_johnson_features)\n",
    "power_transformer = PowerTransformer(variables=power_features, exp=0.5)\n",
    "\n",
    "# Combine all transformers into a single pipeline\n",
    "feature_transformer = Pipeline([\n",
    "    ('yeo_johnson', yeo_johnson_transformer),\n",
    "    ('power', power_transformer),\n",
    "])\n",
    "\n",
    "# Define the columns for Winsorization\n",
    "winsorize_features = ['LotArea', 'NF_TotalLivingArea_mul_OverallCond']\n",
    "\n",
    "# Initialize the Winsorizer transformer\n",
    "winsorize_transformer = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=winsorize_features)\n",
    "\n",
    "# Create the post-feature transformations pipeline\n",
    "post_feature_transformer = Pipeline([\n",
    "    ('winsorize', winsorize_transformer),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create a pipeline for transforming the target variable\n",
    "target_transformation_pipeline = Pipeline([\n",
    "    ('log_transform', LogTransformer()),  # Log transformation\n",
    "])\n",
    "\n",
    "\n",
    "def create_pipeline(model, target_transformer):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing, feature transformation, selection, and modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - pre_feature_transformations: A tuple of preprocessing steps before feature transformation.\n",
    "    - feature_transformer: The feature transformer to be applied.\n",
    "    - post_feature_transformer: A tuple of post-feature transformation steps.\n",
    "    - model: The regressor model to be used in the pipeline.\n",
    "    - target_transformer: The transformer for the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - main_pipeline: A scikit-learn Pipeline object.\n",
    "    \"\"\"\n",
    "    # Define the steps of the pipeline\n",
    "    steps = [\n",
    "        ('pre_transformations', pre_feature_transformations),  # Preprocessing steps\n",
    "        ('transformations', feature_transformer),  # Feature transformations\n",
    "        ('post_transformations', post_feature_transformer),  # Post-transformations\n",
    "        # Add the model with target transformation step\n",
    "        ('model', TransformedTargetRegressor(regressor=model, transformer=target_transformer))\n",
    "    ]\n",
    "\n",
    "    # Define the complete pipeline\n",
    "    main_pipeline = Pipeline(steps)\n",
    "\n",
    "    return main_pipeline\n",
    "\n",
    "# Save the pipeline\n"
   ],
   "id": "207c9746cb8964af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Grid Search",
   "id": "88c3a74e9666aeeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class grid_cv_search_hp:\n",
    "    \"\"\"\n",
    "    Class to perform hyperparameter optimization across multiple machine learning models.\n",
    "    \n",
    "    Attributes:\n",
    "        models (dict): Dictionary of models to evaluate.\n",
    "        params (dict): Dictionary of hyperparameters for the models.\n",
    "        grid_searches (dict): Dictionary to store the results of GridSearchCV.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, params, target_transformer):\n",
    "        \"\"\"\n",
    "        Initializes the GridCvSearchHP with models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            models (dict): A dictionary of model names and instances.\n",
    "            params (dict): A dictionary of model names and their hyperparameters.\n",
    "            target_transformer: Transformer to apply to the target variable.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.grid_searches = {}\n",
    "        self.target_transformer = target_transformer\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=10, scoring='r2', refit=False):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization using GridSearchCV for each model.\n",
    "        \n",
    "        Args:\n",
    "            X (array-like): Training data features.\n",
    "            y (array-like): Training data target values.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "            n_jobs (int): Number of jobs to run in parallel.\n",
    "            verbose (int): Controls the verbosity of the output.\n",
    "            scoring (str): Scoring metric for model evaluation.\n",
    "            refit (bool): Whether to refit the best model on the whole dataset after searching.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for key in self.models:\n",
    "            print(f\"\\nOptimizing hyperparameters for {key}...\\n\")\n",
    "            model = create_pipeline(self.models[key], self.target_transformer)\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring, refit=refit)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        \"\"\"\n",
    "        Summarize the grid search results.\n",
    "        \n",
    "        Args:\n",
    "            sort_by (str): The column to sort the results by.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A pandas DataFrame containing the summary of grid search results.\n",
    "            dict: The grid search results.\n",
    "        \"\"\"\n",
    "\n",
    "        def row(key, scores, params):\n",
    "            \"\"\"Creates a row for the summary dataframe.\"\"\"\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = f\"split{i}_test_score\"\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append(row(k, s, p))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns += [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches\n"
   ],
   "id": "39299a1f4a08cb0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Hyper Parameters",
   "id": "e4d8a2da17097fe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "models_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": CatBoostRegressor(logging_level='Silent')\n",
    "}\n",
    "\n",
    "# Define the parameter grid with appropriate settings\n",
    "params_tune_search_catboost = {\n",
    "    \"CatBoostRegressor\": {\n",
    "        'model__regressor__iterations': [1000],\n",
    "        # Number of trees. More iterations can improve performance but increase computation time.\n",
    "        'model__regressor__learning_rate': [0.01],\n",
    "        # Learning rate. Lower values make the model more robust to overfitting but require more iterations.\n",
    "        'model__regressor__depth': [6],\n",
    "        # Depth of the trees. Deeper trees can capture more complexity but increase the risk of overfitting.\n",
    "        'model__regressor__l2_leaf_reg': [3],\n",
    "        # L2 regularization term. Higher values prevent overfitting by penalizing large weights.\n",
    "        'model__regressor__bagging_temperature': [0.0],\n",
    "        # Controls the variance of bagging. Higher values increase the randomization.\n",
    "        'model__regressor__random_strength': [1],\n",
    "        # Amount of randomness for scoring splits. Higher values add more randomness, helping to prevent overfitting.\n",
    "    }\n",
    "}"
   ],
   "id": "22fd88f36d873df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Performing Grid Search",
   "id": "137fca40d7d00f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_tuned_catboost = grid_cv_search_hp(models=models_tune_search_catboost, params=params_tune_search_catboost,\n",
    "                                          target_transformer=target_transformation_pipeline)\n",
    "search_tuned_catboost.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5, refit=True)"
   ],
   "id": "896b7ec63b61d977",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models_tune_summary_catboost, models_tune_pipelines_catboost = search_tuned_catboost.score_summary(sort_by='mean_score')\n",
    "models_tune_summary_catboost"
   ],
   "id": "8bec6e9d38b836a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Selecting best Model\n",
    "\n",
    "best_model_catboost = models_tune_summary_catboost.iloc[0]['estimator']\n",
    "best_model_catboost"
   ],
   "id": "15afc3a8d23a0cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters\n",
    "\n",
    "best_parameters_catboost = models_tune_pipelines_catboost[best_model_catboost].best_params_\n",
    "best_parameters_catboost"
   ],
   "id": "a7f5745ecd5abfde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_pipeline_catboost = models_tune_pipelines_catboost[best_model_catboost].best_estimator_\n",
    "best_pipeline_catboost"
   ],
   "id": "ded6463202c3a540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "def plot_feature_importance_absolute(selected_pipeline):\n",
    "    \"\"\"\n",
    "    Plot the absolute feature importance from a given pipeline.\n",
    "\n",
    "    Args:\n",
    "        selected_pipeline (Pipeline): The complete pipeline including feature selection and model.\n",
    "\n",
    "    Raises:\n",
    "        AttributeError: If the sub-pipeline does not support the transform operation.\n",
    "        ValueError: If there is a mismatch in feature support mask length and transformed features.\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_model(pipeline):\n",
    "        \"\"\"\n",
    "        Extract the final model from the pipeline.\n",
    "        \"\"\"\n",
    "        for step_name, step in pipeline.steps:\n",
    "            if isinstance(step, TransformedTargetRegressor):\n",
    "                return step.regressor_\n",
    "            elif hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):\n",
    "                return step\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract the sub-pipeline up to the model step\n",
    "        feature_names = None\n",
    "        for name, step in selected_pipeline.named_steps.items():\n",
    "            if name == 'model':\n",
    "                break\n",
    "            if hasattr(step, 'get_feature_names_out'):\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = step.get_feature_names_out(feature_names)\n",
    "            else:\n",
    "                # Simulate the transformation to get feature names\n",
    "                if feature_names is None:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=[f'feature_{i}' for i in range(\n",
    "                        step.transform(pd.DataFrame()).shape[1])])).columns.tolist()\n",
    "                else:\n",
    "                    feature_names = step.transform(pd.DataFrame(columns=feature_names)).columns.tolist()\n",
    "\n",
    "        if feature_names is None:\n",
    "            raise ValueError(\"Could not retrieve transformed feature names.\")\n",
    "\n",
    "        # Extract the final model from the pipeline\n",
    "        model = extract_model(selected_pipeline)\n",
    "\n",
    "        if model is None:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Get feature importances or coefficients\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            importance_type = 'importance'\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            importances = model.coef_.flatten()\n",
    "            importance_type = 'coefficient'\n",
    "        else:\n",
    "            raise ValueError(\"The model does not have feature importances or coefficients.\")\n",
    "\n",
    "        # Create a DataFrame for feature importances or coefficients\n",
    "        feature_importances_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            importance_type: importances\n",
    "        }).sort_values(by=importance_type, ascending=False)\n",
    "\n",
    "        # Plotting the feature importances or coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x=importance_type, y='Feature', data=feature_importances_df)\n",
    "        plt.xlabel(importance_type.capitalize())\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Feature {importance_type.capitalize()}s')\n",
    "        plt.show()\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"AttributeError: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ],
   "id": "1be67ee8154b1681",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_feature_importance_absolute(best_pipeline_catboost)",
   "id": "60dd5685b66c58aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a regression model on both the training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "    return (r2_train, mae_train, mse_train, rmse_train, msle_train), (r2_test, mae_test, mse_test, rmse_test, msle_test)\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "    \"\"\"\n",
    "    Evaluates a regression model on a given dataset and prints key metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Data features.\n",
    "        y (array-like): Data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "    \"\"\"\n",
    "    prediction = pipeline.predict(X)\n",
    "    r2 = r2_score(y, prediction)\n",
    "    mae = mean_absolute_error(y, prediction)\n",
    "    mse = mean_squared_error(y, prediction)\n",
    "    rmse = np.sqrt(mse)\n",
    "    msle = mean_squared_log_error(y, prediction)\n",
    "\n",
    "    return r2, mae, mse, rmse, msle\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "    \"\"\"\n",
    "    Plots actual vs predicted values for both training and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X_train (array-like): Training data features.\n",
    "        y_train (array-like): Training data target values.\n",
    "        X_test (array-like): Test data features.\n",
    "        y_test (array-like): Test data target values.\n",
    "        pipeline (Pipeline): The regression model pipeline to evaluate.\n",
    "        alpha_scatter (float): Transparency of the scatter plot points.\n",
    "    \"\"\"\n",
    "    pred_train = pipeline.predict(X_train)\n",
    "    pred_test = pipeline.predict(X_test)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
    "\n",
    "    # Train set evaluation\n",
    "    r2_train, mae_train, mse_train, rmse_train, msle_train = regression_evaluation(X_train, y_train, pipeline)\n",
    "    # Test set evaluation\n",
    "    r2_test, mae_test, mse_test, rmse_test, msle_test = regression_evaluation(X_test, y_test, pipeline)\n",
    "\n",
    "    # Train plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_train, y=pred_train, alpha=alpha_scatter, ax=axes[0, 0], color='blue')\n",
    "    axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "    axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 0].set_ylabel(\"Predictions\")\n",
    "    axes[0, 0].set_title(\"Train Set: Actual vs Predicted\")\n",
    "    train_metrics_text = (f'R2: {round(r2_train, 3)}\\n'\n",
    "                          f'MAE: {round(mae_train, 3)}\\n'\n",
    "                          f'MSE: {round(mse_train, 3)}\\n'\n",
    "                          f'RMSE: {round(rmse_train, 3)}\\n'\n",
    "                          f'MSLE: {round(msle_train, 3)}')\n",
    "    axes[0, 0].text(0.05, 0.95, train_metrics_text, transform=axes[0, 0].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Test plot: Actual vs Predicted\n",
    "    sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[0, 1], color='green')\n",
    "    axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "    axes[0, 1].set_ylabel(\"Predictions\")\n",
    "    axes[0, 1].set_title(\"Test Set: Actual vs Predicted\")\n",
    "    test_metrics_text = (f'R2: {round(r2_test, 3)}\\n'\n",
    "                         f'MAE: {round(mae_test, 3)}\\n'\n",
    "                         f'MSE: {round(mse_test, 3)}\\n'\n",
    "                         f'RMSE: {round(rmse_test, 3)}\\n'\n",
    "                         f'MSLE: {round(msle_test, 3)}')\n",
    "    axes[0, 1].text(0.05, 0.95, test_metrics_text, transform=axes[0, 1].transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "    # Residuals plot: Train\n",
    "    residuals_train = y_train - pred_train\n",
    "    sns.scatterplot(x=pred_train, y=residuals_train, alpha=alpha_scatter, ax=axes[1, 0], color='blue')\n",
    "    axes[1, 0].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_xlabel(\"Predictions\")\n",
    "    axes[1, 0].set_ylabel(\"Residuals\")\n",
    "    axes[1, 0].set_title(\"Train Set: Residuals\")\n",
    "\n",
    "    # Residuals plot: Test\n",
    "    residuals_test = y_test - pred_test\n",
    "    sns.scatterplot(x=pred_test, y=residuals_test, alpha=alpha_scatter, ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].axhline(0, color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel(\"Predictions\")\n",
    "    axes[1, 1].set_ylabel(\"Residuals\")\n",
    "    axes[1, 1].set_title(\"Test Set: Residuals\")\n",
    "\n",
    "    # Error distribution plot: Train\n",
    "    sns.histplot(residuals_train, kde=True, ax=axes[1, 2], color='blue')\n",
    "    axes[1, 2].set_xlabel(\"Residuals\")\n",
    "    axes[1, 2].set_ylabel(\"Frequency\")\n",
    "    axes[1, 2].set_title(\"Train Set: Error Distribution\")\n",
    "\n",
    "    # Error distribution plot: Test\n",
    "    sns.histplot(residuals_test, kde=True, ax=axes[0, 2], color='green')\n",
    "    axes[0, 2].set_xlabel(\"Residuals\")\n",
    "    axes[0, 2].set_ylabel(\"Frequency\")\n",
    "    axes[0, 2].set_title(\"Test Set: Error Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "39720708dcd1c181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test, best_pipeline_catboost)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, best_pipeline_catboost)"
   ],
   "id": "9728ea745c660236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Saving current pipeline\n",
    "\n",
    "We will save current pipeline in outputs/ml_pipeline/predict_saleprice.pkl"
   ],
   "id": "8e7741db2129bddf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(value=best_pipeline_catboost, filename='outputs/ml_pipeline/predict_sale_price.pkl')"
   ],
   "id": "c70e930c44943e69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Business Requirements\n",
    "\n",
    "As model is complete and validated, we have to satisfy business requirements\n",
    "\n",
    "### Business Requirement 1 - Data Visualisation and Correlation Study\n",
    "\n",
    "We already Have Feature Importance from our model, now we will create plots of Correlation between Given Features and SalePrice"
   ],
   "id": "9c3f1e5d39bcf1da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_transformed = pre_feature_transformations.fit_transform(X_train)",
   "id": "eb718e6c998c8821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_transformed",
   "id": "50a7c325dba22db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of numerical features to plot\n",
    "numerical_features = df_transformed.columns.tolist()\n",
    "\n",
    "# Create scatter plots for numerical features\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df_transformed[feature], y=y_train)\n",
    "    plt.title(f'SalePrice vs {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()"
   ],
   "id": "d5202744a5a5cea8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_transformed and y_train are already defined\n",
    "\n",
    "# Compute the Pearson correlation of each feature with the target variable\n",
    "pearson_corr = df_transformed.apply(lambda x: x.corr(y_train, method='pearson'))\n",
    "\n",
    "# Compute the Spearman correlation of each feature with the target variable\n",
    "spearman_corr = df_transformed.apply(lambda x: x.corr(y_train, method='spearman'))\n",
    "\n",
    "# Combine the correlations into a DataFrame\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Pearson': pearson_corr,\n",
    "    'Spearman': spearman_corr\n",
    "})\n",
    "\n",
    "# Plot the combined correlations\n",
    "correlation_df_sorted = correlation_df.sort_values(by='Pearson', ascending=False)\n",
    "\n",
    "# Create a grouped bar plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(correlation_df_sorted))\n",
    "\n",
    "bars1 = plt.bar(index, correlation_df_sorted['Pearson'], bar_width, label='Pearson', color='skyblue')\n",
    "bars2 = plt.bar(index + bar_width, correlation_df_sorted['Spearman'], bar_width, label='Spearman', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Pearson and Spearman Correlation of Features with SalePrice')\n",
    "plt.xticks(index + bar_width / 2, correlation_df_sorted.index, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "\n",
    "# Add the correlation coefficients on top of each bar\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "a2a8855ec33203a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting SalePrice For customer",
   "id": "fc9e628cb36da74f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_predict = pd.read_csv(\"outputs/datasets/collection/InheritedHouses.csv\")",
   "id": "be38edeb94c9e533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_predict = df_predict[['YearBuilt', 'GarageYrBlt', 'LotArea', '1stFlrSF', '2ndFlrSF',\n",
    "                         'GrLivArea', 'OverallQual', 'OverallCond']]"
   ],
   "id": "80748837ee7ea44f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions = best_pipeline_catboost.predict(df_predict)",
   "id": "af86ab725f7ea798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_predict.loc[:, 'SalePrice'] = predictions.round().astype(int)",
   "id": "a214ce104986700d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_predict",
   "id": "923b1f537d8fcf1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "markdown_table = tabulate(df_predict, headers='keys', tablefmt='pipe', showindex=False)"
   ],
   "id": "8a04d67815711ad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(markdown_table)",
   "id": "dff6327a6982b6f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_pipeline_catboost.named_steps",
   "id": "b599431b3013699c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5bdaef566c548704",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
