{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook 03 - Data Cleaning\n",
    "\n",
    "## Objectives\n",
    "* Clean data\n",
    "* Evaluate and process missing data\n",
    "\n",
    "## Inputs\n",
    "* outputs/datasets/collection/HousePricesRecords.csv\n",
    "\n",
    "## Outputs\n",
    "* Create Clean dataset:\n",
    "    * all new datasets of cleaning will be stored in inputs/datasets/cleaning\n",
    "* Split created dataset in to 2 parts:\n",
    "    * Train\n",
    "    * Test\n",
    "* all new datasets (train and test) will be stored in outputs/datasets/cleaned"
   ],
   "id": "a7875c24c43c0d1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Change working directory\n",
    "In This section we will get location of current directory and move one step up, to parent folder, so App will be accessing project folder.\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ],
   "id": "47a32a78833a8ea4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:47:41.906186Z",
     "start_time": "2024-05-03T17:47:41.897249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "441b1d998870df76",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chdir() defines the new current directory"
   ],
   "id": "af8ce4b5f364e90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:47:44.271028Z",
     "start_time": "2024-05-03T17:47:44.264234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"you have set a new current directory\")"
   ],
   "id": "c8c62acadc6c7d2",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confirm new current directory",
   "id": "59f6db5b4d3b9e3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:47:45.423217Z",
     "start_time": "2024-05-03T17:47:45.415093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ],
   "id": "6d1302e4d5192730",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading Dataset",
   "id": "891d66d7edd5b491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:20:13.748426Z",
     "start_time": "2024-05-03T19:20:13.601239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
    "df.head()"
   ],
   "id": "d983f77e553443e0",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Will make a copy of dataset, so later it can be compared with cleaned one",
   "id": "46a82af219cc1b93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exploring Data\n",
    "\n",
    "We will get all features that are missing data as a list"
   ],
   "id": "957218d717e97c5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "features_with_missing_data"
   ],
   "id": "d450551279e1eeb9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualizing Missing Data\n",
    "\n",
    "Visualize the missing data to better understand the pattern of missingness."
   ],
   "id": "8f92e45858086afa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the background style\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Prepare the data for heatmap, converting True/False to integers for color mapping\n",
    "heatmap_data = df.isnull().astype(int)\n",
    "\n",
    "# Visualize missing values as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Using `cbar=True` to show the color bar\n",
    "ax = sns.heatmap(heatmap_data, yticklabels=False, cbar=True, cmap='viridis',\n",
    "                 cbar_kws={'label': 'Missing Data Indicator'})\n",
    "\n",
    "# Customize the plot with titles and labels as needed\n",
    "plt.title('Missing Data Heatmap', fontsize=16, color='navy')\n",
    "plt.xticks(fontsize=12, color='darkred')  # Set x-tick colors to dark red\n",
    "plt.yticks(fontsize=12, color='darkred')  # Set y-tick colors to dark red\n",
    "\n",
    "# Set color bar label and style\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label('Missing Data Indicator', rotation=270, labelpad=20)\n",
    "cbar.set_ticks([0.25, 0.75])  # Setting tick positions\n",
    "cbar.set_ticklabels(['Present', 'Missing'])  # Setting tick labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "977c7f5b71a393e8",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8eb5b42cd79911b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will generate a profiling report for features with missing data, which will assist us in selecting the most effective method for data cleaning.",
   "id": "386c56982028ba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df=df[features_with_missing_data], minimal=True)\n",
    "profile.to_notebook_iframe()"
   ],
   "id": "9843a601856f1124",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b5da735548f077fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## All Data Cleaning\n",
    "\n",
    "All Data cleaning with all steps, graphs, etc. will be performed in folder jupyter_notebooks/data_cleaning/\n",
    "\n",
    "* All this is because we have noticed quite a few missing data vales, also we need to perform more deep checking on data is it all correct and valid.\n",
    "* After All cleaning and fixing data in folder jupyter_notebooks/data_cleaning is completed, we will import inputs/datasets/cleaning/clean_finished.csv as current dataframe\n",
    "\n",
    "When jupyter_notebooks/data_cleaning is being processed, we will add cleaning code below"
   ],
   "id": "c0e3fed0608a12d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:47:55.070571Z",
     "start_time": "2024-05-03T17:47:54.833904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.loc[:, 'LotFrontage'] = df['LotFrontage'].fillna(70)\n",
    "\n",
    "# Lists of columns grouped by their fill values and type conversions\n",
    "fill_zero_and_convert = ['1stFlrSF', '2ndFlrSF', 'GarageArea', 'GarageYrBlt',\n",
    "                         'EnclosedPorch', 'MasVnrArea', 'WoodDeckSF', 'BedroomAbvGr']\n",
    "fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
    "\n",
    "# Fill missing values with zero and convert to integers for numerical columns\n",
    "df[fill_zero_and_convert] = df[fill_zero_and_convert].fillna(0).astype(int)\n",
    "\n",
    "# Fill missing values with 'None' for categorical columns\n",
    "df[fill_none] = df[fill_none].fillna('None')\n",
    "df['LotFrontage'] = df['LotFrontage'].round().astype(int)\n",
    "\n",
    "df.loc[df['2ndFlrSF'] == 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 2)\n",
    "df.loc[df['2ndFlrSF'] > 0, 'BedroomAbvGr'] = df['BedroomAbvGr'].replace(0, 3)\n",
    "\n",
    "# Swap values where '2ndFlrSF' is greater than '1stFlrSF'\n",
    "swap_idx = df['2ndFlrSF'] > df['1stFlrSF']\n",
    "df.loc[swap_idx, ['1stFlrSF', '2ndFlrSF']] = df.loc[swap_idx, ['2ndFlrSF', '1stFlrSF']].values\n",
    "\n",
    "# Define features and their 'no presence' values\n",
    "basement_features = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\n",
    "features_and_values = {\"BsmtExposure\": \"None\", \"BsmtFinType1\": \"None\", \"BsmtFinSF1\": 0, \"BsmtUnfSF\": 0,\n",
    "                       \"TotalBsmtSF\": 0}\n",
    "\n",
    "# Check and update inconsistencies for each feature\n",
    "for feature in basement_features:\n",
    "    primary_value = features_and_values[feature]\n",
    "    df['Consistency'] = df.apply(\n",
    "        lambda row: all(row[f] == v for f, v in features_and_values.items()) if row[feature] == primary_value else True,\n",
    "        axis=1\n",
    "    )\n",
    "    inconsistent_idx = df[~df['Consistency']].index\n",
    "    if feature in ['BsmtExposure', 'BsmtFinType1']:\n",
    "        correction = 'No' if feature == 'BsmtExposure' else 'Unf'\n",
    "        df.loc[inconsistent_idx, feature] = correction\n",
    "\n",
    "# Correct zero values and adjust inconsistent records using vectorized operations\n",
    "df.loc[df['BsmtUnfSF'] == 0, 'BsmtUnfSF'] = df['TotalBsmtSF'] - df['BsmtFinSF1']\n",
    "df.loc[df['BsmtFinSF1'] == 0, 'BsmtFinSF1'] = df['TotalBsmtSF'] - df['BsmtUnfSF']\n",
    "df.loc[df['TotalBsmtSF'] == 0, 'TotalBsmtSF'] = df['BsmtUnfSF'] + df['BsmtFinSF1']\n",
    "\n",
    "# Identify and adjust records with inconsistent basement measurements using a ratio (example: 3)\n",
    "mask = df['BsmtFinSF1'] + df['BsmtUnfSF'] != df['TotalBsmtSF']\n",
    "df.loc[mask, 'BsmtUnfSF'] = (df.loc[mask, 'TotalBsmtSF'] / 3).astype(int)\n",
    "df.loc[mask, 'BsmtFinSF1'] = df.loc[mask, 'TotalBsmtSF'] - df.loc[mask, 'BsmtUnfSF']\n",
    "\n",
    "# Define a dictionary for checking consistency based on 'GarageFinish'\n",
    "features_and_values = {\"GarageArea\": 0, \"GarageFinish\": 'None', \"GarageYrBlt\": 0}\n",
    "\n",
    "\n",
    "def check_consistency(df, primary_feature):\n",
    "    primary_value = features_and_values[primary_feature]\n",
    "    return df.apply(\n",
    "        lambda row: all(row[feature] == value for feature, value in features_and_values.items())\n",
    "        if row[primary_feature] == primary_value else True, axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "# Apply consistency check and correct 'GarageFinish'\n",
    "consistency_mask = check_consistency(df, 'GarageFinish')\n",
    "df.loc[~consistency_mask, 'GarageFinish'] = 'Unf'\n",
    "\n",
    "# Correct garage years that are earlier than the house build year\n",
    "df.loc[df['GarageYrBlt'] < df['YearBuilt'], 'GarageYrBlt'] = df['YearBuilt']"
   ],
   "id": "faf716153a9b930",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "During cleaning, we did not drop any features, even some of them were missing nearly 90% of data.\n",
    "We keep them to explore any potential correlations."
   ],
   "id": "fb7256e19753fd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Spliting data to train and test",
   "id": "277f5fd9c5161802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:17:06.645056Z",
     "start_time": "2024-05-03T18:17:06.633006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure output directories exist\n",
    "output_dir = 'outputs/datasets/cleaned'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Creates the directory if it does not exist\n",
    "\n",
    "# No need to separate features and target just yet, keep the dataframe whole for the split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=13)"
   ],
   "id": "a88ea0efe7bf69f3",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Save the datasets to PARQUET files including 'SalePrice'\n",
    "df_train.to_parquet(f'{output_dir}/train.parquet.gzip', compression='gzip')\n",
    "df_test.to_parquet(f'{output_dir}/test.parquet.gzip', compression='gzip')"
   ],
   "id": "bc0851c445c2e727",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Before we start plotting, we need to drop 'Unnamed: 0'\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ],
   "id": "471f8d9e916a7868",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation Study",
   "id": "968a6bf7e76a0283"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initial study of correlations between features",
   "id": "f7c332e2dc885dfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before we start checking correlations, we need to ",
   "id": "67c04c8785ff8ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:48:21.853626Z",
     "start_time": "2024-05-03T17:48:01.165339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "%matplotlib inline\n",
    "\n",
    "def format_annotation(val, fmt=\"{:.2f}\"):\n",
    "    \"\"\"\n",
    "    Custom function to format the annotation, ensuring that formatting is only applied to non-masked values.\n",
    "    Returns formatted string or empty string if the value is NaN.\n",
    "    \"\"\"\n",
    "    return fmt.format(val) if not np.isnan(val) else ''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "import pandas as pd  # Ensure pandas is imported if not already\n",
    "%matplotlib inline\n",
    "\n",
    "# Other function definitions remain the same\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    \"\"\"\n",
    "    Generates a correlation heatmap using seaborn with optional annotations.\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5, fmt=\"\",  # Removed fmt and applied custom annotation\n",
    "                    cbar_kws={\"format\": plt.FuncFormatter(lambda x, pos: format_annotation(x))})\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    \"\"\"\n",
    "    Generates a Power Predictive Score (PPS) heatmap using seaborn.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing PPS values.\n",
    "        threshold (float): Threshold value to mask the PPS scores for better visibility.\n",
    "        figsize (tuple): Dimensions of the figure.\n",
    "        font_annot (int): Font size for annotations.\n",
    "    \"\"\"\n",
    "    if len(df.columns) > 1:\n",
    "        # Create a mask for values below threshold\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.abs(df) < threshold] = False\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                    linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def calculate_corr_and_pps(df):\n",
    "    \"\"\"\n",
    "    Calculates both Pearson and Spearman correlations and Power Predictive Scores for the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Contains DataFrames for Pearson correlation, Spearman correlation, and PPS matrix.\n",
    "    \"\"\"\n",
    "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
    "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def display_corr_and_pps(df_corr_pearson, df_corr_spearman, pps_matrix, corr_threshold, pps_threshold,\n",
    "                         figsize=(20, 12), font_annot=8):\n",
    "    \"\"\"\n",
    "    Displays correlation and PPS heatmaps for analysis.\n",
    "\n",
    "    Parameters:\n",
    "        df_corr_pearson (DataFrame): Pearson correlation DataFrame.\n",
    "        df_corr_spearman (DataFrame): Spearman correlation DataFrame.\n",
    "        pps_matrix (DataFrame): PPS matrix DataFrame.\n",
    "        corr_threshold (float): Threshold for correlation masking.\n",
    "        pps_threshold (float): Threshold for PPS masking.\n",
    "        figsize (tuple): Dimensions of the figure.\n",
    "        font_annot (int): Font size for annotations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\n",
    "        \"* Analyze how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyze multi colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=corr_threshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=corr_threshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=pps_threshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "\n",
    "df_corr_pearson, df_corr_spearman, pps_matrix = calculate_corr_and_pps(df_train)\n",
    "\n",
    "display_corr_and_pps(df_corr_pearson=df_corr_pearson,\n",
    "                     df_corr_spearman=df_corr_spearman,\n",
    "                     pps_matrix=pps_matrix,\n",
    "                     corr_threshold=0.01, pps_threshold=0.01,\n",
    "                     figsize=(12, 10), font_annot=10)"
   ],
   "id": "3e08513ee88cfedf",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5bfa0890591149c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "We will explore correlation (pearson and spearman methods) of Sales Price and all the rest features, sort values by absolute value and see most correlated features\n",
    "Before we process to correlation, all data objects needs to be encoded"
   ],
   "id": "57e070a6d14f8fec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:23:10.041702Z",
     "start_time": "2024-05-03T19:23:02.507530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "\n",
    "def plot_single_correlation(correlation, correlation_type, target):\n",
    "    \"\"\"\n",
    "    Plots a single correlation type.\n",
    "    correlation: DataFrame with correlation values\n",
    "    correlation_type: String, name of the correlation type\n",
    "    target: String, name of the target variable\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = correlation.plot(kind='bar', color='lightgreen')\n",
    "    plt.title(f'{correlation_type} Correlation Coefficients with {target}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel(f'{correlation_type} Correlation Coefficient')\n",
    "    plt.grid(True)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(p.get_x() + p.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_numerical_correlations(df, target):\n",
    "    \"\"\"Calculates and returns a dictionary of correlation DataFrames for numerical features.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    correlations = {}\n",
    "    for method in ['Pearson', 'Spearman', 'Kendall']:\n",
    "        corrs = []\n",
    "        for col in numerical_cols:\n",
    "            if col != target:\n",
    "                if method == 'Pearson':\n",
    "                    corr_value = ss.pearsonr(df[col], df[target])[0]\n",
    "                elif method == 'Spearman':\n",
    "                    corr_value = ss.spearmanr(df[col], df[target])[0]\n",
    "                elif method == 'Kendall':\n",
    "                    corr_value = ss.kendalltau(df[col], df[target])[0]\n",
    "                corrs.append(corr_value)\n",
    "        correlations[method] = pd.Series(corrs, index=[col for col in numerical_cols if col != target])\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def calculate_categorical_correlations(df, target):\n",
    "    \"\"\"Calculates and returns a dictionary of correlation DataFrames for categorical features.\"\"\"\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    correlations = {'Cramer\\'s V': {}, 'Mutual Information': {}}\n",
    "    for col in categorical_cols:\n",
    "        if col != target:\n",
    "            table = pd.crosstab(df[col], df[target])\n",
    "            chi2 = ss.chi2_contingency(table)[0]\n",
    "            correlations['Cramer\\'s V'][col] = np.sqrt(chi2 / (table.sum().sum() * (min(table.shape) - 1)))\n",
    "            correlations['Mutual Information'][col] = mutual_info_score(df[col], df[target])\n",
    "    for key in correlations:\n",
    "        correlations[key] = pd.Series(correlations[key])\n",
    "    return correlations\n",
    "\n",
    "\n",
    "def plot_top_correlations(df, target, top_n=10):\n",
    "    num_corr = calculate_numerical_correlations(df, target)\n",
    "    cat_corr = calculate_categorical_correlations(df, target)\n",
    "    all_correlations = {**num_corr, **cat_corr}\n",
    "\n",
    "    # Plot individual correlation types\n",
    "    for corr_type, corr_values in all_correlations.items():\n",
    "        plot_single_correlation(corr_values, corr_type, target)\n",
    "\n",
    "    # Convert dictionary of Series to DataFrame\n",
    "    correlation_df = pd.DataFrame(all_correlations)\n",
    "\n",
    "    # Calculate the maximum correlation value for each feature and determine the corresponding method\n",
    "    max_values = correlation_df.max(axis=1)\n",
    "    max_methods = correlation_df.idxmax(axis=1)\n",
    "\n",
    "    # Create a DataFrame to store these maximum values along with their corresponding methods\n",
    "    result_df = pd.DataFrame({\n",
    "        'Feature': max_values.index,\n",
    "        'Method': max_methods.values,\n",
    "        'Value': max_values.values\n",
    "    })\n",
    "\n",
    "    # Sort the DataFrame by the 'Value' column in descending order to find the highest correlations\n",
    "    result_df = result_df.sort_values(by='Value', ascending=False).head(top_n)\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(result_df['Feature'] + \" (\" + result_df['Method'] + \")\", result_df['Value'], color='skyblue')\n",
    "    plt.title('Top Correlation Coefficients Across All Methods')\n",
    "    plt.xlabel('Feature (Method)')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Annotating the bars with their correlation values\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.2f}\", va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "list_of_top = plot_top_correlations(df_train, 'SalePrice', top_n=10)"
   ],
   "id": "407613fdc03d09dd",
   "execution_count": 75,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Both methods showed some high and moderate levels of correlation between SalePrice and given features (variables).\n",
    "We will keep all values where correlation is 0.6 from both methods"
   ],
   "id": "56315963ef047277"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:23:23.169744Z",
     "start_time": "2024-05-03T19:23:23.158689Z"
    }
   },
   "cell_type": "code",
   "source": "list_of_top",
   "id": "89f060c8dc990401",
   "execution_count": 76,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have a list of features we will be investigating:\n",
    "* BsmtFinType1\n",
    "* KitchenQual\n",
    "* OverallQual\n",
    "* BsmtExposure\n",
    "* GrLivArea\n",
    "* GarageFinish\n",
    "* 1stFlrSF\n",
    "* TotalBsmtSF\n",
    "* GarageArea\n",
    "* YearBuilt\n",
    "\n",
    "We will convert top features to list and save it in jupyter_notebooks/intern_notebook_information_share.json\n",
    "This might be handy in future, less hand typing."
   ],
   "id": "b4f614ecb9675f59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T18:55:07.774626Z",
     "start_time": "2024-05-03T18:55:07.765550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Converting to list all top features\n",
    "top_correlations_list_hypothesis_1 = list_of_top.values.tolist()\n",
    "# Making a list of features + SalePrice, so later it is easier to manage dataframe.\n",
    "features_list = [item[0] for item in top_correlations_list_hypothesis_1]\n",
    "features_list.append('SalePrice')\n",
    "\n",
    "# Importing some extra functions we have created\n",
    "from extra_funcionality import save_data\n",
    "\n",
    "# Saving both lists for future reuse\n",
    "save_data('hypothesis_1_top_correlations', top_correlations_list_hypothesis_1)\n",
    "save_data('hypothesis_1_features', features_list)"
   ],
   "id": "1cb4d95456e11b9",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## EDA on selected features\n",
    "\n",
    "We will create separate dataframe just with selected features + SalePrice"
   ],
   "id": "eeb16cfb95622c1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:02:03.809236Z",
     "start_time": "2024-05-03T19:02:03.774354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df_train[features_list]\n",
    "df"
   ],
   "id": "9ed89bb1c6d3667e",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will check how each feature is distributed against the Price, so we can see correlations",
   "id": "82bc4f2c7dbd4b3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T19:07:39.805509Z",
     "start_time": "2024-05-03T19:07:34.575890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numeric_columns = df.columns\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot each feature against SalePrice\n",
    "for column in numeric_columns[:-1]:\n",
    "    sns.scatterplot(x=column, y='SalePrice', data=df)\n",
    "    plt.title(f'{column} vs. SalePrice')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()"
   ],
   "id": "e0273db2c314cd07",
   "execution_count": 71,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hypothesis\n",
    "\n",
    "Based on Plots above we can conclude these points:\n",
    "\n",
    "* BsmtFinType1\n",
    "* KitchenQual - \n",
    "* OverallQual\n",
    "* BsmtExposure\n",
    "* GrLivArea\n",
    "* GarageFinish\n",
    "* 1stFlrSF\n",
    "* TotalBsmtSF\n",
    "* GarageArea\n",
    "* YearBuilt\n",
    "\n",
    "\n",
    "* TotalBsmtSF - By increasing Basement we are increasing SalePrice\n",
    "* 1stFlrSF - By increasing 1st floor living area Sale Price tends to increase\n",
    "* YearBuilt - By increasing Garage Year Built we are increasing SalePrice, but it looks more exponential after 1980 ish...\n",
    "* OverallQual - Overall Quality is most correlated feature from all, and when it increases, SalePrice also increases\n",
    "* GrLivArea - Increasing Ground Living Area SalePrice also increases\n",
    "* 1stFlrSF - Similar to Ground Living Area, when it goes up, SalePrice also increases\n",
    "* GarageArea - When Garage Area goes up, Sale Price also increase. Most of the houses without garages are 150.000 or less\n",
    "Other Features are categorical, so it is hard to describe how they impact Price:\n",
    "* BsmtFinType1\n",
    "* KitchenQual\n",
    "* BsmtExposure\n",
    "* \n",
    "* \n",
    "* GrLivArea\n",
    "* GarageFinish\n",
    "* 1stFlrSF\n",
    "* TotalBsmtSF\n",
    "* GarageArea\n",
    "* YearBuilt\n"
   ],
   "id": "1707db0016b22579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Next is Feature Engineering",
   "id": "1a3f56f90eb6269e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "d9f71c5ff0fec22c",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
